import os
import torch
import whisper
import moviepy.editor as mp
from tqdm import tqdm
import concurrent.futures
import numpy as np
import re  # Th√™m th∆∞ vi·ªán re cho clean_transcript
import subprocess
import shutil
import signal

# Import c√°c h√†m c·∫ßn thi·∫øt t·ª´ c√°c module kh√°c
# N·∫øu c·∫ßn import circular, d√πng import ƒëi·ªÅu ki·ªán
def import_from_modules():
    global fast_motion_detection, fast_motion_detection_with_decord, fast_extract_segments, fast_extract_segments_cuda
    try:
        from action_detection import (
            fast_motion_detection, 
            fast_motion_detection_with_decord, 
            fast_extract_segments, 
            fast_extract_segments_cuda
        )
    except ImportError:
        print("‚ö†Ô∏è Kh√¥ng th·ªÉ import c√°c h√†m t·ª´ action_detection")
    
    global filter_irrelevant_content, enhanced_summarize_transcript
    try:
        from summary_generator import filter_irrelevant_content, enhanced_summarize_transcript
    except ImportError:
        print("‚ö†Ô∏è Kh√¥ng th·ªÉ import c√°c h√†m t·ª´ summary_generator")

def extract_audio(video_path, audio_path):
    """Tr√≠ch xu·∫•t √¢m thanh t·ª´ video"""
    print("ƒêang tr√≠ch xu·∫•t √¢m thanh...")
    video = mp.VideoFileClip(video_path)
    video.audio.write_audiofile(audio_path, verbose=False, logger=None)
    print("‚úÖ Tr√≠ch xu·∫•t √¢m thanh ho√†n t·∫•t")
    return audio_path

def filter_audio(input_audio_path, output_audio_path):
    """L·ªçc nhi·ªÖu √¢m thanh ƒë·ªÉ c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng nh·∫≠n d·∫°ng"""
    import subprocess
    import shutil
    
    print("ƒêang l·ªçc nhi·ªÖu √¢m thanh...")
    try:
        subprocess.run(["ffmpeg", "-version"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)
        
        cmd = ["ffmpeg", "-i", input_audio_path, "-af", "afftdn=nf=-25", "-y", output_audio_path]
        subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        print("‚úÖ L·ªçc nhi·ªÖu √¢m thanh th√†nh c√¥ng")
        return True
    except subprocess.CalledProcessError:
        print("‚ö†Ô∏è Kh√¥ng th·ªÉ l·ªçc √¢m thanh, ffmpeg kh√¥ng kh·∫£ d·ª•ng ho·∫∑c g·∫∑p l·ªói")
        shutil.copy(input_audio_path, output_audio_path)
        return False
    except Exception as e:
        print(f"‚ùå L·ªói khi l·ªçc √¢m thanh: {e}")
        shutil.copy(input_audio_path, output_audio_path)
        return False

def initialize_whisper_model(device, model_size="base"):
    """Kh·ªüi t·∫°o model Whisper ph√π h·ª£p v·ªõi ph·∫ßn c·ª©ng"""
    print(f"ƒêang kh·ªüi t·∫°o model Whisper tr√™n {device}...")
    
    if torch.cuda.is_available():
        # Ki·ªÉm tra b·ªô nh·ªõ GPU ƒë·ªÉ ch·ªçn k√≠ch th∆∞·ªõc model ph√π h·ª£p
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9  # GB
        
        if gpu_memory >= 8.0:
            whisper_model_size = "medium"
            print(f"üîç GPU c√≥ {gpu_memory:.1f}GB RAM, s·ª≠ d·ª•ng model Whisper {whisper_model_size}")
        else:
            whisper_model_size = "base"
            print(f"‚ö†Ô∏è GPU ch·ªâ c√≥ {gpu_memory:.1f}GB RAM, s·ª≠ d·ª•ng model Whisper {whisper_model_size}")
            
        try:
            # S·ª≠ d·ª•ng FP16 ƒë·ªÉ tƒÉng t·ªëc v√† ti·∫øt ki·ªám b·ªô nh·ªõ
            whisper_model = whisper.load_model(whisper_model_size, device=device)
            print("‚úÖ ƒê√£ kh·ªüi t·∫°o Whisper tr√™n GPU")
        except Exception as e:
            print(f"‚ùå L·ªói khi t·∫£i model Whisper: {e}")
            print("Th·ª≠ t·∫£i model nh·ªè h∆°n...")
            whisper_model = whisper.load_model("base", device=device)
    else:
        whisper_model = whisper.load_model("base", device="cpu")
        print("‚úÖ ƒê√£ kh·ªüi t·∫°o Whisper tr√™n CPU")
        
    return whisper_model

def transcribe_audio(audio_path, whisper_model):
    """Chuy·ªÉn ƒë·ªïi gi·ªçng n√≥i th√†nh vƒÉn b·∫£n b·∫±ng Whisper AI"""
    print(f"ƒêang chuy·ªÉn ƒë·ªïi √¢m thanh th√†nh vƒÉn b·∫£n (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)...")
    
    # Gi·∫£i ph√≥ng b·ªô nh·ªõ GPU tr∆∞·ªõc khi ch·∫°y Whisper
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        
    # S·ª≠ d·ª•ng FP16 ƒë·ªÉ t·ªëi ∆∞u h√≥a
    result = whisper_model.transcribe(audio_path, fp16=True)
    
    return result["text"]

def transcribe_audio_with_timestamps(audio_path, whisper_model):
    """Chuy·ªÉn ƒë·ªïi √¢m thanh th√†nh vƒÉn b·∫£n v·ªõi timestamp cho m·ªói c√¢u"""
    print(f"ƒêang chuy·ªÉn ƒë·ªïi √¢m thanh th√†nh vƒÉn b·∫£n v·ªõi timestamps...")
    
    # Gi·∫£i ph√≥ng b·ªô nh·ªõ GPU tr∆∞·ªõc khi ch·∫°y Whisper
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        
    try:
        # S·ª≠ d·ª•ng tham s·ªë word_timestamps=True ƒë·ªÉ l·∫•y timestamp cho m·ªói t·ª´
        result = whisper_model.transcribe(
            audio_path, 
            fp16=True,
            word_timestamps=True,
            verbose=False
        )
        
        # Tr·∫£ v·ªÅ to√†n b·ªô k·∫øt qu·∫£ bao g·ªìm c·∫£ segments
        return result
        
    except Exception as e:
        print(f"L·ªói khi transcribe v·ªõi timestamps: {e}")
        # Ph∆∞∆°ng ph√°p d·ª± ph√≤ng - kh√¥ng c√≥ timestamps
        result = whisper_model.transcribe(audio_path, fp16=True)
        return result

def transcribe_audio_optimized(audio_path, whisper_model=None, model_size="medium", chunk_length=30, has_pydub=True):
    """Ph∆∞∆°ng ph√°p t·ªëi ∆∞u ƒë·ªÉ chuy·ªÉn ƒë·ªïi √¢m thanh th√†nh vƒÉn b·∫£n"""
    print("B·∫Øt ƒë·∫ßu ph∆∞∆°ng ph√°p chuy·ªÉn ƒë·ªïi √¢m thanh t·ªëi ∆∞u...")
    
    # Lu√¥n th·ª≠ d√πng Faster-Whisper tr∆∞·ªõc n·∫øu c√≥ GPU
    if torch.cuda.is_available():
        try:
            print("üöÄ S·ª≠ d·ª•ng Faster-Whisper (GPU) ƒë·ªÉ tƒÉng t·ªëc 3-5 l·∫ßn...")
            result = transcribe_audio_faster(audio_path, language="auto")
            if result and result["text"]:
                return result["text"]
            print("Faster-Whisper kh√¥ng th√†nh c√¥ng, chuy·ªÉn sang ph∆∞∆°ng ph√°p song song...")
        except Exception as e:
            print(f"L·ªói khi d√πng Faster-Whisper: {e}")
    
    # Th·ª≠ d√πng ph∆∞∆°ng ph√°p song song th·ª© hai
    try:
        result = transcribe_audio_parallel(
            audio_path, 
            language="auto", 
            chunk_length_sec=15,
            max_workers=4
        )
        if result and result["text"]:
            return result["text"]
        print("Ph∆∞∆°ng ph√°p song song kh√¥ng th√†nh c√¥ng, chuy·ªÉn sang ph∆∞∆°ng ph√°p c≈©...")
    except Exception as e:
        print(f"L·ªói khi d√πng ph∆∞∆°ng ph√°p song song: {e}")
        print("Chuy·ªÉn sang ph∆∞∆°ng ph√°p c≈©...")
    
    # N·∫øu Faster-Whisper v√† ph∆∞∆°ng ph√°p song song kh√¥ng th√†nh c√¥ng, d√πng ph∆∞∆°ng ph√°p c≈©
    if not has_pydub:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y th∆∞ vi·ªán pydub, s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p th√¥ng th∆∞·ªùng...")
        if whisper_model is None:
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            whisper_model = whisper.load_model("base", device=device)
        return transcribe_audio(audio_path, whisper_model)
        
    from pydub import AudioSegment
    
    # T·∫°o th∆∞ m·ª•c t·∫°m ƒë·ªÉ l∆∞u c√°c file √¢m thanh ƒë√£ chia nh·ªè
    temp_dir = os.path.join('output_videos', 'temp_audio_chunks')
    os.makedirs(temp_dir, exist_ok=True)
    
    # Chia nh·ªè file √¢m thanh
    print(f"ƒêang chia nh·ªè file √¢m thanh th√†nh c√°c ƒëo·∫°n {chunk_length}s...")
    audio = AudioSegment.from_file(audio_path)
    duration_seconds = len(audio) / 1000
    chunks_count = int(duration_seconds / chunk_length) + 1
    
    # T·∫°o danh s√°ch ch·ª©a ƒë∆∞·ªùng d·∫´n c√°c file t·∫°m
    chunk_files = []
    
    # Chia nh·ªè v√† l∆∞u c√°c ƒëo·∫°n √¢m thanh
    for i in range(chunks_count):
        start_ms = i * chunk_length * 1000
        end_ms = min((i + 1) * chunk_length * 1000, len(audio))
        chunk = audio[start_ms:end_ms]
        
        # L∆∞u ƒëo·∫°n t·∫°m th·ªùi
        chunk_path = os.path.join(temp_dir, f"chunk_{i:03d}.wav")
        chunk.export(chunk_path, format="wav")
        chunk_files.append(chunk_path)
    
    # T·∫°o h√†m ƒë·ªÉ x·ª≠ l√Ω t·ª´ng ƒëo·∫°n
    def process_chunk(chunk_file):
        # Gi·∫£i ph√≥ng b·ªô nh·ªõ GPU n·∫øu c·∫ßn
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
        # S·ª≠ d·ª•ng m√¥ h√¨nh to√†n c·ª•c thay v√¨ t·∫°o m√¥ h√¨nh m·ªõi cho t·ª´ng ƒëo·∫°n
        if whisper_model is None:
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            local_model = whisper.load_model("base", device=device)
            result = local_model.transcribe(chunk_file, fp16=torch.cuda.is_available())
        else:
            result = whisper_model.transcribe(chunk_file, fp16=torch.cuda.is_available())
        
        # Gi·∫£i ph√≥ng b·ªô nh·ªõ
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
        return result["text"]
    
    # X·ª≠ l√Ω tu·∫ßn t·ª± t·∫•t c·∫£ c√°c ƒëo·∫°n audio
    print(f"X·ª≠ l√Ω {len(chunk_files)} ƒëo·∫°n √¢m thanh tu·∫ßn t·ª±...")
    results = []
    for chunk_file in tqdm(chunk_files, desc="Nh·∫≠n d·∫°ng gi·ªçng n√≥i"):
        results.append(process_chunk(chunk_file))
    
    # G·ªôp k·∫øt qu·∫£ v√† x·ª≠ l√Ω vƒÉn b·∫£n
    transcript = " ".join(results)
    
    # X√≥a c√°c file t·∫°m
    for chunk_file in chunk_files:
        os.remove(chunk_file)
        
    # X√≥a th∆∞ m·ª•c t·∫°m
    try:
        os.rmdir(temp_dir)
    except:
        pass
        
    return transcript

def generate_voice_over(text, output_path, lang='vi'):
    """Generate voice-over audio file from text using Google Text-to-Speech"""
    try:
        from gtts import gTTS
        print("ƒêang t·∫°o voice-over t·ª´ vƒÉn b·∫£n...")
        tts = gTTS(text=text, lang=lang)
        tts.save(output_path)
        print(f"‚úÖ ƒê√£ t·∫°o voice-over th√†nh c√¥ng: {output_path}")
        return True
    except ImportError:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y th∆∞ vi·ªán gtts, vui l√≤ng c√†i ƒë·∫∑t: pip install gtts")
        return False
    except Exception as e:
        print(f"‚ùå L·ªói khi t·∫°o voice-over: {e}")
        return False

def initialize_faster_whisper():
    """Kh·ªüi t·∫°o Faster Whisper model v·ªõi compute type ph√π h·ª£p"""
    try:
        from faster_whisper import WhisperModel
        
        print("Kh·ªüi t·∫°o Faster Whisper model...")
        # Ki·ªÉm tra GPU
        if torch.cuda.is_available():
            # Ki·ªÉm tra b·ªô nh·ªõ GPU ƒë·ªÉ ch·ªçn k√≠ch th∆∞·ªõc model ph√π h·ª£p
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9  # GB
            
            if gpu_memory >= 8.0:
                model_size = "medium"
            else:
                model_size = "small"
            
            # D√πng float32 thay v√¨ float16 cho GTX 1060 v√† c√°c GPU c≈© h∆°n
            try:
                # Th·ª≠ tr∆∞·ªõc v·ªõi float32
                print(f"üîç GPU c√≥ {gpu_memory:.1f}GB RAM, s·ª≠ d·ª•ng model {model_size} v·ªõi float32")
                model = WhisperModel(model_size, device="cuda", compute_type="float32")
                return model
            except Exception:
                # N·∫øu float32 th·∫•t b·∫°i, d√πng int8
                print(f"üîç GPU c√≥ {gpu_memory:.1f}GB RAM, s·ª≠ d·ª•ng model {model_size} v·ªõi int8")
                model = WhisperModel(model_size, device="cpu", compute_type="int8")
                return model
        else:
            print("‚ö†Ô∏è Kh√¥ng ph√°t hi·ªán GPU, s·ª≠ d·ª•ng CPU v·ªõi model small")
            model = WhisperModel("small", device="cpu", compute_type="int8")
            return model
    except ImportError:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y Faster Whisper, s·ª≠ d·ª•ng Whisper th√¥ng th∆∞·ªùng")
        return None
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi kh·ªüi t·∫°o Faster Whisper: {e}")
        return None

def transcribe_audio_faster(audio_path, language="auto"):
    """S·ª≠ d·ª•ng Faster-Whisper ƒë·ªÉ nh·∫≠n d·∫°ng - tƒÉng t·ªëc 3-5x"""
    try:
        model = initialize_faster_whisper()
        if model is None:
            # Fallback to standard whisper
            return None
            
        print(f"ƒêang chuy·ªÉn ƒë·ªïi √¢m thanh th√†nh vƒÉn b·∫£n v·ªõi Faster Whisper...")
        
        # Fix the language parameter issue - use None for auto detection instead of "auto"
        detect_language = None if language == "auto" else language
        
        segments, info = model.transcribe(audio_path, beam_size=5, language=detect_language)
        
        # Print the detected language
        print(f"ƒê√£ ph√°t hi·ªán ng√¥n ng·ªØ: {info.language} v·ªõi ƒë·ªô tin c·∫≠y {info.language_probability:.2f}")
        
        # Collect all segments
        result = {"text": "", "segments": []}
        for segment in segments:
            result["text"] += segment.text + " "
            result["segments"].append({
                "text": segment.text,
                "start": segment.start,
                "end": segment.end
            })
        
        return result
    except Exception as e:
        print(f"‚ùå L·ªói khi s·ª≠ d·ª•ng Faster Whisper: {e}")
        return None

def split_audio_into_chunks(audio_path, chunk_length_ms=15000, output_dir=None):
    """Chia file √¢m thanh th√†nh nhi·ªÅu ƒëo·∫°n nh·ªè ƒë·ªÉ x·ª≠ l√Ω song song"""
    from pydub import AudioSegment
    
    if output_dir is None:
        output_dir = os.path.join('output_videos', 'audio_chunks')
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"ƒêang chia audio th√†nh c√°c ƒëo·∫°n {chunk_length_ms/1000}s...")
    audio = AudioSegment.from_file(audio_path)
    
    # T√≠nh s·ªë l∆∞·ª£ng chunk
    chunk_files = []
    for i, start in enumerate(range(0, len(audio), chunk_length_ms)):
        end = min(start + chunk_length_ms, len(audio))
        chunk = audio[start:end]
        # M·ªü r·ªông chunk ƒë·ªÉ tr√°nh c·∫Øt gi·ªØa t·ª´ (n·∫øu kh√¥ng ph·∫£i chunk cu·ªëi)
        if end < len(audio):
            # Th√™m 500ms v√†o cu·ªëi m·ªói chunk ƒë·ªÉ ch·ªìng l·∫•p
            end = min(end + 500, len(audio))
            chunk = audio[start:end]
        chunk_file = os.path.join(output_dir, f"chunk_{i:03d}.wav")
        chunk.export(chunk_file, format="wav")
        chunk_files.append({
            'path': chunk_file,
            'start_ms': start,
            'end_ms': end,
            'index': i
        })
        
    return chunk_files

def process_audio_chunk(chunk_info, model, language="auto"):
    """X·ª≠ l√Ω m·ªôt chunk audio v·ªõi Faster Whisper"""
    try:
        print(f"X·ª≠ l√Ω chunk {chunk_info['index']}...")
        
        if isinstance(model, str) and model == "standard_whisper":
            # S·ª≠ d·ª•ng Whisper th∆∞·ªùng
            import whisper
            whisper_model = whisper.load_model("base")
            result = whisper_model.transcribe(chunk_info['path'], language=language)
            text = result["text"]
            return {
                'text': text,
                'start_time': chunk_info['start_ms'] / 1000,
                'index': chunk_info['index']
            }
        else:
            # S·ª≠ d·ª•ng Faster Whisper
            segments, _ = model.transcribe(
                chunk_info['path'], 
                beam_size=5,
                language=language,  # Use None instead of "auto" for Faster Whisper
                word_timestamps=True
            )
            
            # Collect results from this chunk
            text_segments = []
            for segment in segments:
                segment_dict = {
                    'text': segment.text,
                    'start': segment.start + chunk_info['start_ms']/1000,
                    'end': segment.end + chunk_info['start_ms']/1000,
                    'words': []
                }
                # Add word timestamps if available
                if hasattr(segment, 'words') and segment.words:
                    for word in segment.words:
                        word_dict = {
                            'word': word.word,
                            'start': word.start + chunk_info['start_ms']/1000,
                            'end': word.end + chunk_info['start_ms']/1000,
                            'probability': word.probability
                        }
                        segment_dict['words'].append(word_dict)
                text_segments.append(segment_dict)
            
            # Combine text from all segments
            full_text = " ".join([s['text'] for s in text_segments])
            return {
                'text': full_text,
                'segments': text_segments,
                'start_time': chunk_info['start_ms'] / 1000,
                'index': chunk_info['index']
            }
    except Exception as e:
        print(f"L·ªói khi x·ª≠ l√Ω chunk {chunk_info['index']}: {e}")
        return {
            'text': "",
            'start_time': chunk_info['start_ms'] / 1000,
            'index': chunk_info['index'],
            'error': str(e)
        }

def transcribe_audio_parallel(audio_path, language="auto", chunk_length_sec=15, max_workers=4):
    """Chuy·ªÉn ƒë·ªïi √¢m thanh th√†nh vƒÉn b·∫£n nhanh h∆°n v·ªõi x·ª≠ l√Ω song song"""
    print("üöÄ ƒêang chuy·ªÉn ƒë·ªïi √¢m thanh th√†nh vƒÉn b·∫£n v·ªõi x·ª≠ l√Ω song song...")
    
    # T·∫°o model Faster Whisper ho·∫∑c s·ª≠ d·ª•ng Whisper th∆∞·ªùng
    model = initialize_faster_whisper()
    if model is None:
        model = "standard_whisper"
        print("S·ª≠ d·ª•ng Whisper ti√™u chu·∫©n ƒë·ªÉ x·ª≠ l√Ω song song c√°c chunk")
    
    # Chia nh·ªè file √¢m thanh
    chunks = split_audio_into_chunks(audio_path, chunk_length_ms=chunk_length_sec*1000)
    print(f"ƒê√£ chia th√†nh {len(chunks)} chunk, m·ªói chunk d√†i {chunk_length_sec}s")
    
    # ƒêi·ªÅu ch·ªânh s·ªë workers ƒë·ªÉ kh√¥ng qu√° t·∫£i system
    max_workers = min(max_workers, len(chunks), os.cpu_count())
    print(f"S·ª≠ d·ª•ng {max_workers} workers ƒë·ªÉ x·ª≠ l√Ω song song v·ªõi ng√¥n ng·ªØ={language}")
    
    # X·ª≠ l√Ω song song c√°c chunk
    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Use partial to ensure we pass the correct language parameter
        import functools
        process_chunk_func = functools.partial(process_audio_chunk, model=model, language=language)
        
        future_to_chunk = {
            executor.submit(process_chunk_func, chunk): chunk
            for chunk in chunks
        }
        
        for future in tqdm(concurrent.futures.as_completed(future_to_chunk), 
                          total=len(chunks), 
                          desc="X·ª≠ l√Ω c√°c chunk √¢m thanh"):
            chunk = future_to_chunk[future]
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                print(f"Chunk {chunk['index']} g·∫∑p l·ªói: {e}")
    
    # S·∫Øp x·∫øp k·∫øt qu·∫£ theo th·ª© t·ª± chunk
    results.sort(key=lambda x: x['index'])
    
    # G·ªôp c√°c k·∫øt qu·∫£
    full_transcript = ""
    all_segments = []
    for result in results:
        if 'error' not in result:
            full_transcript += result['text'] + " "
            if 'segments' in result:
                all_segments.extend(result['segments'])
    
    # Clean up temporary files
    for chunk in chunks:
        try:
            os.remove(chunk['path'])
        except:
            pass
    
    # X√≥a th∆∞ m·ª•c t·∫°m n·∫øu tr·ªëng
    try:
        chunk_dir = os.path.dirname(chunks[0]['path'])
        if not os.listdir(chunk_dir):
            os.rmdir(chunk_dir)
    except:
        pass
    
    # T·∫°o ƒë·ªëi t∆∞·ª£ng k·∫øt qu·∫£ t∆∞∆°ng th√≠ch v·ªõi whisper
    final_result = {
        "text": full_transcript.strip(),
        "segments": all_segments
    }
    
    return final_result

# Th√™m h√†m clean_transcript
def clean_transcript(text):
    """L√†m s·∫°ch transcript, lo·∫°i b·ªè c√°c k√Ω t·ª± v√† t·ª´ ng·ªØ kh√¥ng c·∫ßn thi·∫øt"""
    if not text:
        return ""
    
    # Chuy·ªÉn ƒë·ªïi xu·ªëng d√≤ng th√†nh d·∫•u c√°ch
    text = re.sub(r'\n+', ' ', text)
    
    # Lo·∫°i b·ªè d·∫•u c√°ch th·ª´a
    text = re.sub(r'\s+', ' ', text)
    
    # Lo·∫°i b·ªè c√°c c·ª•m t·ª´ kh√¥ng c·∫ßn thi·∫øt (th∆∞·ªùng t·ª´ Whisper)
    unnecessary_phrases = [
        r'\[Music\]', r'\[music\]', r'\[√Çm nh·∫°c\]', r'\[√¢m nh·∫°c\]',
        r'\[Applause\]', r'\[applause\]', r'\[Ti·∫øng v·ªó tay\]',
        r'\[Background noise\]', r'\[Ti·∫øng ·ªìn n·ªÅn\]',
        r'\[Laughter\]', r'\[laughter\]', r'\[Ti·∫øng c∆∞·ªùi\]'
    ]
    for phrase in unnecessary_phrases:
        text = re.sub(phrase, '', text)
    
    # Lo·∫°i b·ªè d·∫•u ngo·∫∑c vu√¥ng v√† n·ªôi dung b√™n trong [...]
    text = re.sub(r'\[.*?\]', '', text)
    
    # Lo·∫°i b·ªè l·∫°i d·∫•u c√°ch th·ª´a sau khi ƒë√£ x√≥a n·ªôi dung kh√¥ng c·∫ßn thi·∫øt
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

# Th√™m ph∆∞∆°ng ph√°p d·ª± ph√≤ng cho c√°c h√†m c√≥ th·ªÉ kh√¥ng t·ªìn t·∫°i
def dummy_filter_irrelevant_content(text):
    """Ph∆∞∆°ng ph√°p d·ª± ph√≤ng khi kh√¥ng import ƒë∆∞·ª£c t·ª´ summary_generator"""
    return text

def dummy_enhanced_summarize_transcript(text, ratio=0.3):
    """Ph∆∞∆°ng ph√°p d·ª± ph√≤ng khi kh√¥ng import ƒë∆∞·ª£c t·ª´ summary_generator"""
    lines = text.split('.')
    num_lines = max(1, int(len(lines) * ratio))
    return '. '.join(lines[:num_lines]) + ('.' if not lines[0].endswith('.') else '')

# S·ª≠a h√†m process_review_mode_with_timeout
def process_review_mode_with_timeout(input_path, output_path, device, model=None, ratio=0.3, timeout=300,
                                   use_faster_whisper=True, use_cuda=True):
    """Phi√™n b·∫£n process_review_mode v·ªõi timeout ƒë·ªÉ tr√°nh x·ª≠ l√Ω qu√° l√¢u"""
    import signal
    
    # Import c√°c h√†m c·∫ßn thi·∫øt t·ª´ c√°c module kh√°c
    import_from_modules()
    
    # ƒê·ªãnh nghƒ©a c√°c h√†m d·ª± ph√≤ng n·∫øu c·∫ßn
    _filter_irrelevant_content = filter_irrelevant_content if 'filter_irrelevant_content' in globals() else dummy_filter_irrelevant_content
    _enhanced_summarize_transcript = enhanced_summarize_transcript if 'enhanced_summarize_transcript' in globals() else dummy_enhanced_summarize_transcript
    
    # H√†m x·ª≠ l√Ω khi timeout
    def timeout_handler(signum, frame):
        raise TimeoutError("Qu√° th·ªùi gian x·ª≠ l√Ω")
    
    # Thi·∫øt l·∫≠p timeout
    signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(timeout)
    
    try:
        # Ph·∫ßn 1: X·ª≠ l√Ω audio (c·∫ßn thi·∫øt)
        raw_audio_path = os.path.join('output_videos', 'temp_audio_raw.wav')
        filtered_audio_path = os.path.join('output_videos', 'temp_audio.wav')
        extract_audio(input_path, raw_audio_path)
        filter_audio(raw_audio_path, filtered_audio_path)
        
        # S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p song song ƒë·ªÉ chuy·ªÉn ƒë·ªïi √¢m thanh nhanh h∆°n
        print("üöÄ S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p chuy·ªÉn ƒë·ªïi √¢m thanh nhanh...")
        transcript = ""
        
        if use_faster_whisper:
            # D√πng ph∆∞∆°ng ph√°p x·ª≠ l√Ω v·ªõi Faster-Whisper
            try:
                result = transcribe_audio_faster(filtered_audio_path)
                if result and result["text"]:
                    transcript = result["text"]
                    transcript = clean_transcript(transcript)
                    print("‚úÖ Chuy·ªÉn ƒë·ªïi √¢m thanh th√†nh c√¥ng v·ªõi Faster-Whisper")
                else:
                    raise Exception("Faster-Whisper kh√¥ng tr·∫£ v·ªÅ k·∫øt qu·∫£")
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói khi d√πng Faster-Whisper: {e}")
                use_faster_whisper = False
            
        if not transcript and not use_faster_whisper:
            # Th·ª≠ v·ªõi ph∆∞∆°ng ph√°p song song
            try:
                result = transcribe_audio_parallel(
                    filtered_audio_path, 
                    chunk_length_sec=15,
                    max_workers=4
                )
                if result and result["text"]:
                    transcript = result["text"]
                    transcript = clean_transcript(transcript)
                    print("‚úÖ Chuy·ªÉn ƒë·ªïi √¢m thanh th√†nh c√¥ng v·ªõi ph∆∞∆°ng ph√°p song song")
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói khi d√πng ph∆∞∆°ng ph√°p song song: {e}")
                # Kh·ªüi t·∫°o model Whisper v√† fallback
                print("Chuy·ªÉn sang whisper th√¥ng th∆∞·ªùng...")
                whisper_model = initialize_whisper_model(device)
                transcript = transcribe_audio_optimized(filtered_audio_path, whisper_model)
                transcript = clean_transcript(transcript)
            
        # T√≥m t·∫Øt transcript
        if transcript and len(transcript.split()) >= 10:
            filtered_transcript = _filter_irrelevant_content(transcript)
            summary = _enhanced_summarize_transcript(filtered_transcript, ratio)
        else:
            summary = "Kh√¥ng ph√°t hi·ªán l·ªùi tho·∫°i c√≥ nghƒ©a trong video."
            
        # Reset timeout cho ph·∫ßn 2
        signal.alarm(timeout)
            
        # Ph·∫ßn 2: Ph√¢n t√≠ch video (th·ª≠ n·∫øu c√≤n th·ªùi gian)
        try:
            segments = []
            # S·ª≠ d·ª•ng phi√™n b·∫£n nhanh ƒë·ªÉ ph√°t hi·ªán chuy·ªÉn ƒë·ªông
            if use_cuda and 'fast_motion_detection_with_decord' in globals():
                print("üöÄ S·ª≠ d·ª•ng GPU ƒë·ªÉ ph√°t hi·ªán chuy·ªÉn ƒë·ªông...")
                segments = fast_motion_detection_with_decord(input_path)
            elif 'fast_motion_detection' in globals():
                segments = fast_motion_detection(input_path)
            else:
                print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y h√†m ph√°t hi·ªán chuy·ªÉn ƒë·ªông")
            
            # Th√™m th√¥ng tin v·ªÅ c√°c ƒëo·∫°n chuy·ªÉn ƒë·ªông v√†o summary
            if segments:
                summary += "\n\n## C√°c ƒëo·∫°n c√≥ chuy·ªÉn ƒë·ªông m·∫°nh:\n"
                for i, (start, end) in enumerate(segments):
                    summary += f"- ƒêo·∫°n {i+1}: {start:.1f}s - {end:.1f}s (th·ªùi l∆∞·ª£ng: {end-start:.1f}s)\n"
                    
            # T·∫°o video highlight n·∫øu c√≥ segments
            if segments and model is not None:
                highlight_path = os.path.join('output_videos', 'review_highlights.mp4')
                extracted = False
                if use_cuda and 'fast_extract_segments_cuda' in globals():
                    extracted = fast_extract_segments_cuda(input_path, highlight_path, segments)
                elif 'fast_extract_segments' in globals():
                    extracted = fast_extract_segments(input_path, highlight_path, segments)
                if extracted:
                    print(f"‚úÖ ƒê√£ t·∫°o video highlight: {highlight_path}")
                else:
                    print("‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫°o video highlight")
                    
        except Exception as e:
            print(f"‚ö†Ô∏è Ph√¢n t√≠ch video timeout ho·∫∑c g·∫∑p l·ªói: {e}")
        
        # L∆∞u b·∫£n t√≥m t·∫Øt
        summary_path = os.path.join('output_videos', 'review_summary.txt')
        with open(summary_path, "w", encoding="utf-8") as f:
            f.write(summary)
        
        # T·∫°o voice-over
        voice_over_path = os.path.join('output_videos', 'review_voiceover.mp3')
        generate_voice_over(summary, voice_over_path)
        
        # D·ªçn d·∫πp
        if os.path.exists(raw_audio_path):
            os.remove(raw_audio_path)
        if os.path.exists(filtered_audio_path):
            os.remove(filtered_audio_path)
            
        return summary, voice_over_path
    
    except TimeoutError:
        print("‚ö†Ô∏è Qu√° th·ªùi gian x·ª≠ l√Ω, tr·∫£ v·ªÅ k·∫øt qu·∫£ ƒë√£ c√≥")
        # Tr·∫£ v·ªÅ k·∫øt qu·∫£ t·∫°m th·ªùi n·∫øu c√≥
        summary_path = os.path.join('output_videos', 'review_summary.txt')
        if os.path.exists(summary_path):
            with open(summary_path, "r", encoding="utf-8") as f:
                summary = f.read()
        else:
            summary = "X·ª≠ l√Ω timeout, kh√¥ng th·ªÉ t·∫°o t√≥m t·∫Øt ƒë·∫ßy ƒë·ªß."
            with open(summary_path, "w", encoding="utf-8") as f:
                f.write(summary)
        voice_over_path = os.path.join('output_videos', 'review_voiceover.mp3')
        if not os.path.exists(voice_over_path):
            generate_voice_over(summary, voice_over_path)
            
        return summary, voice_over_path
    
    except Exception as e:
        print(f"‚ùå L·ªói trong qu√° tr√¨nh x·ª≠ l√Ω: {e}")
        summary = f"ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh x·ª≠ l√Ω: {e}"
        summary_path = os.path.join('output_videos', 'review_summary.txt')
        with open(summary_path, "w", encoding="utf-8") as f:
            f.write(summary)
        return summary, None
        
    finally:
        # Reset alarm
        signal.alarm(0)
