import cv2  # Import n√†y thi·∫øu trong file
import torch
import os
import numpy as np
import subprocess
from moviepy.editor import VideoFileClip, concatenate_videoclips
from tqdm import tqdm
import multiprocessing
from functools import partial

try:
    import decord
    from decord import VideoReader, cpu, gpu
    DECORD_AVAILABLE = True
    
    # Bu·ªôc s·ª≠ d·ª•ng GPU n·∫øu c√≥
    if torch.cuda.is_available():
        try:
            decord.bridge.set_bridge('torch')
            print("‚úÖ Decord s·∫Ω s·ª≠ d·ª•ng GPU v·ªõi PyTorch bridge")
            # Kh·ªüi t·∫°o GPU context s·ªõm ƒë·ªÉ ki·ªÉm tra
            test_ctx = decord.gpu(0)
            print("‚úÖ Decord GPU context kh·ªüi t·∫°o th√†nh c√¥ng")
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ kh·ªüi t·∫°o Decord GPU: {e}")
            decord.bridge.set_bridge('native')
            print("‚ö†Ô∏è Fallback to native bridge")
    else:
        decord.bridge.set_bridge('native')
        print("‚úÖ Decord s·∫Ω s·ª≠ d·ª•ng CPU v·ªõi native bridge")
except ImportError:
    DECORD_AVAILABLE = False
    print("‚ö†Ô∏è Decord library not found. Using slower OpenCV for frame extraction.")

def detect_objects(model, frame, frame_idx, device, use_opencv_cuda=False):
    """Nh·∫≠n di·ªán v·∫≠t th·ªÉ trong frame."""
    # Chuy·ªÉn ƒë·ªïi m√†u v√† resize
    if isinstance(frame, np.ndarray):
        if use_opencv_cuda:
            # S·ª≠ d·ª•ng OpenCV CUDA
            gpu_frame = cv2.cuda_GpuMat()
            gpu_frame.upload(frame)
            gpu_frame_resized = cv2.cuda.resize(gpu_frame, (640, 640))
            frame_resized = gpu_frame_resized.download()
        else:
            frame_resized = cv2.resize(frame, (640, 640))
    else:
        frame = np.array(frame)
        if use_opencv_cuda:
            gpu_frame = cv2.cuda_GpuMat()
            gpu_frame.upload(frame)
            gpu_frame_resized = cv2.cuda.resize(gpu_frame, (640, 640))
            frame_resized = gpu_frame_resized.download()
        else:
            frame_resized = cv2.resize(frame, (640, 640))
    
    # Th·ª±c hi·ªán ph√°t hi·ªán v·ªõi batch processing ƒë·ªÉ t·ªëi ∆∞u h√≥a GPU model
    results = model(frame_resized)
    detections = results.xyxy[0].cpu().numpy()  # Lu√¥n chuy·ªÉn v·ªÅ CPU ƒë·ªÉ x·ª≠ l√Ω numpy
    
    for detection in detections:
        # Ch·ªâ l·∫•y ng∆∞·ªùi (class 0 trong COCO dataset)
        if int(detection[5]) == 0 and float(detection[4]) > 0.4:  # Th√™m ng∆∞·ª°ng tin c·∫≠y
            return (True, frame_idx, float(detection[4]))  # Tr·∫£ v·ªÅ tuple v·ªõi ƒëi·ªÉm tin c·∫≠y
    return (False, frame_idx, 0.0)

def detect_objects_optimized(model, frames, device, batch_size=8):
    """Process multiple frames at once for efficient object detection"""
    results = []
    
    # Process frames in batches to make better use of the GPU
    for i in range(0, len(frames), batch_size):
        batch = frames[i:i+batch_size]
        
        # Convert frames to format expected by YOLO
        processed_batch = []
        for frame in batch:
            # Resize to expected input size
            resized = cv2.resize(frame, (640, 640))
            processed_batch.append(resized)
        
        # Run detection on batch
        batch_results = model(processed_batch)
        
        # Extract detection results
        for j, detections in enumerate(batch_results.xyxy):
            frame_idx = i + j
            has_person = False
            confidence = 0.0
            
            for det in detections:
                # Check if detection is a person (class 0 in COCO)
                if int(det[5]) == 0 and float(det[4]) > 0.4:
                    has_person = True
                    confidence = max(confidence, float(det[4]))
            
            results.append((has_person, frame_idx, confidence))
    
    return results

def detect_motion(prev_frame, current_frame, threshold=2.0):
    """Ph√°t hi·ªán chuy·ªÉn ƒë·ªông gi·ªØa hai frame li√™n ti·∫øp"""
    if prev_frame is None or current_frame is None:
        return False, 0
        
    try:
        # Chuy·ªÉn ƒë·ªïi sang grayscale
        prev_gray = cv2.cvtColor(np.array(prev_frame).astype('uint8'), cv2.COLOR_RGB2GRAY)
        curr_gray = cv2.cvtColor(np.array(current_frame).astype('uint8'), cv2.COLOR_RGB2GRAY)
        
        # T√≠nh to√°n optical flow
        flow = cv2.calcOpticalFlowFarneback(prev_gray, curr_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
        
        # T√≠nh magnitude v√† angle
        mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        motion_magnitude = np.mean(mag)
        
        # Tr·∫£ v·ªÅ True n·∫øu chuy·ªÉn ƒë·ªông ƒë·ªß l·ªõn
        return motion_magnitude > threshold, motion_magnitude
    except Exception as e:
        print(f"L·ªói khi ph√°t hi·ªán chuy·ªÉn ƒë·ªông: {e}")
        return False, 0

def process_frame(t, clip, batch_start, model, device, use_opencv_cuda=False):
    """X·ª≠ l√Ω m·ªôt frame trong video"""
    try:
        frame = clip.get_frame(t)
        result = detect_objects(model, frame, t, device, use_opencv_cuda)
        return result
    except Exception as e:
        print(f"L·ªói khi x·ª≠ l√Ω frame t·∫°i gi√¢y {t}: {e}")
        return (False, t, 0.0)

def extract_important_clips(input_path, output_path, model, device, buffer_time=3, sample_rate=1):
    """C·∫Øt ƒëo·∫°n video c√≥ v·∫≠t th·ªÉ quan tr·ªçng v√† chuy·ªÉn ƒë·ªông."""
    # Check if we can use the optimized version with Decord
    if DECORD_AVAILABLE and device.type == 'cuda':
        print("üöÄ S·ª≠ d·ª•ng Decord ƒë·ªÉ tr√≠ch xu·∫•t frame nhanh h∆°n")
        return extract_important_clips_optimized(input_path, output_path, model, device, buffer_time, sample_rate)
    
    clip = VideoFileClip(input_path)
    fps = clip.fps
    duration = clip.duration
    important_times = []
    
    print("B·∫Øt ƒë·∫ßu qu√©t video ƒë·ªÉ t√¨m khung h√¨nh c√≥ ng∆∞·ªùi v√† chuy·ªÉn ƒë·ªông...")
    total_seconds = int(duration)
    
    # X·ª≠ l√Ω theo ƒëo·∫°n ƒë·ªÉ gi·∫£m t·∫£i b·ªô nh·ªõ GPU
    batch_size = min(300, total_seconds)  # X·ª≠ l√Ω t·ªëi ƒëa 5 ph√∫t m·ªói l·∫ßn
    
    # X√°c ƒë·ªãnh s·ªë l∆∞·ª£ng worker cho multiprocessing
    num_workers = min(os.cpu_count(), 4) if torch.cuda.is_available() else max(1, os.cpu_count() - 1)
    print(f"S·ª≠ d·ª•ng {num_workers} worker cho x·ª≠ l√Ω song song")
    
    # X√°c ƒë·ªãnh c√≥ s·ª≠ d·ª•ng OpenCV CUDA kh√¥ng
    use_opencv_cuda = False
    if device.type == 'cuda':
        try:
            cv2.cuda.getCudaEnabledDeviceCount()
            use_opencv_cuda = True
            print("‚úÖ K√≠ch ho·∫°t OpenCV CUDA ƒë·ªÉ tƒÉng t·ªëc x·ª≠ l√Ω h√¨nh ·∫£nh")
        except:
            print("‚ö†Ô∏è OpenCV CUDA kh√¥ng kh·∫£ d·ª•ng")
    
    # Th√¥ng tin ph√°t hi·ªán ng∆∞·ªùi v√† chuy·ªÉn ƒë·ªông
    person_detections = []
    motion_scores = []
    previous_frame = None
    
    for batch_start in range(0, total_seconds, batch_size):
        batch_end = min(batch_start + batch_size, total_seconds)
        print(f"X·ª≠ l√Ω ƒëo·∫°n: {batch_start}s - {batch_end}s")
        
        # Gi·∫£i ph√≥ng b·ªô nh·ªõ GPU n·∫øu c·∫ßn
        if device.type == 'cuda':
            torch.cuda.empty_cache()
            
        # Chu·∫©n b·ªã danh s√°ch th·ªùi ƒëi·ªÉm c·∫ßn x·ª≠ l√Ω
        times_to_process = list(range(batch_start, batch_end, sample_rate))
                
        # S·ª≠ d·ª•ng multiprocessing n·∫øu kh√¥ng d√πng GPU ho·∫∑c n·∫øu d√πng GPU v·ªõi x·ª≠ l√Ω song song
        if device.type == 'cpu' or (device.type == 'cuda' and torch.cuda.device_count() > 1):
            # X·ª≠ l√Ω song song v·ªõi multiprocessing
            with multiprocessing.Pool(processes=num_workers) as pool:
                process_func = partial(process_frame, clip=clip, batch_start=batch_start, 
                                      model=model, device=device, use_opencv_cuda=False)
                results = list(tqdm(
                    pool.imap(process_func, times_to_process),
                    total=len(times_to_process),
                    desc="X·ª≠ l√Ω video"
                ))
                
                # Thu th·∫≠p k·∫øt qu·∫£
                for has_person, frame_idx, confidence in results:
                    # L∆∞u th√¥ng tin ph√°t hi·ªán ng∆∞·ªùi
                    person_detections.append((frame_idx, has_person, confidence))
        else:
            # X·ª≠ l√Ω tu·∫ßn t·ª± khi d√πng GPU
            for t in tqdm(times_to_process, desc="X·ª≠ l√Ω video"):
                try:
                    frame = clip.get_frame(t)
                    
                    # Ph√°t hi·ªán ng∆∞·ªùi trong frame v·ªõi GPU acceleration
                    has_person, _, confidence = detect_objects(model, frame, t, device, use_opencv_cuda)
                    person_detections.append((t, has_person, confidence))
                    
                    # T√≠nh to√°n chuy·ªÉn ƒë·ªông gi·ªØa c√°c frame li√™n ti·∫øp
                    if previous_frame is not None:
                        has_motion, motion_magnitude = detect_motion(previous_frame, frame)
                        if has_motion:
                            motion_scores.append((t, motion_magnitude))
                    
                    previous_frame = frame
                    
                except Exception as e:
                    print(f"L·ªói khi x·ª≠ l√Ω frame t·∫°i gi√¢y {t}: {e}")
    
    # Ph√¢n t√≠ch k·∫øt qu·∫£ ph√°t hi·ªán ng∆∞·ªùi
    important_segments = []
    current_segment = None
    
    # Ng∆∞·ª°ng ph√°t hi·ªán chuy·ªÉn ƒë·ªông
    motion_threshold = 2.0 if motion_scores else 0
    if motion_scores:
        # T√≠nh ng∆∞·ª°ng th√≠ch ·ª©ng: 80% c·ªßa trung v·ªã
        motion_values = [score for _, score in motion_scores]
        motion_threshold = np.median(motion_values) * 0.8
        print(f"Ng∆∞·ª°ng chuy·ªÉn ƒë·ªông t·ª± ƒë·ªông: {motion_threshold:.2f}")
    
    # S·∫Øp x·∫øp c√°c ph√°t hi·ªán theo th·ªùi gian
    person_detections.sort(key=lambda x: x[0])
    
    # T·∫°o c√°c ƒëo·∫°n c√≥ ng∆∞·ªùi xu·∫•t hi·ªán
    for t, has_person, confidence in person_detections:
        if has_person:
            # Ki·ªÉm tra m·ª©c ƒë·ªô chuy·ªÉn ƒë·ªông n·∫øu c√≥ d·ªØ li·ªáu
            has_motion = True
            if motion_scores:
                # T√¨m ƒëi·ªÉm chuy·ªÉn ƒë·ªông g·∫ßn nh·∫•t v·ªõi th·ªùi ƒëi·ªÉm t
                nearest_motion = min(motion_scores, key=lambda x: abs(x[0] - t))
                if nearest_motion[1] < motion_threshold:
                    has_motion = False
            
            # N·∫øu c√≥ ng∆∞·ªùi v√† c√≥ chuy·ªÉn ƒë·ªông ƒë·ªß l·ªõn
            if has_motion:
                if current_segment is None:
                    current_segment = [t, t]
                else:
                    # M·ªü r·ªông ƒëo·∫°n hi·ªán t·∫°i
                    current_segment[1] = t
            
        elif current_segment is not None:
            # K·∫øt th√∫c ƒëo·∫°n hi·ªán t·∫°i
            if current_segment[1] - current_segment[0] >= 2:  # ƒêo·∫°n ph·∫£i d√†i √≠t nh·∫•t 2 gi√¢y
                important_segments.append(current_segment.copy())
            current_segment = None
    
    # Th√™m ƒëo·∫°n cu·ªëi n·∫øu c√≤n
    if current_segment is not None and current_segment[1] - current_segment[0] >= 2:
        important_segments.append(current_segment)
    
    # Gh√©p c√°c ƒëo·∫°n g·∫ßn nhau th√¥ng minh h∆°n
    merged_segments = []
    max_merge_gap = 5  # Kho·∫£ng c√°ch t·ªëi ƒëa ƒë·ªÉ gh√©p (gi√¢y)
    
    if important_segments:
        # S·∫Øp x·∫øp theo th·ªùi gian b·∫Øt ƒë·∫ßu
        important_segments.sort(key=lambda x: x[0])
        
        current_merged = [max(0, important_segments[0][0] - buffer_time), 
                         min(duration, important_segments[0][1] + buffer_time)]
        
        for i in range(1, len(important_segments)):
            segment_start = max(0, important_segments[i][0] - buffer_time)
            segment_end = min(duration, important_segments[i][1] + buffer_time)
            
            # N·∫øu ƒëo·∫°n hi·ªán t·∫°i g·∫ßn v·ªõi ƒëo·∫°n ƒë√£ gh√©p
            if segment_start - current_merged[1] <= max_merge_gap:
                # Gh√©p ƒëo·∫°n n·∫øu kho·∫£ng c√°ch gi·ªØa ch√∫ng kh√¥ng qu√° l·ªõn
                current_merged[1] = segment_end
            else:
                # T√≠nh to√°n ƒë·ªô d√†i ƒëo·∫°n ƒë√£ gh√©p
                merged_duration = current_merged[1] - current_merged[0]
                
                # Ch·ªâ gi·ªØ l·∫°i ƒëo·∫°n ƒë·ªß d√†i v√† c√≥ ƒë·ªß chuy·ªÉn ƒë·ªông
                if merged_duration >= 3:  # ƒêo·∫°n gh√©p ph·∫£i d√†i √≠t nh·∫•t 3 gi√¢y
                    merged_segments.append(current_merged.copy())
                
                # B·∫Øt ƒë·∫ßu ƒëo·∫°n gh√©p m·ªõi
                current_merged = [segment_start, segment_end]
        
        # X·ª≠ l√Ω ƒëo·∫°n cu·ªëi c√πng
        final_duration = current_merged[1] - current_merged[0]
        if final_duration >= 3:
            merged_segments.append(current_merged)
    
    print(f"T√¨m th·∫•y {len(merged_segments)} ƒëo·∫°n quan tr·ªçng, ƒëang xu·∫•t video...")
    
    # Th√™m ph√¢n t√≠ch xem c√°c ƒëo·∫°n c√≥ chi·∫øm bao nhi√™u % t·ªïng th·ªùi gian
    if merged_segments:
        total_selected_duration = sum(end - start for start, end in merged_segments)
        percentage = (total_selected_duration / duration) * 100
        print(f"T·ªïng th·ªùi gian ƒë∆∞·ª£c ch·ªçn: {total_selected_duration:.1f}s ({percentage:.1f}% c·ªßa video g·ªëc)")
    
    # Xu·∫•t video
    if merged_segments:
        final_clips = []
        for i, (start, end) in enumerate(merged_segments):
            print(f"ƒêo·∫°n {i+1}/{len(merged_segments)}: {start:.1f}s - {end:.1f}s (ƒë·ªô d√†i: {end-start:.1f}s)")
            final_clips.append(clip.subclip(start, end))
        
        print("ƒêang xu·∫•t file video...")
        # D√πng threads ƒë·ªÉ tƒÉng t·ªëc xu·∫•t file
        final_video = concatenate_videoclips(final_clips)
        final_video.write_videofile(output_path, codec="libx264", fps=clip.fps, threads=min(4, os.cpu_count()))
        print(f"‚úÖ Xu·∫•t video th√†nh c√¥ng! File: {output_path}")
        return True
    else:
        print("‚õî Kh√¥ng c√≥ c·∫£nh quan tr·ªçng n√†o, kh√¥ng xu·∫•t video.")
        return False

def extract_important_clips_optimized(input_path, output_path, model, device, buffer_time=3, sample_rate=1):
    """Optimized version of extract_important_clips using Decord for frame extraction"""
    print("üöÄ Using accelerated frame extraction for object detection")
    
    if DECORD_AVAILABLE:
        # Use GPU if available
        ctx = decord.gpu(0) if torch.cuda.is_available() else decord.cpu()
        try:
            vr = decord.VideoReader(input_path, ctx=ctx)
            
            fps = vr.get_avg_fps()
            frame_count = len(vr)
            duration = frame_count / fps
            
            # Sample frames at the specified rate (default: 1 frame per second)
            frame_indices = list(range(0, frame_count, int(fps * sample_rate)))
            
            print(f"Analyzing {len(frame_indices)} frames out of {frame_count}...")
            
            # Extract frames in batches
            batch_size = 16
            all_frames = []
            
            for i in tqdm(range(0, len(frame_indices), batch_size), desc="Extracting frames"):
                batch_indices = frame_indices[i:i+batch_size]
                # Get frames directly as torch tensors
                if torch.cuda.is_available():
                    batch_frames = vr.get_batch(batch_indices).cuda()
                else:
                    batch_frames = vr.get_batch(batch_indices)
                    
                # Convert from RGB to BGR for OpenCV compatibility
                for j in range(len(batch_frames)):
                    frame = batch_frames[j].cpu().numpy()
                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                    all_frames.append(frame)
                
                # Free up memory
                del batch_frames
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # Process frames for object detection
            detection_results = detect_objects_optimized(model, all_frames, device)
            
            # Process detection results
            important_indices = []
            for has_person, frame_idx, confidence in detection_results:
                if has_person:
                    important_indices.append(frame_idx)
            
            # Convert frame indices to timestamps
            important_times = [frame_indices[idx] / fps for idx in important_indices]
            
            # Group adjacent frames into segments
            segments = []
            if important_times:
                start_time = important_times[0]
                prev_time = important_times[0]
                
                for t in important_times[1:]:
                    # If gap is too big, start new segment
                    if t - prev_time > 2.0:  # 2 second gap
                        segments.append([start_time, prev_time])
                        start_time = t
                    prev_time = t
                
                # Add the last segment
                segments.append([start_time, prev_time])
            
            # Add buffer and merge close segments
            merged_segments = []
            if segments:
                for start, end in segments:
                    start_with_buffer = max(0, start - buffer_time)
                    end_with_buffer = min(duration, end + buffer_time)
                    
                    # Merge with previous segment if close enough
                    if merged_segments and start_with_buffer - merged_segments[-1][1] < 3.0:
                        merged_segments[-1][1] = end_with_buffer
                    else:
                        merged_segments.append([start_with_buffer, end_with_buffer])
            
            # Create clips and output video
            if merged_segments:
                clip = VideoFileClip(input_path)
                clips = []
                
                for i, (start, end) in enumerate(merged_segments):
                    if end - start >= 2.0:  # Only use segments of at least 2 seconds
                        print(f"Segment {i+1}: {start:.1f}s - {end:.1f}s")
                        clips.append(clip.subclip(start, end))
                
                if clips:
                    final_video = concatenate_videoclips(clips)
                    final_video.write_videofile(output_path, codec="libx264")
                    final_video.close()
                    clip.close()
                    
                    print(f"‚úÖ Created short video with {len(clips)} important clips")
                    return True
                else:
                    clip.close()
                    print("No valid segments found")
                    return False
            else:
                print("No important segments detected")
                return False
                
        except Exception as e:
            print(f"Error using Decord: {e}")
            # Fall back to original implementation
            return extract_important_clips(input_path, output_path, model, device, buffer_time, sample_rate)
    
    # Fall back to original implementation
    return extract_important_clips(input_path, output_path, model, device, buffer_time, sample_rate)

def detect_action(model, input_path, output_path, action_classes=['person'], threshold=0.4):
    """Nh·∫≠n di·ªán c√°c h√†nh ƒë·ªông c·ª• th·ªÉ trong video"""
    print(f"ƒêang ph√¢n t√≠ch video ƒë·ªÉ t√¨m h√†nh ƒë·ªông: {', '.join(action_classes)}...")
    
    cap = cv2.VideoCapture(input_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    # Danh s√°ch c√°c segment c√≥ action
    action_segments = []
    current_segment = None
    
    # C√°c ch·ªâ s·ªë l·ªõp c·∫ßn t√¨m
    class_indices = []
    for action in action_classes:
        if action.lower() in model.names:
            class_indices.append(list(model.names.values()).index(action.lower()))
    
    print(f"Qu√©t {frame_count} frames...")
    
    # Ph√¢n t√≠ch t·ª´ng frame
    for frame_idx in tqdm(range(0, frame_count, int(fps/2))):  # L·∫•y m·∫´u 2 frame/gi√¢y
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        if not ret:
            break
            
        # Nh·∫≠n di·ªán ƒë·ªëi t∆∞·ª£ng trong frame
        results = model(frame)
        detections = results.xyxy[0].cpu().numpy()
        
        action_detected = False
        
        # Ki·ªÉm tra xem c√≥ ƒë·ªëi t∆∞·ª£ng c·∫ßn t√¨m kh√¥ng
        for detection in detections:
            if int(detection[5]) in class_indices and detection[4] > threshold:
                action_detected = True
                break
                
        # X·ª≠ l√Ω segment
        time_sec = frame_idx / fps
        
        if action_detected:
            if current_segment is None:  # B·∫Øt ƒë·∫ßu segment m·ªõi
                current_segment = [time_sec, time_sec]
            else:  # M·ªü r·ªông segment hi·ªán t·∫°i
                current_segment[1] = time_sec
        elif current_segment is not None:
            # K·∫øt th√∫c segment hi·ªán t·∫°i n·∫øu kh√¥ng ph√°t hi·ªán action
            action_segments.append(current_segment)
            current_segment = None
    
    # Th√™m segment cu·ªëi c√πng n·∫øu c√≤n
    if current_segment is not None:
        action_segments.append(current_segment)
    
    cap.release()
    
    # Th√™m buffer v√† merge c√°c segment g·∫ßn nhau
    buffer_time = 1.0  # Th√™m 1 gi√¢y tr∆∞·ªõc v√† sau m·ªói segment
    merged_segments = []
    
    for segment in action_segments:
        start_time = max(0, segment[0] - buffer_time)
        end_time = segment[1] + buffer_time
        
        if not merged_segments or start_time > merged_segments[-1][1]:
            merged_segments.append([start_time, end_time])
        else:
            merged_segments[-1][1] = end_time
    
    # Xu·∫•t video c√°c segment
    if merged_segments:
        clip = VideoFileClip(input_path)
        final_clips = []
        
        for i, (start, end) in enumerate(merged_segments):
            print(f"H√†nh ƒë·ªông {i+1}: {start:.1f}s - {end:.1f}s")
            final_clips.append(clip.subclip(start, end))
        
        # Gh√©p v√† xu·∫•t video
        print(f"T·∫°o video v·ªõi {len(final_clips)} ƒëo·∫°n h√†nh ƒë·ªông...")
        final_video = concatenate_videoclips(final_clips)
        final_video.write_videofile(output_path, codec="libx264")
        clip.close()
        
        return True, merged_segments
    else:
        print(f"Kh√¥ng t√¨m th·∫•y h√†nh ƒë·ªông {action_classes} trong video.")
        return False, []

def create_highlight_compilation(input_path, output_path, segments, max_duration=60):
    """T·∫°o video t·ªïng h·ª£p c√°c ph·∫ßn hay nh·∫•t v·ªõi th·ªùi l∆∞·ª£ng t·ªëi ƒëa"""
    if not segments:
        print("Kh√¥ng c√≥ ƒëo·∫°n video n√†o ƒë·ªÉ t·∫°o highlight")
        return False
        
    print(f"T·∫°o video highlight v·ªõi th·ªùi l∆∞·ª£ng t·ªëi ƒëa {max_duration}s...")
    
    # S·∫Øp x·∫øp c√°c segment theo m·ª©c ƒë·ªô quan tr·ªçng (gi·∫£ s·ª≠ l√† ƒë·ªô d√†i)
    segments_with_scores = [(start, end, end-start) for start, end in segments]
    segments_with_scores.sort(key=lambda x: x[2], reverse=True)
    
    # Ch·ªçn c√°c segment quan tr·ªçng nh·∫•t m√† v·∫´n ƒë·∫£m b·∫£o th·ªùi l∆∞·ª£ng t·ªëi ƒëa
    selected_segments = []
    current_duration = 0
    
    for start, end, duration in segments_with_scores:
        if current_duration + duration <= max_duration:
            selected_segments.append((start, end))
            current_duration += duration
        else:
            # C·∫Øt segment cu·ªëi n·∫øu v∆∞·ª£t qu√° th·ªùi l∆∞·ª£ng
            remaining_time = max_duration - current_duration
            if remaining_time > 5:  # Ch·ªâ th√™m n·∫øu c√≤n √≠t nh·∫•t 5 gi√¢y
                selected_segments.append((start, start + remaining_time))
            break
    
    # S·∫Øp x·∫øp l·∫°i theo th·ª© t·ª± th·ªùi gian
    selected_segments.sort()
    
    # T·∫°o video highlight
    clip = VideoFileClip(input_path)
    highlight_clips = [clip.subclip(start, end) for start, end in selected_segments]
    
    if highlight_clips:
        final_video = concatenate_videoclips(highlight_clips)
        final_video.write_videofile(output_path, codec="libx264")
        clip.close()
        print(f"‚úÖ ƒê√£ t·∫°o video highlight th√†nh c√¥ng: {output_path}")
        return True
    else:
        print("‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫°o video highlight")
        return False

def initialize_faster_whisper():
    try:
        from faster_whisper import WhisperModel
        
        print("Kh·ªüi t·∫°o Faster Whisper model...")
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9  # GB
            
            if gpu_memory >= 8.0:
                model_size = "medium"
            else:
                model_size = "small"
            
            try:
                # D√πng float32 thay v√¨ float16 cho GTX 1060
                print(f"üîç GPU c√≥ {gpu_memory:.1}GB RAM, s·ª≠ d·ª•ng model {model_size} v·ªõi float32")
                model = WhisperModel(model_size, device="cuda", compute_type="float32")
                return model
            except:
                print(f"üîç Th·ª≠ s·ª≠ d·ª•ng int8 v·ªõi {model_size}")
                model = WhisperModel(model_size, device="cuda", compute_type="int8")
                return model
        else:
            print("‚ö†Ô∏è Kh√¥ng ph√°t hi·ªán GPU, s·ª≠ d·ª•ng CPU v·ªõi model small")
            model = WhisperModel("small", device="cpu", compute_type="int8")
            return model
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi kh·ªüi t·∫°o Faster Whisper: {e}")
        return None

def extract_frames_with_decord(video_path, sample_rate=1, device_id=0):
    """Extract frames from video using Decord library"""
    try:
        # Use CPU context if device_id is -1 or GPU not available
        if device_id < 0 or not torch.cuda.is_available():
            ctx = decord.cpu()
        else:
            ctx = decord.gpu(device_id)
            
        # Create video reader
        vr = decord.VideoReader(video_path, ctx=ctx)
        fps = vr.get_avg_fps()
        frame_count = len(vr)
        
        # Calculate which frames to extract
        frame_indices = list(range(0, frame_count, int(fps * sample_rate)))
        timestamps = [idx / fps for idx in frame_indices]
        
        # Extract frames
        frames = vr.get_batch(frame_indices).asnumpy()
        
        return frames, timestamps
    except Exception as e:
        print(f"Error using Decord: {e}")
        return None, None

def extract_frames_with_cpp(video_path, sample_rate=1.0, resize_width=0, resize_height=0):
    """Extract frames from video using C++ extension if available"""
    try:
        from utils.gpu_utils import try_load_cpp_extensions
        cpp_extensions = try_load_cpp_extensions()
        
        if "video_extract" in cpp_extensions:
            print("Using C++ video frame extraction")
            video_extract = cpp_extensions["video_extract"]
            
            # Get video info first
            try:
                video_info = video_extract.get_video_info(video_path)
                print(f"Video info: {video_info.width}x{video_info.height}, {video_info.fps} fps, {video_info.duration} seconds")
            except Exception as e:
                print(f"Error getting video info: {e}")
                return None, None, None
            
            # Extract frames using C++ module
            frames, timestamps, fps = video_extract.extract_frames_with_sample_rate(
                video_path, sample_rate, resize_width, resize_height)
            
            return frames, timestamps, fps
    except Exception as e:
        print(f"Error using C++ video extraction: {e}")
    
    # Fallback to existing methods
    if DECORD_AVAILABLE:
        print("Falling back to Decord for frame extraction")
        return extract_frames_with_decord(video_path, sample_rate)
    else:
        print("Falling back to OpenCV for frame extraction")
        return extract_frames_with_opencv(video_path, sample_rate)

def concatenate_videos_cpp(input_files, output_file, codec="libx264"):
    """Concatenate videos using direct FFmpeg C++ API if available"""
    try:
        from utils.gpu_utils import try_load_cpp_extensions
        cpp_extensions = try_load_cpp_extensions()
        
        if "video_concat" in cpp_extensions:
            print("Using C++ FFmpeg API for video concatenation")
            video_concat = cpp_extensions["video_concat"]
            
            # Concatenate videos using C++ module
            success = video_concat.concatenate_videos(input_files, output_file, codec)
            if success:
                print(f"‚úÖ Successfully concatenated videos to: {output_file}")
                return True
            else:
                print("‚ùå Failed to concatenate videos with C++ extension")
    except Exception as e:
        print(f"Error using C++ video concatenation: {e}")
    
    # Fallback to existing FFmpeg subprocess method
    print("Falling back to FFmpeg subprocess for concatenation")
    return concatenate_videos_ffmpeg(input_files, output_file, codec)

def extract_frames_with_opencv(video_path, sample_rate=1):
    """Extract frames using OpenCV at specified sample rate"""
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"Error: Could not open video {video_path}")
        return None, None, None
    
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    # Calculate frame indices based on sample rate
    frame_skip = int(fps * sample_rate)
    frame_indices = list(range(0, total_frames, frame_skip))
    
    frames = []
    timestamps = []
    
    for frame_idx in tqdm(frame_indices, desc="Extracting frames"):
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        if ret:
            frames.append(frame)
            timestamps.append(frame_idx / fps)
    
    cap.release()
    return np.array(frames), timestamps, fps

def concatenate_videos_ffmpeg(input_files, output_file, codec="libx264"):
    """Concatenate videos using FFmpeg subprocess"""
    try:
        # Prepare file list for FFmpeg
        with open("file_list.txt", "w") as f:
            for file in input_files:
                f.write(f"file '{file}'\n")
        
        # Run FFmpeg command
        cmd = [
            "ffmpeg", "-y",
            "-f", "concat",
            "-safe", "0",
            "-i", "file_list.txt",
            "-c:v", codec,
            "-c:a", "aac",
            output_file
        ]
        subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)
        os.remove("file_list.txt")
        return True
    except Exception as e:
        print(f"Error concatenating videos with FFmpeg: {e}")
        return False