import os
import torch
import argparse
import cv2
import numpy as np
from video_processing import extract_important_clips
from audio_processing import extract_audio, filter_audio, initialize_whisper_model, transcribe_audio_optimized, generate_voice_over
# Add missing imports for the undefined functions
from audio_processing import transcribe_audio_faster, transcribe_audio_parallel
from action_detection import detect_climax_scenes, extract_climax_scenes, fast_motion_detection, fast_extract_segments
from summary_generator import summarize_transcript, summarize_transcript_with_textrank, enhanced_summarize_transcript, filter_irrelevant_content, clean_transcript, validate_summary
import multiprocessing
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from functools import partial
import json
import signal
import subprocess
import shutil
import time
import sys
import threading
from utils.gpu_utils import force_gpu_usage, build_cpp_extensions

def setup_directories():
    """T·∫°o c√°c th∆∞ m·ª•c c·∫ßn thi·∫øt"""
    os.makedirs('input_videos', exist_ok=True)
    os.makedirs('output_videos', exist_ok=True)
    os.makedirs('models', exist_ok=True)

def setup_device(use_cuda=True):
    """Thi·∫øt l·∫≠p v√† ki·ªÉm tra device (CPU/GPU)"""
    # TH√äM: ∆Øu ti√™n bi·∫øn m√¥i tr∆∞·ªùng FORCE_CUDA
    force_cuda = os.environ.get('FORCE_CUDA', '0') == '1'
    
    has_cuda = torch.cuda.is_available()
    if (has_cuda or force_cuda):
        device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else "Forced CUDA Device"
        print(f"üîç Ph√°t hi·ªán GPU: {device_name}")
        if not use_cuda:
            print("‚ö†Ô∏è GPU b·ªã t·∫Øt theo tham s·ªë --use-cuda=False")
            device = torch.device('cpu')
        else:
            device = torch.device('cuda')
            if torch.cuda.is_available():
                torch.backends.cudnn.benchmark = True
                torch.cuda.empty_cache()  # X√≥a cache CUDA
                print(f"üöÄ GPU ƒë√£ ƒë∆∞·ª£c k√≠ch ho·∫°t: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB RAM")
            else:
                print("üöÄ GPU ƒë∆∞·ª£c k√≠ch ho·∫°t qua ch·∫ø ƒë·ªô FORCE_CUDA")
    else:
        device = torch.device('cpu')
        print("‚ö†Ô∏è Kh√¥ng ph√°t hi·ªán GPU, s·∫Ω s·ª≠ d·ª•ng CPU")

    return device, has_cuda or force_cuda  # Tr·∫£ v·ªÅ has_cuda ho·∫∑c force_cuda

def optimize_cuda_memory():
    """T·ªëi ∆∞u b·ªô nh·ªõ CUDA ƒë·ªÉ ƒë·∫°t hi·ªáu su·∫•t t·ªët nh·∫•t"""
    if torch.cuda.is_available():
        # X√≥a cache hi·ªán t·∫°i
        torch.cuda.empty_cache()
        
        # √Åp d·ª•ng c√°c t·ªëi ∆∞u cho PyTorch CUDA
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        if hasattr(torch.backends.cuda, 'matmul') and hasattr(torch.backends.cuda.matmul, 'allow_tf32'):
            torch.backends.cuda.matmul.allow_tf32 = True
        
        # Qu·∫£n l√Ω b·ªô nh·ªõ th√¥ng minh
        total_gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"GPU memory total: {total_gpu_mem:.1f} GB")
        
        if total_gpu_mem > 6.0:
            print("‚ùáÔ∏è GPU l·ªõn, k√≠ch ho·∫°t t·ªëi ∆∞u memory cho video n·∫∑ng")
            if hasattr(torch.cuda, 'memory_stats'):
                torch.cuda.memory_stats()
        else:
            print("‚ö†Ô∏è GPU nh·ªè, √°p d·ª•ng t·ªëi ∆∞u cho thi·∫øt b·ªã h·∫°n ch·∫ø")
            
        return True
    return False

def load_yolo_model(device):
    """T·∫£i model YOLO"""
    # Ki·ªÉm tra & t·∫£i YOLOv5 n·∫øu ch∆∞a c√≥
    model_path = os.path.join('models', 'yolov5s.pt')
    if not os.path.exists(model_path):
        print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y model t·∫°i {model_path}")
        # Th·ª≠ ki·ªÉm tra trong th∆∞ m·ª•c g·ªëc
        alt_model_path = 'yolov5s.pt'
        if (os.path.exists(alt_model_path)):
            print(f"‚úÖ ƒê√£ t√¨m th·∫•y model t·∫°i {alt_model_path}")
            model_path = alt_model_path
        else:
            print("ƒêang t·∫£i model YOLOv5...")
            os.makedirs('models', exist_ok=True)
            try:
                torch.hub.download_url_to_file(
                    'https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5s.pt', 
                    model_path
                )
            except Exception as e:
                print(f"‚õî Kh√¥ng th·ªÉ t·∫£i model t·ª´ GitHub: {e}")
                print("Vui l√≤ng t·∫£i th·ªß c√¥ng model YOLOv5s t·ª´:")
                print("https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5s.pt")
                print(f"v√† ƒë·∫∑t v√†o th∆∞ m·ª•c {os.path.dirname(model_path)} v·ªõi t√™n 'yolov5s.pt'")
                exit(1)

    # Load YOLOv5 model
    print(f"ƒêang kh·ªüi t·∫°o model YOLOv5 t·ª´ {model_path}...")
    try:
        # Th·ª≠ t·∫£i v·ªõi torch.hub
        model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path, force_reload=False, trust_repo=True)
    except Exception as e:
        print(f"L·ªói khi t·∫£i model qua torch.hub: {e}")
        print("Th·ª≠ t·∫£i model tr·ª±c ti·∫øp...")
        
        try:
            # T·∫£i tr·ª±c ti·∫øp v·ªõi PyTorch
            model = torch.load(model_path, map_location=device)
            if hasattr(model, 'module'):  # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p model ƒë∆∞·ª£c l∆∞u v·ªõi DataParallel
                model = model.module
        except Exception as e2:
            print(f"‚õî Kh√¥ng th·ªÉ t·∫£i model: {e2}")
            exit(1)

    model.to(device)  # Chuy·ªÉn m√¥ h√¨nh l√™n GPU n·∫øu c√≥
    model.conf = 0.25  # Ng∆∞·ª°ng tin c·∫≠y
    model.iou = 0.45   # Ng∆∞·ª°ng IoU
    if device.type == 'cuda':
        model.amp = True  # S·ª≠ d·ª•ng mixed precision khi c√≥ GPU
        print("‚úÖ YOLO model ƒë∆∞·ª£c t·∫£i l√™n GPU v·ªõi Automatic Mixed Precision (AMP)")
    
    return model

def has_dialogue(audio_path, threshold_db=-35, min_dialogue_duration=0.5):
    """Ki·ªÉm tra nhanh xem video c√≥ l·ªùi tho·∫°i hay kh√¥ng"""
    try:
        from pydub import AudioSegment
        import numpy as np
        
        print("ƒêang ki·ªÉm tra nhanh l·ªùi tho·∫°i...")
        audio = AudioSegment.from_file(audio_path)
        
        # L·∫•y m·∫´u audio
        samples = np.array(audio.get_array_of_samples())
        if (audio.channels == 2):
            samples = samples.reshape((-1, 2)).mean(axis=1)  # Chuy·ªÉn stereo th√†nh mono
            
        # Chuy·ªÉn sang dB
        samples = np.abs(samples)
        max_amplitude = np.iinfo(samples.dtype).max
        samples_db = 20 * np.log10(samples / max_amplitude + 1e-10)
        
        # Ki·ªÉm tra c√°c ƒëo·∫°n c√≥ √¢m thanh tr√™n ng∆∞·ª°ng
        samples_above_threshold = samples_db > threshold_db
        
        # T√≠nh s·ªë l∆∞·ª£ng m·∫´u li√™n ti·∫øp tr√™n ng∆∞·ª°ng
        count = 0
        max_count = 0
        for is_above in samples_above_threshold:
            if is_above:
                count += 1
                max_count = max(max_count, count)
            else:
                count = 0
                
        # Ki·ªÉm tra c√≥ ƒë·ªß m·∫´u li√™n ti·∫øp kh√¥ng
        min_samples = min_dialogue_duration * audio.frame_rate
        has_speech = max_count >= min_samples
        
        if has_speech:
            print("‚úÖ Ph√°t hi·ªán l·ªùi tho·∫°i trong video")
        else:
            print("‚ÑπÔ∏è Kh√¥ng ph√°t hi·ªán l·ªùi tho·∫°i r√µ r√†ng trong video")
            
        return has_speech
        
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi ki·ªÉm tra l·ªùi tho·∫°i: {e}")
        # N·∫øu c√≥ l·ªói, gi·∫£ ƒë·ªãnh l√† c√≥ l·ªùi tho·∫°i ƒë·ªÉ an to√†n
        return True

def process_audio_task(input_path, device, ratio=0.3):
    """T√°c v·ª• x·ª≠ l√Ω audio ch·∫°y trong process ri√™ng"""
    try:
        print("\nüîä ƒêang x·ª≠ l√Ω √¢m thanh v√† nh·∫≠n d·∫°ng l·ªùi tho·∫°i...")
        # Kh·ªüi t·∫°o model Whisper
        whisper_model = initialize_whisper_model(device)
        
        # Tr√≠ch xu·∫•t v√† x·ª≠ l√Ω √¢m thanh
        raw_audio_path = os.path.join('output_videos', 'temp_audio_raw.wav')
        filtered_audio_path = os.path.join('output_videos', 'temp_audio.wav')
        extract_audio(input_path, raw_audio_path)
        
        # Ki·ªÉm tra nhanh xem video c√≥ l·ªùi tho·∫°i kh√¥ng
        if not has_dialogue(raw_audio_path):
            print("‚ö†Ô∏è Video kh√¥ng c√≥ l·ªùi tho·∫°i r√µ r√†ng, kh√¥ng c·∫ßn x·ª≠ l√Ω ti·∫øp audio")
            result = {
                "success": False,
                "reason": "no_dialogue",
                "transcript": "",
                "summary": "Video kh√¥ng c√≥ l·ªùi tho·∫°i."
            }
            return result
            
        filter_audio(raw_audio_path, filtered_audio_path)
        
        # Nh·∫≠n d·∫°ng gi·ªçng n√≥i
        transcript = transcribe_audio_optimized(filtered_audio_path, whisper_model)
        transcript = clean_transcript(transcript)
        
        if not transcript or len(transcript.split()) < 5:
            print("‚ö†Ô∏è Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c l·ªùi tho·∫°i c√≥ nghƒ©a")
            result = {
                "success": False,
                "reason": "empty_transcript",
                "transcript": transcript,
                "summary": "Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c l·ªùi tho·∫°i c√≥ nghƒ©a."
            }
            return result
            
        # L·ªçc v√† t√≥m t·∫Øt n·ªôi dung
        filtered_transcript = filter_irrelevant_content(transcript)
        summary = enhanced_summarize_transcript(filtered_transcript, ratio)
        
        # L∆∞u transcript v√† summary
        full_transcript_path = os.path.join('output_videos', 'review_transcript.txt')
        summary_path = os.path.join('output_videos', 'review_summary.txt')
        
        with open(full_transcript_path, "w", encoding="utf-8") as f:
            f.write(transcript)
        
        with open(summary_path, "w", encoding="utf-8") as f:
            f.write(summary)
        
        print(f"‚úÖ ƒê√£ l∆∞u b·∫£n t√≥m t·∫Øt review phim v√†o: {summary_path}")
        
        # T·∫°o voice-over cho b·∫£n t√≥m t·∫Øt
        voice_over_path = os.path.join('output_videos', 'review_voiceover.mp3')
        generate_voice_over(summary, voice_over_path)
        
        # X√≥a file √¢m thanh t·∫°m
        if os.path.exists(raw_audio_path):
            os.remove(raw_audio_path)
        if os.path.exists(filtered_audio_path):
            os.remove(filtered_audio_path)
        
        result = {
            "success": True,
            "transcript": transcript,
            "summary": summary,
            "full_transcript_path": full_transcript_path,
            "summary_path": summary_path,
            "voice_over_path": voice_over_path
        }
        return result
        
    except Exception as e:
        print(f"‚ùå L·ªói khi x·ª≠ l√Ω audio: {e}")
        result = {
            "success": False,
            "reason": "error",
            "error": str(e)
        }
        return result

def process_video_task(input_path, output_path, model, device):
    """T√°c v·ª• x·ª≠ l√Ω video ch·∫°y trong thread ri√™ng"""
    try:
        print("\nüé¨ ƒêang ph√¢n t√≠ch video ƒë·ªÉ t√¨m c·∫£nh quan tr·ªçng...")
        
        # T√πy ch·ªçn ph∆∞∆°ng ph√°p ph√°t hi·ªán - m·∫∑c ƒë·ªãnh s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p k·∫øt h·ª£p
        detection_method = 3
        buffer_time = 1.0
        
        # Th·ª±c hi·ªán ph√°t hi·ªán theo ph∆∞∆°ng ph√°p ƒë√£ ch·ªçn
        if detection_method == 1:
            # Ph√°t hi·ªán ng∆∞·ªùi xu·∫•t hi·ªán
            success = extract_important_clips(input_path, output_path, model, device, buffer_time, sample_rate=1)
        elif detection_method == 2:
            # Ph√°t hi·ªán c·∫£nh cao tr√†o
            success = extract_climax_scenes(input_path, output_path, buffer_time)
        else:
            # K·∫øt h·ª£p c·∫£ hai ph∆∞∆°ng ph√°p
            from action_detection import extract_climax_scenes_improved
            success = extract_climax_scenes_improved(input_path, output_path, model, device, buffer_time)
        
        result = {
            "success": success,
            "output_path": output_path
        }
        return result
        
    except Exception as e:
        print(f"‚ùå L·ªói khi x·ª≠ l√Ω video: {e}")
        result = {
            "success": False,
            "reason": "error",
            "error": str(e)
        }
        return result

def process_both_modes_parallel(input_path, output_dir, model, device, summary_ratio=0.3):
    """X·ª≠ l√Ω c·∫£ hai ch·∫ø ƒë·ªô song song"""
    print("\n=== ƒêANG X·ª¨ L√ù SONG SONG CH·∫æ ƒê·ªò REVIEW V√Ä SHORT ===")
    
    # ƒê∆∞·ªùng d·∫´n output
    short_output_path = os.path.join(output_dir, 'short.mp4')
    
    # T·∫°o pool cho c√°c process v√† thread
    audio_process = None
    video_future = None
    
    # Chu·∫©n b·ªã device cho t·ª´ng process
    if device.type == 'cuda' and torch.cuda.device_count() >= 2:
        # N·∫øu c√≥ 2 GPU tr·ªü l√™n, m·ªói process d√πng 1 GPU
        audio_device = torch.device('cuda:0')
        video_device = torch.device('cuda:1')
        print("üöÄ Ph√°t hi·ªán nhi·ªÅu GPU, ph√¢n b·ªï t√°c v·ª• tr√™n c√°c GPU kh√°c nhau")
    else:
        # N·∫øu ch·ªâ c√≥ 1 GPU ho·∫∑c CPU, d√πng chung
        audio_device = device
        video_device = device
    
    try:
        # Kh·ªüi ƒë·ªông quy tr√¨nh x·ª≠ l√Ω audio trong m·ªôt process ri√™ng
        audio_process = multiprocessing.Process(
            target=process_audio_wrapper,
            args=(input_path, audio_device, summary_ratio, output_dir)
        )
        audio_process.start()
        
        # Kh·ªüi ƒë·ªông quy tr√¨nh x·ª≠ l√Ω video trong thread pool
        with ThreadPoolExecutor(max_workers=1) as executor:
            video_future = executor.submit(
                process_video_task, input_path, short_output_path, model, video_device
            )
        
        # ƒê·ª£i v√† l·∫•y k·∫øt qu·∫£ t·ª´ thread x·ª≠ l√Ω video
        video_result = video_future.result()
        
        # In th√¥ng b√°o ƒëang ƒë·ª£i x·ª≠ l√Ω audio
        print("\n‚è≥ ƒêang ƒë·ª£i x·ª≠ l√Ω audio ho√†n t·∫•t...")
        
        # ƒê·ª£i process audio ho√†n t·∫•t
        audio_process.join(timeout=300)  # Timeout 5 ph√∫t
        
        # Ki·ªÉm tra xem c·∫£ hai qu√° tr√¨nh ƒë√£ ho√†n t·∫•t ch∆∞a
        if not audio_process.is_alive() and video_result["success"]:
            # ƒê·ªçc k·∫øt qu·∫£ t·ª´ file t·∫°m
            audio_result_path = os.path.join(output_dir, 'audio_result.json')
            if os.path.exists(audio_result_path):
                import json
                with open(audio_result_path, 'r', encoding='utf-8') as f:
                    audio_result = json.load(f)
                
                # X√≥a file t·∫°m
                os.remove(audio_result_path)
            else:
                audio_result = {"success": False}
                
            # T·∫°o video review v·ªõi voiceover n·∫øu c·∫£ hai qu√° tr√¨nh th√†nh c√¥ng
            if audio_result["success"] and video_result["success"]:
                voice_over_path = audio_result.get("voice_over_path")
                short_video_path = video_result.get("output_path")
                
                if os.path.exists(voice_over_path) and os.path.exists(short_video_path):
                    create_video_with_voiceover(short_video_path, voice_over_path, output_dir)
        
        else:
            print("‚ö†Ô∏è M·ªôt ho·∫∑c c·∫£ hai qu√° tr√¨nh x·ª≠ l√Ω kh√¥ng ho√†n t·∫•t ƒë√∫ng h·∫°n.")
            
            # Ng·∫Øt process n·∫øu v·∫´n ƒëang ch·∫°y
            if audio_process.is_alive():
                print("‚õî ƒêang d·ª´ng qu√° tr√¨nh x·ª≠ l√Ω audio...")
                audio_process.terminate()
                
    except Exception as e:
        print(f"‚ùå L·ªói khi x·ª≠ l√Ω song song: {e}")
        
        # D·ª´ng c√°c process ƒëang ch·∫°y

        if audio_process and audio_process.is_alive():
            audio_process.terminate()
            
    finally:
        # D·ªçn d·∫πp t√†i nguy√™n
        audio_result_path = os.path.join(output_dir, 'audio_result.json')
        if os.path.exists(audio_result_path):
            os.remove(audio_result_path)
            
def process_audio_wrapper(input_path, device, ratio, output_dir):
    """Wrapper ƒë·ªÉ ch·∫°y process_audio_task trong process ri√™ng v√† l∆∞u k·∫øt qu·∫£"""
    try:
        # Chuy·ªÉn ƒë·ªïi torch device th√†nh string ƒë·ªÉ truy·ªÅn qua process
        device_str = 'cuda' if device.type == 'cuda' else 'cpu'
        device = torch.device(device_str)
        
        # Th·ª±c hi·ªán x·ª≠ l√Ω audio
        result = process_audio_task(input_path, device, ratio)
        
        # L∆∞u k·∫øt qu·∫£ v√†o file ƒë·ªÉ process ch√≠nh c√≥ th·ªÉ ƒë·ªçc
        import json
        audio_result_path = os.path.join(output_dir, 'audio_result.json')
        with open(audio_result_path, 'w', encoding='utf-8') as f:
            # Chuy·ªÉn ƒë·ªïi ƒë∆∞·ªùng d·∫´n th√†nh string n·∫øu c·∫ßn
            serializable_result = {}
            for key, value in result.items():
                if isinstance(value, (str, bool, int, float)):
                    serializable_result[key] = value
                elif value is None:
                    serializable_result[key] = None
                else:
                    serializable_result[key] = str(value)
            json.dump(serializable_result, f)
            
    except Exception as e:
        print(f"‚ùå L·ªói trong process audio: {e}")
        # L∆∞u th√¥ng tin l·ªói
        import json
        audio_result_path = os.path.join(output_dir, 'audio_result.json')
        with open(audio_result_path, 'w', encoding='utf-8') as f:
            json.dump({"success": False, "error": str(e)}, f)

def create_video_with_voiceover(video_path, audio_path, output_dir):
    """T·∫°o video review v·ªõi voiceover"""
    try:
        from moviepy.editor import VideoFileClip, AudioFileClip
        
        print("\n=== T·∫†O VIDEO REVIEW WITH VOICEOVER ===")
        video_clip = VideoFileClip(video_path)
        audio_clip = AudioFileClip(audio_path)
        
        # L·∫•y ƒë·ªô d√†i ng·∫Øn h∆°n gi·ªØa video v√† audio ƒë·ªÉ tr√°nh l·ªói
        min_duration = min(video_clip.duration, audio_clip.duration)
        video_clip = video_clip.subclip(0, min_duration)
        audio_clip = audio_clip.subclip(0, min_duration)
        
        # Th√™m voiceover v√†o video
        final_clip = video_clip.set_audio(audio_clip)
        review_video_path = os.path.join(output_dir, 'review_with_voiceover.mp4')
        final_clip.write_videofile(review_video_path, codec="libx264", threads=min(4, os.cpu_count()))
        
        print(f"‚úÖ ƒê√£ t·∫°o video review k√®m l·ªùi tho·∫°i: {review_video_path}")
        
        # ƒê√≥ng c√°c clip
        video_clip.close()
        audio_clip.close()
        final_clip.close()
        
    except Exception as e:
        print(f"‚ùå L·ªói khi t·∫°o video review k√®m l·ªùi tho·∫°i: {e}")

def process_review_mode(input_path, output_path, device, model=None, ratio=0.3, 
                       use_faster_whisper=True, use_cuda=True):
    """Ch·∫ø ƒë·ªô Review Phim - T√≥m t·∫Øt n·ªôi dung theo l·ªùi tho·∫°i v√† c·∫£nh cao tr√†o"""
    print("\n=== CH·∫æ ƒê·ªò REVIEW PHIM ===")
    
    # Ph√¢n t√≠ch cao tr√†o tr∆∞·ªõc ƒë·ªÉ ch·∫°y song song v·ªõi x·ª≠ l√Ω audio
    print("üé¨ ƒêang ph√°t hi·ªán c√°c c·∫£nh cao tr√†o ƒë·ªÉ b·ªï sung v√†o review...")
    
    # N·∫øu model ch∆∞a ƒë∆∞·ª£c truy·ªÅn v√†o, t·∫£i YOLO model
    if (model is None):
        try:
            model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)
            model.to(device)
            if device.type == 'cuda':
                model.amp = True  # Enable mixed precision for better performance
                print("‚úÖ YOLO model s·ª≠ d·ª•ng GPU v·ªõi Automatic Mixed Precision")
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫£i model YOLO: {e}")
            print("Ti·∫øp t·ª•c m√† kh√¥ng c√≥ ph√¢n t√≠ch cao tr√†o")
            model = None
    
    # T·∫°o m·ªôt thread ri√™ng ƒë·ªÉ ph√°t hi·ªán cao tr√†o
    climax_segments = []
    climax_future = None
    
    with ThreadPoolExecutor(max_workers=1) as executor:
        if model is not None:
            try:
                from action_detection import detect_climax_scenes_improved
                # B·∫Øt ƒë·∫ßu ti·∫øn tr√¨nh ph√°t hi·ªán cao tr√†o trong thread ri√™ng
                climax_future = executor.submit(
                    detect_climax_scenes_improved, input_path, model, device, threshold_multiplier=0.9
                )
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói khi b·∫Øt ƒë·∫ßu ph√°t hi·ªán cao tr√†o: {e}")
    
    # S·ª≠ d·ª•ng Faster-Whisper n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu v√† c√≥ GPU
    transcript = ""
    if use_faster_whisper and torch.cuda.is_available():
        try:
            print("üöÄ ƒêang s·ª≠ d·ª•ng Faster-Whisper ƒë·ªÉ tƒÉng t·ªëc 3-5 l·∫ßn...")
            
            # Tr√≠ch xu·∫•t v√† x·ª≠ l√Ω √¢m thanh
            raw_audio_path = os.path.join('output_videos', 'temp_audio_raw.wav')
            filtered_audio_path = os.path.join('output_videos', 'temp_audio.wav')
            extract_audio(input_path, raw_audio_path)
            
            # Ki·ªÉm tra nhanh xem video c√≥ l·ªùi tho·∫°i kh√¥ng
            has_speech = has_dialogue(raw_audio_path)
            
            if has_speech:
                # Ti·∫øp t·ª•c v·ªõi quy tr√¨nh ph√¢n t√≠ch l·ªùi tho·∫°i th√¥ng th∆∞·ªùng
                filter_audio(raw_audio_path, filtered_audio_path)
                
                # Nh·∫≠n d·∫°ng gi·ªçng n√≥i v·ªõi Faster-Whisper
                print("üîç Ph√¢n t√≠ch n·ªôi dung l·ªùi tho·∫°i...")
                result = transcribe_audio_faster(filtered_audio_path)
                if result and result.get("text"):
                    transcript = result["text"] 
                    transcript = clean_transcript(transcript)
            else:
                print("‚ö†Ô∏è Kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c l·ªùi tho·∫°i r√µ r√†ng")
                transcript = ""
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi s·ª≠ d·ª•ng Faster-Whisper: {e}")
            print("Chuy·ªÉn sang ph∆∞∆°ng ph√°p song song...")
            use_faster_whisper = False
    
    # Th·ª≠ ph∆∞∆°ng ph√°p song song n·∫øu Faster-Whisper th·∫•t b·∫°i
    if not transcript and use_faster_whisper:
        try:
            print("Th·ª≠ d√πng ph∆∞∆°ng ph√°p song song v·ªõi c√°c chunk...")
            raw_audio_path = os.path.join('output_videos', 'temp_audio_raw.wav')
            filtered_audio_path = os.path.join('output_videos', 'temp_audio.wav')
            if not os.path.exists(filtered_audio_path):
                extract_audio(input_path, raw_audio_path)
                filter_audio(raw_audio_path, filtered_audio_path)
            
            result = transcribe_audio_parallel(
                filtered_audio_path, 
                chunk_length_sec=15,
                max_workers=4
            )
            if result and result["text"]:
                transcript = result["text"]
                transcript = clean_transcript(transcript)
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi d√πng ph∆∞∆°ng ph√°p song song: {e}")
            print("Chuy·ªÉn sang Whisper th∆∞·ªùng...")
    
    # N·∫øu hai ph∆∞∆°ng ph√°p tr√™n th·∫•t b·∫°i, d√πng ph∆∞∆°ng ph√°p th√¥ng th∆∞·ªùng
    if not transcript:
        # Kh·ªüi t·∫°o model Whisper
        whisper_model = initialize_whisper_model(device)
        
        # Tr√≠ch xu·∫•t v√† x·ª≠ l√Ω √¢m thanh n·∫øu ch∆∞a c√≥
        raw_audio_path = os.path.join('output_videos', 'temp_audio_raw.wav')
        filtered_audio_path = os.path.join('output_videos', 'temp_audio.wav')
        if not os.path.exists(raw_audio_path):
            extract_audio(input_path, raw_audio_path)
        if not os.path.exists(filtered_audio_path):
            filter_audio(raw_audio_path, filtered_audio_path)
        
        # Ki·ªÉm tra nhanh xem video c√≥ l·ªùi tho·∫°i kh√¥ng
        has_speech = has_dialogue(raw_audio_path)
        
        if has_speech:
            # Nh·∫≠n d·∫°ng gi·ªçng n√≥i
            print("üîç Ph√¢n t√≠ch n·ªôi dung l·ªùi tho·∫°i...")
            transcript = transcribe_audio_optimized(filtered_audio_path, whisper_model)
            transcript = clean_transcript(transcript)
        else:
            print("‚ö†Ô∏è Kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c l·ªùi tho·∫°i r√µ r√†ng")
            transcript = ""
    
    # L·∫•y k·∫øt qu·∫£ t·ª´ thread ph√°t hi·ªán cao tr√†o (n·∫øu c√≥)
    action_summary = []
    if climax_future:
        try:
            climax_segments = climax_future.result()
            if climax_segments:
                print(f"üé≠ Ph√°t hi·ªán ƒë∆∞·ª£c {len(climax_segments)} c·∫£nh cao tr√†o")
                action_summary = [f"C·∫£nh cao tr√†o t·ª´ {start:.1f}s ƒë·∫øn {end:.1f}s" for start, end in climax_segments]
            else:
                print("‚ö†Ô∏è Kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c c·∫£nh cao tr√†o n√†o")
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi ph√°t hi·ªán cao tr√†o: {e}")
    
    # LOGIC TH√îNG MINH: K·∫øt h·ª£p t√≥m t·∫Øt l·ªùi tho·∫°i v√† cao tr√†o
    if transcript and len(transcript.split()) >= 5:
        # L·ªçc v√† t√≥m t·∫Øt n·ªôi dung l·ªùi tho·∫°i
        filtered_transcript = filter_irrelevant_content(transcript)
        summary = enhanced_summarize_transcript(filtered_transcript, ratio)
        
        # B·ªï sung th√¥ng tin cao tr√†o v√†o cu·ªëi b·∫£n t√≥m t·∫Øt
        if action_summary:
            summary += "\n\n## C√°c c·∫£nh cao tr√†o:\n"
            summary += "\n".join([f"- {item}" for item in action_summary])
    else:
        # N·∫øu kh√¥ng c√≥ l·ªùi tho·∫°i ƒë√°ng k·ªÉ, t·∫°o t√≥m t·∫Øt t·ª´ c·∫£nh cao tr√†o
        if action_summary:
            print("üìù T·∫°o t√≥m t·∫Øt d·ª±a tr√™n c·∫£nh cao tr√†o")
            summary = "Video kh√¥ng c√≥ l·ªùi tho·∫°i r√µ r√†ng ho·∫∑c c√≥ r·∫•t √≠t l·ªùi tho·∫°i.\n\n"
            summary += "## T√≥m t·∫Øt d·ª±a tr√™n ph√¢n t√≠ch h√¨nh ·∫£nh:\n\n"
            summary += "Video ch·ª©a c√°c c·∫£nh cao tr√†o sau:\n"
            summary += "\n".join([f"- {item}" for item in action_summary])
        else:
            # Kh√¥ng c√≥ c·∫£ l·ªùi tho·∫°i v√† cao tr√†o
            summary = "Video kh√¥ng c√≥ l·ªùi tho·∫°i r√µ r√†ng v√† kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c c·∫£nh cao tr√†o ƒë√°ng ch√∫ √Ω."
    
    # L∆∞u transcript v√† summary
    full_transcript_path = os.path.join('output_videos', 'review_transcript.txt')
    summary_path = os.path.join('output_videos', 'review_summary.txt')
    
    with open(full_transcript_path, "w", encoding="utf-8") as f:
        f.write(transcript if transcript else "Kh√¥ng c√≥ l·ªùi tho·∫°i")
    
    with open(summary_path, "w", encoding="utf-8") as f:
        f.write(summary)
    
    print(f"‚úÖ ƒê√£ l∆∞u b·∫£n t√≥m t·∫Øt review phim v√†o: {summary_path}")
    print("\n--- N·ªôi dung t√≥m t·∫Øt ---")
    print(summary[:500] + "..." if len(summary) > 500 else summary)
    
    # T·∫°o voice-over cho b·∫£n t√≥m t·∫Øt n·∫øu c·∫ßn
    if summary.strip():  # Ch·ªâ t·∫°o voice-over n·∫øu c√≥ n·ªôi dung t√≥m t·∫Øt
        voice_over_path = os.path.join('output_videos', 'review_voiceover.mp3')
        generate_voice_over(summary, voice_over_path)
    else:
        voice_over_path = None
    
    # X√≥a file √¢m thanh t·∫°m
    if os.path.exists(raw_audio_path):
        os.remove(raw_audio_path)
    if os.path.exists(filtered_audio_path):
        os.remove(filtered_audio_path)
    
    # T·∫°o video highlight c√°c c·∫£nh cao tr√†o n·∫øu c√≥
    if climax_segments:
        try:
            highlight_path = os.path.join('output_videos', 'review_highlights.mp4')
            
            # S·ª≠ d·ª•ng CUDA n·∫øu c√≥ th·ªÉ
            if use_cuda and torch.cuda.is_available():
                from action_detection import fast_extract_segments_cuda
                if fast_extract_segments_cuda(input_path, highlight_path, climax_segments):
                    print(f"‚úÖ ƒê√£ t·∫°o video highlight c√°c c·∫£nh cao tr√†o v·ªõi CUDA: {highlight_path}")
            else:
                from action_detection import extract_segments_to_video
                if extract_segments_to_video(input_path, highlight_path, climax_segments, buffer_time=1.0):
                    print(f"‚úÖ ƒê√£ t·∫°o video highlight c√°c c·∫£nh cao tr√†o: {highlight_path}")
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫°o video highlight: {e}")
    
    print("‚úÖ Ho√†n th√†nh ch·∫ø ƒë·ªô Review Phim!")
    return summary, voice_over_path

def process_short_mode(input_path, output_dir, model, device):
    """Ch·∫ø ƒë·ªô C·∫Øt Video Short - Ph√°t hi·ªán v√† c·∫Øt c√°c c·∫£nh cao tr√†o"""
    print("\n=== CH·∫æ ƒê·ªò C·∫ÆT VIDEO SHORT ===")
    
    # ƒê∆∞·ªùng d·∫´n file output v·ªõi t√™n ph√¢n bi·ªát 
    summary_video_path = os.path.join(output_dir, "summary_output.mp4")
    climax_video_path = os.path.join(output_dir, "climax_output.mp4")
    combined_video_path = os.path.join(output_dir, "short.mp4")  # Output cu·ªëi c√πng
    
    # Ph√°t hi·ªán c√°c c·∫£nh cao tr√†o ho·∫∑c c√≥ ng∆∞·ªùi xu·∫•t hi·ªán
    print("üîç ƒêang ph√¢n t√≠ch video ƒë·ªÉ t√¨m c·∫£nh ph√π h·ª£p cho video short...")
    
    # T√πy ch·ªçn ph∆∞∆°ng ph√°p ph√°t hi·ªán
    print("\nCh·ªçn ph∆∞∆°ng ph√°p ph√°t hi·ªán c·∫£nh:")
    print("1. Ph√°t hi·ªán ng∆∞·ªùi xu·∫•t hi·ªán")
    print("2. Ph√°t hi·ªán c·∫£nh cao tr√†o (chuy·ªÉn ƒë·ªông m·∫°nh, thay ƒë·ªïi c·∫£nh)")
    print("3. K·∫øt h·ª£p c·∫£ hai ph∆∞∆°ng ph√°p")
    
    while True:
        try:
            detection_method = int(input("L·ª±a ch·ªçn c·ªßa b·∫°n (1-3): "))
            if 1 <= detection_method <= 3:
                break
            else:
                print("Vui l√≤ng nh·∫≠p s·ªë t·ª´ 1-3!")
        except ValueError:
            print("Vui l√≤ng nh·∫≠p s·ªë!")
            detection_method = 3  # M·∫∑c ƒë·ªãnh k·∫øt h·ª£p
            break
    
    # X√°c ƒë·ªãnh th·ªùi l∆∞·ª£ng mong mu·ªën cho video short
    print("\nNh·∫≠p th·ªùi l∆∞·ª£ng t·ªëi ƒëa mong mu·ªën cho video short (gi√¢y):")
    try:
        max_duration = int(input("Th·ªùi l∆∞·ª£ng (th∆∞·ªùng l√† 15-60 gi√¢y): "))
    except ValueError:
        max_duration = 30
        print(f"Gi√° tr·ªã kh√¥ng h·ª£p l·ªá, s·ª≠ d·ª•ng gi√° tr·ªã m·∫∑c ƒë·ªãnh: {max_duration}s")
    
    # Buffer time gi·ªØa c√°c c·∫£nh
    buffer_time = 1.0
    
    # Th·ª±c hi·ªán ph√°t hi·ªán theo ph∆∞∆°ng ph√°p ƒë√£ ch·ªçn
    if detection_method == 1:
        # Ph√°t hi·ªán ng∆∞·ªùi xu·∫•t hi·ªán
        success = extract_important_clips(input_path, summary_video_path, model, device, buffer_time, sample_rate=1)
        if success:
            import shutil
            shutil.copy(summary_video_path, combined_video_path)
            print(f"‚úÖ ƒê√£ t·∫°o video short d·ª±a tr√™n ph√°t hi·ªán ng∆∞·ªùi xu·∫•t hi·ªán: {combined_video_path}")
        return success, combined_video_path
        
    elif detection_method == 2:
        # Ph√°t hi·ªán c·∫£nh cao tr√†o
        success = extract_climax_scenes(input_path, climax_video_path, buffer_time)
        if success:
            import shutil
            shutil.copy(climax_video_path, combined_video_path)
            print(f"‚úÖ ƒê√£ t·∫°o video short d·ª±a tr√™n ph√°t hi·ªán c·∫£nh cao tr√†o: {combined_video_path}")
        return success, combined_video_path
        
    else:
        # K·∫øt h·ª£p c·∫£ hai ph∆∞∆°ng ph√°p
        print("ƒêang th·ª±c hi·ªán ph∆∞∆°ng ph√°p 1: Ph√°t hi·ªán ng∆∞·ªùi xu·∫•t hi·ªán...")
        people_detected = extract_important_clips(input_path, summary_video_path, model, device, buffer_time, sample_rate=1)
        
        print("ƒêang th·ª±c hi·ªán ph∆∞∆°ng ph√°p 2: Ph√°t hi·ªán c·∫£nh cao tr√†o...")
        from action_detection import extract_climax_scenes_improved
        climax_detected = extract_climax_scenes_improved(input_path, climax_video_path, model, device, buffer_time=buffer_time, threshold_multiplier=1.0)
        
        # X√°c ƒë·ªãnh video n√†o s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng l√†m output cu·ªëi c√πng
        final_success = False
        
        # Ki·ªÉm tra v√† ch·ªçn video t·ªët nh·∫•t
        if people_detected and os.path.exists(summary_video_path) and os.stat(summary_video_path).st_size > 0:
            from moviepy.editor import VideoFileClip
            clip1 = VideoFileClip(summary_video_path)
            duration1 = clip1.duration
            clip1.close()
            
            if climax_detected and os.path.exists(climax_video_path) and os.stat(climax_video_path).st_size > 0:
                clip2 = VideoFileClip(climax_video_path)
                duration2 = clip2.duration
                clip2.close()
                
                # N·∫øu c·∫£ hai ƒë·ªÅu th√†nh c√¥ng, ch·ªçn video d√†i h∆°n ho·∫∑c gh√©p
                if abs(duration1 - duration2) < 10:
                    print(f"ƒê√£ t·∫°o c·∫£ hai lo·∫°i video c√≥ ƒë·ªô d√†i t∆∞∆°ng ƒë∆∞∆°ng.")
                    
                    # L·∫•y video ph√°t hi·ªán ng∆∞·ªùi v√¨ th∆∞·ªùng ƒëa d·∫°ng h∆°n
                    import shutil
                    shutil.copy(summary_video_path, combined_video_path)
                    print(f"‚úÖ ƒê√£ ch·ªçn video ph√°t hi·ªán ng∆∞·ªùi: {combined_video_path}")
                    final_success = True
                    
                elif duration1 > duration2 and duration1 <= max_duration:
                    import shutil
                    shutil.copy(summary_video_path, combined_video_path)
                    print(f"‚úÖ ƒê√£ ch·ªçn video d√†i h∆°n (ph√°t hi·ªán ng∆∞·ªùi): {combined_video_path}")
                    final_success = True
                    
                elif duration2 <= max_duration:
                    import shutil
                    shutil.copy(climax_video_path, combined_video_path)
                    print(f"‚úÖ ƒê√£ ch·ªçn video d√†i h∆°n (c·∫£nh cao tr√†o): {combined_video_path}")
                    final_success = True
                    
                else:
                    # N·∫øu c·∫£ hai ƒë·ªÅu d√†i h∆°n max_duration, l·∫•y c√°i ng·∫Øn h∆°n
                    if duration1 <= duration2:
                        import shutil
                        shutil.copy(summary_video_path, combined_video_path)
                        print(f"‚úÖ ƒê√£ ch·ªçn video ng·∫Øn h∆°n: {combined_video_path}")
                    else:
                        import shutil
                        shutil.copy(climax_video_path, combined_video_path)
                        print(f"‚úÖ ƒê√£ ch·ªçn video ng·∫Øn h∆°n: {combined_video_path}")
                    final_success = True
                    
            else:
                # Ch·ªâ ph√°t hi·ªán ng∆∞·ªùi th√†nh c√¥ng
                import shutil
                shutil.copy(summary_video_path, combined_video_path)
                print(f"‚úÖ Ch·ªâ ph√°t hi·ªán ng∆∞·ªùi th√†nh c√¥ng, ƒë√£ ch·ªçn: {combined_video_path}")
                final_success = True
                
        elif climax_detected and os.path.exists(climax_video_path) and os.stat(climax_video_path).st_size > 0:
            # Ch·ªâ ph√°t hi·ªán c·∫£nh cao tr√†o th√†nh c√¥ng
            import shutil
            shutil.copy(climax_video_path, combined_video_path)
            print(f"‚úÖ Ch·ªâ ph√°t hi·ªán c·∫£nh cao tr√†o th√†nh c√¥ng, ƒë√£ ch·ªçn: {combined_video_path}")
            final_success = True
        
        else:
            print("‚ùå Kh√¥ng th·ªÉ t·∫°o video short v·ªõi c·∫£ hai ph∆∞∆°ng ph√°p.")
            final_success = False
        
        # Gi·ªØ l·∫°i c√°c file trung gian n·∫øu c·∫ßn debug
        # N·∫øu kh√¥ng c·∫ßn debug, h√£y m·ªü comment c√°c d√≤ng x√≥a file
        try:
            if final_success:  # Ch·ªâ x√≥a n·∫øu ƒë√£ t·∫°o th√†nh c√¥ng file output cu·ªëi c√πng
                if os.path.exists(summary_video_path):
                    print(f"‚ö†Ô∏è Ki·ªÉm tra file tr∆∞·ªõc khi x√≥a: {summary_video_path}")
                    if "summary" in summary_video_path:
                        print(f"‚ùå Kh√¥ng x√≥a {summary_video_path}, v√¨ ƒë√≥ l√† video t√≥m t·∫Øt!")
                    else:
                        os.remove(summary_video_path)
                        print(f"‚úÖ ƒê√£ x√≥a {summary_video_path} v√¨ kh√¥ng c·∫ßn thi·∫øt.")
                if os.path.exists(climax_video_path):
                    print(f"‚ö†Ô∏è Ki·ªÉm tra file tr∆∞·ªõc khi x√≥a: {climax_video_path}")
                    if "climax" in climax_video_path:
                        print(f"‚ùå Kh√¥ng x√≥a {climax_video_path}, v√¨ ƒë√≥ l√† video cao tr√†o!")
                    else:
                        os.remove(climax_video_path)
                        print(f"‚úÖ ƒê√£ x√≥a {climax_video_path} v√¨ kh√¥ng c·∫ßn thi·∫øt.")
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω file t·∫°m: {e}")
    
    print("‚úÖ Ho√†n th√†nh ch·∫ø ƒë·ªô C·∫Øt Video Short!")
    return final_success, combined_video_path

def process_review_mode_fast(input_path, output_path, device, ratio=0.3):
    """Phi√™n b·∫£n t·ªëi ∆∞u t·ªëc ƒë·ªô c·ªßa ch·∫ø ƒë·ªô Review Phim"""
    print("\n=== CH·∫æ ƒê·ªò REVIEW PHIM (T·ªêC ƒê·ªò CAO) ===")
    
    # Kh·ªüi t·∫°o model Whisper
    whisper_model = initialize_whisper_model(device)
    
    # Tr√≠ch xu·∫•t v√† x·ª≠ l√Ω √¢m thanh - song song v·ªõi ph√¢n t√≠ch video
    raw_audio_path = os.path.join('output_videos', 'temp_audio_raw.wav')
    filtered_audio_path = os.path.join('output_videos', 'temp_audio.wav')
    extract_audio(input_path, raw_audio_path)
    
    # Kh·ªüi ƒë·ªông ph√°t hi·ªán cao tr√†o trong m·ªôt thread ri√™ng
    climax_segments = []
    with ThreadPoolExecutor(max_workers=1) as executor:
        # S·ª≠ d·ª•ng phi√™n b·∫£n nhanh c·ªßa ph√°t hi·ªán cao tr√†o
        from action_detection import detect_climax_scenes_fast
        future = executor.submit(detect_climax_scenes_fast, input_path, None, device)
    
    # Ti·∫øp t·ª•c x·ª≠ l√Ω √¢m thanh trong khi ph√°t hi·ªán cao tr√†o ch·∫°y
    # Ki·ªÉm tra nhanh xem video c√≥ l·ªùi tho·∫°i kh√¥ng
    has_speech = has_dialogue(raw_audio_path)
    
    if has_speech:
        filter_audio(raw_audio_path, filtered_audio_path)
        
        # Nh·∫≠n d·∫°ng gi·ªçng n√≥i
        print("üîç Ph√¢n t√≠ch n·ªôi dung l·ªùi tho·∫°i...")
        transcript = transcribe_audio_optimized(filtered_audio_path, whisper_model)
        transcript = clean_transcript(transcript)
    else:
        print("‚ö†Ô∏è Kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c l·ªùi tho·∫°i r√µ r√†ng")
        transcript = ""
    
    # L·∫•y k·∫øt qu·∫£ t·ª´ thread ph√°t hi·ªán cao tr√†o (n·∫øu ƒë√£ ho√†n th√†nh)
    try:
        # Ch·ªù k·∫øt qu·∫£ v·ªõi timeout ƒë·ªÉ tr√°nh ch·ªù qu√° l√¢u
        climax_segments = future.result(timeout=60)
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ ho√†n th√†nh ph√°t hi·ªán cao tr√†o: {e}")
    
    # T·∫°o b·∫£n t√≥m t·∫Øt d·ª±a tr√™n l·ªùi tho·∫°i v√†/ho·∫∑c cao tr√†o
    if transcript and len(transcript.split()) >= 5:
        # L·ªçc v√† t√≥m t·∫Øt n·ªôi dung l·ªùi tho·∫°i
        filtered_transcript = filter_irrelevant_content(transcript)
        summary = enhanced_summarize_transcript(filtered_transcript, ratio)
        
        # Th√™m th√¥ng tin cao tr√†o
        if climax_segments:
            summary += "\n\n## C√°c c·∫£nh cao tr√†o:\n"
            for i, (start, end) in enumerate(climax_segments):
                summary += f"- C·∫£nh cao tr√†o {i+1}: {start:.1f}s - {end:.1f}s\n"
    else:
        if climax_segments:
            summary = "## T√≥m t·∫Øt d·ª±a tr√™n ph√¢n t√≠ch h√¨nh ·∫£nh:\n\n"
            summary += "Video c√≥ c√°c c·∫£nh cao tr√†o sau:\n"
            for i, (start, end) in enumerate(climax_segments):
                summary += f"- C·∫£nh cao tr√†o {i+1}: {start:.1f}s - {end:.1f}s\n"
        else:
            summary = "Kh√¥ng th·ªÉ t·∫°o t√≥m t·∫Øt do kh√¥ng c√≥ l·ªùi tho·∫°i v√† kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c c·∫£nh cao tr√†o."
    
    # L∆∞u b·∫£n t√≥m t·∫Øt
    summary_path = os.path.join('output_videos', 'review_summary.txt')
    with open(summary_path, "w", encoding="utf-8") as f:
        f.write(summary)
    
    # Clean up
    if os.path.exists(raw_audio_path):
        os.remove(raw_audio_path)
    if os.path.exists(filtered_audio_path):
        os.remove(filtered_audio_path)
    
    print("‚úÖ Ho√†n th√†nh ch·∫ø ƒë·ªô Review Phim (t·ªëc ƒë·ªô cao)!")
    return summary

def debug_info():
    """Hi·ªÉn th·ªã th√¥ng tin debug cho ng∆∞·ªùi d√πng"""
    print("\n===== TH√îNG TIN H·ªÜ TH·ªêNG =====")
    
    # Ki·ªÉm tra CPU
    import multiprocessing
    print(f"CPU: {multiprocessing.cpu_count()} cores")
    
    # Ki·ªÉm tra GPU
    try:
        import torch
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
                print(f"   - RAM: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB")
        else:
            print("GPU: Not available")
    except Exception as e:
        print(f"GPU check error: {e}")
    
    # Ki·ªÉm tra RAM
    try:
        import psutil
        vm = psutil.virtual_memory()
        print(f"RAM: {vm.total / 1e9:.1f} GB total, {vm.available / 1e9:.1f} GB available")
    except Exception as e:
        print(f"RAM check error: {e}")
    
    # Ki·ªÉm tra FFmpeg
    try:
        import subprocess
        result = subprocess.run(["ffmpeg", "-version"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        if result.returncode == 0:
            version_line = result.stdout.decode().split("\n")[0]
            print(f"FFmpeg: {version_line}")
        else:
            print("FFmpeg: Not found")
    except:
        print("FFmpeg: Not found or error")
        
    # Ki·ªÉm tra th∆∞ vi·ªán
    try:
        import importlib
        libraries = ["cv2", "whisper", "faster_whisper", "moviepy", "torch", "numpy"]
        for lib in libraries:
            try:
                module = importlib.import_module(lib)
                version = getattr(module, "__version__", "unknown")
                print(f"{lib}: {version}")
            except ImportError:
                print(f"{lib}: Not installed")
    except:
        pass
    
    print("=============================\n")

def estimate_processing_time(video_path, mode):
    """∆Ø·ªõc t√≠nh th·ªùi gian x·ª≠ l√Ω d·ª±a tr√™n k√≠ch th∆∞·ªõc v√† th·ªùi l∆∞·ª£ng video"""
    try:
        # L·∫•y k√≠ch th∆∞·ªõc file
        file_size_gb = os.path.getsize(video_path) / 1e9
        
        # L·∫•y ƒë·ªô d√†i video
        import cv2
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration_minutes = (total_frames / fps) / 60
        cap.release()
        
        # ∆Ø·ªõc t√≠nh th·ªùi gian x·ª≠ l√Ω
        if mode == "review":
            # Review mode: mostly audio processing
            estimated_minutes = duration_minutes * 0.5  # Kho·∫£ng 1/2 th·ªùi l∆∞·ª£ng video
        elif mode == "short":
            # Short mode: heavy video processing
            estimated_minutes = duration_minutes * 0.3  # Kho·∫£ng 1/3 th·ªùi l∆∞·ª£ng video
        else:
            # Both modes
            estimated_minutes = duration_minutes * 0.6  # Kho·∫£ng 3/5 th·ªùi l∆∞·ª£ng video
            
        # ƒêi·ªÅu ch·ªânh theo k√≠ch th∆∞·ªõc file v√† RAM
        if file_size_gb > 1:
            estimated_minutes *= (1 + file_size_gb * 0.1)  # Th√™m 10% cho m·ªói GB
            
        # Round up
        estimated_minutes = int(estimated_minutes) + 1
        
        print(f"‚ÑπÔ∏è Th√¥ng tin video: {duration_minutes:.1f} ph√∫t, {file_size_gb:.2f} GB")
        print(f"‚è±Ô∏è ∆Ø·ªõc t√≠nh th·ªùi gian x·ª≠ l√Ω: kho·∫£ng {estimated_minutes} ph√∫t")
        
        return estimated_minutes
    except Exception as e:
        print(f"Kh√¥ng th·ªÉ ∆∞·ªõc t√≠nh th·ªùi gian x·ª≠ l√Ω: {e}")
        return None

def process_review_mode_with_timeout(input_path, output_path, device, model=None, ratio=0.3, timeout=300,
                                   use_faster_whisper=True, use_cuda=True):
    """Phi√™n b·∫£n process_review_mode v·ªõi timeout ƒë·ªÉ tr√°nh x·ª≠ l√Ω qu√° l√¢u"""
    import threading
    import time
    
    # Import c√°c h√†m c·∫ßn thi·∫øt t·ª´ c√°c module kh√°c
    try:
        from action_detection import fast_motion_detection, fast_motion_detection_with_decord
        from action_detection import fast_extract_segments, fast_extract_segments_cuda
    except ImportError:
        print("‚ö†Ô∏è Kh√¥ng th·ªÉ import c√°c h√†m t·ª´ action_detection")
    
    try:
        from summary_generator import filter_irrelevant_content, enhanced_summarize_transcript
    except ImportError:
        print("‚ö†Ô∏è Kh√¥ng th·ªÉ import c√°c h√†m t·ª´ summary_generator")
        # Ph∆∞∆°ng ph√°p d·ª± ph√≤ng
        def filter_irrelevant_content(text): return text
        def enhanced_summarize_transcript(text, ratio): return text[:int(len(text)*ratio)]
    
    # Bi·∫øn d√πng ƒë·ªÉ l∆∞u k·∫øt qu·∫£ v√† ki·ªÉm tra timeout
    result = {"completed": False, "summary": "", "voice_over_path": None}
    timeout_occurred = False
    
    # H√†m th·ª±c thi ch√≠nh - ch·∫°y trong thread ri√™ng
    def process_with_timeout():
        try:
            # Ph·∫ßn 1: X·ª≠ l√Ω audio (c·∫ßn thi·∫øt)
            raw_audio_path = os.path.join('output_videos', 'temp_audio_raw.wav')
            filtered_audio_path = os.path.join('output_videos', 'temp_audio.wav')
            extract_audio(input_path, raw_audio_path)
            filter_audio(raw_audio_path, filtered_audio_path)
            
            # S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p song song ƒë·ªÉ chuy·ªÉn ƒë·ªïi √¢m thanh nhanh h∆°n
            print("üöÄ S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p chuy·ªÉn ƒë·ªïi √¢m thanh nhanh...")
            transcript = ""
            
            if use_faster_whisper:
                # D√πng ph∆∞∆°ng ph√°p x·ª≠ l√Ω v·ªõi Faster-Whisper
                try:
                    from audio_processing import transcribe_audio_faster
                    whisper_result = transcribe_audio_faster(filtered_audio_path)
                    if whisper_result and whisper_result["text"]:
                        transcript = whisper_result["text"]
                        from audio_processing import clean_transcript
                        transcript = clean_transcript(transcript)
                        print("‚úÖ Chuy·ªÉn ƒë·ªïi √¢m thanh th√†nh c√¥ng v·ªõi Faster-Whisper")
                    else:
                        raise Exception("Faster-Whisper kh√¥ng tr·∫£ v·ªÅ k·∫øt qu·∫£")
                except Exception as e:
                    print(f"‚ö†Ô∏è L·ªói khi d√πng Faster-Whisper: {e}")
                    use_faster_whisper = False
            
            if not transcript and not use_faster_whisper:
                # Th·ª≠ v·ªõi ph∆∞∆°ng ph√°p song song
                try:
                    from audio_processing import transcribe_audio_parallel
                    whisper_result = transcribe_audio_parallel(
                        filtered_audio_path, 
                        chunk_length_sec=15,
                        max_workers=4
                    )
                    if whisper_result and whisper_result["text"]:
                        transcript = whisper_result["text"]
                        from audio_processing import clean_transcript
                        transcript = clean_transcript(transcript)
                        print("‚úÖ Chuy·ªÉn ƒë·ªïi √¢m thanh th√†nh c√¥ng v·ªõi ph∆∞∆°ng ph√°p song song")
                except Exception as e:
                    print(f"‚ö†Ô∏è L·ªói khi d√πng ph∆∞∆°ng ph√°p song song: {e}")
                    
                    # Kh·ªüi t·∫°o model Whisper v√† fallback
                    print("Chuy·ªÉn sang whisper th√¥ng th∆∞·ªùng...")
                    whisper_model = initialize_whisper_model(device)
                    transcript = transcribe_audio_optimized(filtered_audio_path, whisper_model)
                    from audio_processing import clean_transcript
                    transcript = clean_transcript(transcript)
            
            # Ph·∫ßn 2: T√≥m t·∫Øt vƒÉn b·∫£n v√† ph√¢n t√≠ch video
            if transcript and len(transcript.split()) >= 10:
                filtered_transcript = filter_irrelevant_content(transcript)
                summary = enhanced_summarize_transcript(filtered_transcript, ratio)
            else:
                summary = "Kh√¥ng ph√°t hi·ªán l·ªùi tho·∫°i c√≥ nghƒ©a trong video."
            
            # Ph·∫ßn 3: Ph√¢n t√≠ch video (th·ª≠ n·∫øu c√≤n th·ªùi gian)
            try:
                segments = []
                # S·ª≠ d·ª•ng phi√™n b·∫£n nhanh ƒë·ªÉ ph√°t hi·ªán chuy·ªÉn ƒë·ªông
                if use_cuda and 'fast_motion_detection_with_decord' in globals():
                    print("üöÄ S·ª≠ d·ª•ng GPU ƒë·ªÉ ph√°t hi·ªán chuy·ªÉn ƒë·ªông...")
                    segments = fast_motion_detection_with_decord(input_path)
                elif 'fast_motion_detection' in globals():
                    segments = fast_motion_detection(input_path)
                else:
                    print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y h√†m ph√°t hi·ªán chuy·ªÉn ƒë·ªông")
                
                # Th√™m th√¥ng tin v·ªÅ c√°c ƒëo·∫°n chuy·ªÉn ƒë·ªông v√†o summary
                if segments:
                    summary += "\n\n## C√°c ƒëo·∫°n c√≥ chuy·ªÉn ƒë·ªông m·∫°nh:\n"
                    for i, (start, end) in enumerate(segments):
                        summary += f"- ƒêo·∫°n {i+1}: {start:.1f}s - {end:.1f}s (th·ªùi l∆∞·ª£ng: {end-start:.1f}s)\n"
                        
                # T·∫°o video highlight n·∫øu c√≥ segments
                if segments and model is not None:
                    highlight_path = os.path.join('output_videos', 'review_highlights.mp4')
                    extracted = False
                    
                    if use_cuda and 'fast_extract_segments_cuda' in globals():
                        extracted = fast_extract_segments_cuda(input_path, highlight_path, segments)
                    elif 'fast_extract_segments' in globals():
                        extracted = fast_extract_segments(input_path, highlight_path, segments)
                        
                    if extracted:
                        print(f"‚úÖ ƒê√£ t·∫°o video highlight: {highlight_path}")
                    else:
                        print("‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫°o video highlight")
                        
            except Exception as e:
                print(f"‚ö†Ô∏è Ph√¢n t√≠ch video g·∫∑p l·ªói: {e}")
            
            # L∆∞u b·∫£n t√≥m t·∫Øt
            summary_path = os.path.join('output_videos', 'review_summary.txt')
            with open(summary_path, "w", encoding="utf-8") as f:
                f.write(summary)
            
            # T·∫°o voice-over
            voice_over_path = os.path.join('output_videos', 'review_voiceover.mp3')
            generate_voice_over(summary, voice_over_path)
            
            # D·ªçn d·∫πp
            if os.path.exists(raw_audio_path):
                os.remove(raw_audio_path)
            if os.path.exists(filtered_audio_path):
                os.remove(filtered_audio_path)
            
            # L∆∞u k·∫øt qu·∫£ v√†o bi·∫øn to√†n c·ª•c ƒë·ªÉ truy c·∫≠p t·ª´ thread ch√≠nh
            result["completed"] = True
            result["summary"] = summary
            result["voice_over_path"] = voice_over_path
            
        except Exception as e:
            print(f"‚ùå L·ªói trong qu√° tr√¨nh x·ª≠ l√Ω: {e}")
            summary = f"ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh x·ª≠ l√Ω: {e}"
            summary_path = os.path.join('output_videos', 'review_summary.txt')
            with open(summary_path, "w", encoding="utf-8") as f:
                f.write(summary)
            
            result["summary"] = summary
    
    # T·∫°o v√† ch·∫°y thread x·ª≠ l√Ω
    processing_thread = threading.Thread(target=process_with_timeout)
    processing_thread.daemon = True  # Cho ph√©p tho√°t ·ª©ng d·ª•ng n·∫øu thread c√≤n ch·∫°y
    processing_thread.start()
    
    # Ch·ªù thread trong kho·∫£ng th·ªùi gian timeout
    start_time = time.time()
    while processing_thread.is_alive():
        processing_thread.join(timeout=0.5)  # Ki·ªÉm tra m·ªói 0.5 gi√¢y
        if time.time() - start_time > timeout:
            timeout_occurred = True
            print(f"‚ö†Ô∏è Qu√° th·ªùi gian x·ª≠ l√Ω {timeout} gi√¢y, ti·∫øn tr√¨nh s·∫Ω ti·∫øp t·ª•c nh∆∞ng k·∫øt qu·∫£ s·∫Ω ƒë∆∞·ª£c tr·∫£ v·ªÅ ngay")
            break
    
    # X·ª≠ l√Ω timeout
    if timeout_occurred:
        print("‚ö†Ô∏è Qu√° th·ªùi gian x·ª≠ l√Ω, tr·∫£ v·ªÅ k·∫øt qu·∫£ ƒë√£ c√≥")
        # Tr·∫£ v·ªÅ k·∫øt qu·∫£ t·∫°m th·ªùi n·∫øu c√≥
        summary_path = os.path.join('output_videos', 'review_summary.txt')
        if os.path.exists(summary_path):
            with open(summary_path, "r", encoding="utf-8") as f:
                summary = f.read()
        else:
            summary = "X·ª≠ l√Ω timeout, kh√¥ng th·ªÉ t·∫°o t√≥m t·∫Øt ƒë·∫ßy ƒë·ªß."
            with open(summary_path, "w", encoding="utf-8") as f:
                f.write(summary)
        
        voice_over_path = os.path.join('output_videos', 'review_voiceover.mp3')
        if not os.path.exists(voice_over_path):
            generate_voice_over(summary, voice_over_path)
            
        return summary, voice_over_path
    
    # Tr·∫£ v·ªÅ k·∫øt qu·∫£ t·ª´ thread x·ª≠ l√Ω
    return result["summary"], result["voice_over_path"]

def setup_yolo8_model(device, task="detect"):
    """Setup YOLOv8 model for detection or pose estimation"""
    try:
        from action_detection import initialize_yolo8_model
        model_type = "yolov8s.pt" if task == "detect" else "yolov8s-pose.pt"
        model = initialize_yolo8_model(device, model_type, task)
        return model
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to set up YOLOv8 model: {e}")
        return None

def process_short_mode_yolo8(input_path, output_dir, device="cuda", mode=1, 
                           target_duration=30.0, buffer_time=1.0):
    """Enhanced short video clipping with YOLOv8"""
    
    print(f"\n=== ENHANCED SHORT VIDEO MODE (YOLOv8) ===")
    
    # Ki·ªÉm tra r√µ r√†ng device
    if device == "cuda" and torch.cuda.is_available():
        print(f"üöÄ S·ª≠ d·ª•ng GPU cho x·ª≠ l√Ω video: {torch.cuda.get_device_name(0)}")
        # ƒê·∫£m b·∫£o c√°c bi·∫øn m√¥i tr∆∞·ªùng ƒë∆∞·ª£c ƒë·∫∑t
        os.environ['CUDA_VISIBLE_DEVICES'] = '0'
        os.environ['FORCE_CUDA'] = '1'
    else:
        print("‚ö†Ô∏è CUDA kh√¥ng kh·∫£ d·ª•ng ho·∫∑c kh√¥ng ƒë∆∞·ª£c y√™u c·∫ßu, s·ª≠ d·ª•ng CPU")
    
    # Set up output paths
    if mode == 1:
        output_path = os.path.join(output_dir, "single_action_short.mp4")
        print("üìä Mode: Single most intense action moment")
    else:
        output_path = os.path.join(output_dir, "multi_action_short.mp4")
        print(f"üìä Mode: Multiple action scenes (target: {target_duration:.1f}s)")
    
    # Initialize YOLOv8
    yolo_model = setup_yolo8_model(device)
    
    # Code ti·∫øp theo kh√¥ng thay ƒë·ªïi...

    if yolo_model is None:
        print("‚ö†Ô∏è Failed to initialize YOLOv8, using fallback method")
        from action_detection import detect_climax_scenes_fast, fast_extract_segments
        
        # Use fallback method
        segments = detect_climax_scenes_fast(input_path, None, device)
        if segments:
            success = fast_extract_segments(input_path, output_path, segments, use_cuda=(device=="cuda"))
            return success, output_path
        else:
            print("‚ùå No action scenes detected")
            return False, None
    
    # Process based on mode
    if mode == 1:
        # Single most intense moment
        from action_detection import create_single_action_short
        success = create_single_action_short(
            input_path, output_path, yolo_model, device, 
            buffer_time=buffer_time, 
            min_duration=5.0,
            max_duration=15.0
        )
    else:
        # Multiple action moments
        from action_detection import create_multi_action_short
        success = create_multi_action_short(
            input_path, output_path, yolo_model, device,
            target_duration=target_duration,
            max_scenes=5,
            buffer_time=buffer_time
        )
    
    # Clean up GPU memory
    if device == "cuda" and torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    if success:
        print(f"‚úÖ Short video created successfully: {output_path}")
        return True, output_path
    else:
        print("‚ùå Failed to create short video")
        return False, None

# Update the main function to use the new YOLOv8 capabilities
def debug_gpu():
    """Ki·ªÉm tra chi ti·∫øt GPU v√† t·∫°o b√°o c√°o"""
    print("\n===== TH√îNG TIN GPU CHI TI·∫æT =====")
    
    # Ki·ªÉm tra xem PyTorch c√≥ th·ªÉ t√¨m th·∫•y CUDA kh√¥ng
    has_cuda_support = torch.cuda.is_available()
    print(f"PyTorch nh·∫≠n di·ªán CUDA: {has_cuda_support}")
    
    # Ki·ªÉm tra th√¥ng tin GPU qua c√°c ph∆∞∆°ng ph√°p kh√°c nhau
    try:
        import subprocess
        # Tr√™n Windows, d√πng nvidia-smi
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        if result.returncode == 0:
            print("\nTh√¥ng tin t·ª´ nvidia-smi:")
            print(result.stdout.split('\n')[0])
            print(result.stdout.split('\n')[1])
            
            # CUDA driver version
            import re
            match = re.search(r'CUDA Version: (\d+\.\d+)', result.stdout)
            if match:
                cuda_version = match.group(1)
                print(f"CUDA Driver Version: {cuda_version}")
                
                # So s√°nh v·ªõi torch.version.cuda
                torch_cuda_version = torch.version.cuda
                print(f"PyTorch CUDA Version: {torch_cuda_version}")
                
                if torch_cuda_version != cuda_version:
                    print(f"‚ö†Ô∏è Phi√™n b·∫£n CUDA kh√¥ng kh·ªõp! Driver: {cuda_version}, PyTorch: {torch_cuda_version}")
                    print("ƒêi·ªÅu n√†y c√≥ th·ªÉ khi·∫øn PyTorch kh√¥ng nh·∫≠n di·ªán GPU")
            else:
                print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y phi√™n b·∫£n CUDA trong nvidia-smi")
        else:
            print("‚ö†Ô∏è Kh√¥ng th·ªÉ ch·∫°y nvidia-smi. GPU c√≥ th·ªÉ kh√¥ng ƒë∆∞·ª£c c√†i ƒë·∫∑t ƒë√∫ng c√°ch")
            
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi ki·ªÉm tra GPU: {e}")
    
    print("================================\n")

import os
import sys
import torch
from utils.gpu_utils import force_gpu_usage, get_gpu_info, debug_gpu_detection

def main():
    print("===== TOOLFILM AI =====")
    
    # TH√äM: Bu·ªôc nh·∫≠n di·ªán GPU ·ªü ƒë√¢y, ƒë·∫ßu ti√™n
    force_gpu_usage()
    
    # Debug GPU/CPU
    gpu_info = get_gpu_info()
    if gpu_info["available"]:
        print(f"üöÄ ƒê√£ ph√°t hi·ªán GPU: {gpu_info['devices'][0]['name']}")
        print(f"   B·ªô nh·ªõ: {gpu_info['devices'][0]['total_memory_gb']:.1f} GB")
    else:
        print("‚ö†Ô∏è Kh√¥ng ph√°t hi·ªán GPU, s·∫Ω s·ª≠ d·ª•ng CPU")
        print("   Ki·ªÉm tra c√†i ƒë·∫∑t CUDA v√† tr√¨nh ƒëi·ªÅu khi·ªÉn NVIDIA")
        
    # Ki·ªÉm tra xem FORCE_CUDA c√≥ ƒë∆∞·ª£c c√†i ƒë·∫∑t kh√¥ng
    force_cuda = os.environ.get('FORCE_CUDA', '0')
    if force_cuda == '1':
        print("‚úÖ Ch·∫ø ƒë·ªô FORCE_CUDA ƒë∆∞·ª£c k√≠ch ho·∫°t")
    
    # Ki·ªÉm tra ULTRALYTICS_DEVICE ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t ch∆∞a
    ultralytics_device = os.environ.get('ULTRALYTICS_DEVICE', 'None')
    print(f"‚úì ULTRALYTICS_DEVICE = {ultralytics_device}")
    
    # S·ª≠ d·ª•ng module yolo_utils thay v√¨ torch.hub tr·ª±c ti·∫øp
    try:
        from utils.yolo_utils import initialize_yolo5_model
        
        # Chuy·ªÉn ƒë·∫øn device ph√π h·ª£p
        device = "cuda" if gpu_info["available"] else "cpu"
        yolo_model = initialize_yolo5_model(weights="models/yolov5s.pt", device=device)
        
        if yolo_model is not None:
            print("‚úÖ Kh·ªüi t·∫°o model YOLOv5 th√†nh c√¥ng")
        else:
            print("‚ùå Kh√¥ng th·ªÉ kh·ªüi t·∫°o model YOLOv5")
    except Exception as e:
        print(f"‚ùå L·ªói khi kh·ªüi t·∫°o model: {e}")
    
    # Th·ª≠ kh·ªüi t·∫°o YOLOv8 ƒë·ªÉ ki·ªÉm tra
    try:
        from yolo8_detection import initialize_yolo8_model
        yolo8_model = initialize_yolo8_model(device="cuda")
        if yolo8_model is not None:
            print("‚úÖ Kh·ªüi t·∫°o model YOLOv8 th√†nh c√¥ng")
            print(f"   Device: {next(yolo8_model.parameters()).device}")
        else:
            print("‚ùå Kh√¥ng th·ªÉ kh·ªüi t·∫°o model YOLOv8")
    except Exception as e:
        print(f"‚ùå L·ªói khi kh·ªüi t·∫°o YOLOv8: {e}")
    
    # Code ti·∫øp theo...
    # ...existing code...

if __name__ == "__main__":
    # Check if the YOLOv8 GPU fix is being applied
    if os.environ.get('ULTRALYTICS_DEVICE') == 'cuda:0':
        print("‚úÖ YOLOv8 GPU fix is active")
    else:
        print("‚ö†Ô∏è YOLOv8 GPU fix is not active. Consider running with run_yolo8_gpu_fix.bat instead")
    
    main()