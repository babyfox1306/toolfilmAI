import os
import numpy as np
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from nltk.tokenize import sent_tokenize
import torch
import nltk
import networkx as nx

# Kiá»ƒm tra vÃ  táº£i dá»¯ liá»‡u NLTK náº¿u cáº§n
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    try:
        nltk.download('punkt', quiet=True)
        print("âœ… ÄÃ£ táº£i dá»¯ liá»‡u NLTK punkt")
    except:
        print("âš ï¸ KhÃ´ng thá»ƒ táº£i dá»¯ liá»‡u NLTK, sáº½ sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p tÃ¡ch cÃ¢u Ä‘Æ¡n giáº£n")

def clean_transcript(text):
    """LÃ m sáº¡ch vÄƒn báº£n transcript"""
    if not text:
        return ""
        
    # XÃ³a cÃ¡c khoáº£ng tráº¯ng dÆ° thá»«a
    text = re.sub(r'\s+', ' ', text).strip()
    
    # XÃ³a cÃ¡c tá»« láº·p láº¡i liÃªn tiáº¿p (thÆ°á»ng lÃ  lá»—i nháº­n dáº¡ng)
    text = re.sub(r'\b(\w+)( \1\b)+', r'\1', text)
    
    # ThÃªm dáº¥u cháº¥m náº¿u cÃ¢u káº¿t thÃºc khÃ´ng cÃ³ dáº¥u
    text = re.sub(r'([a-zA-Z0-9])\s+([A-Z])', r'\1. \2', text)
    
    # Sá»­a cÃ¡c lá»—i phá»• biáº¿n trong nháº­n dáº¡ng tiáº¿ng Viá»‡t
    corrections = {
        'cua': 'cá»§a',
        'dc': 'Ä‘Æ°á»£c',
        'ko': 'khÃ´ng',
        'k ': 'khÃ´ng ',
        'j ': 'gÃ¬ ',
        'ntn': 'nhÆ° tháº¿ nÃ o',
    }
    
    for wrong, correct in corrections.items():
        text = re.sub(r'\b' + wrong + r'\b', correct, text, flags=re.IGNORECASE)
    
    return text

def filter_irrelevant_content(text):
    """Lá»c ná»™i dung khÃ´ng liÃªn quan trong vÄƒn báº£n"""
    if not text:
        return ""
        
    # Loáº¡i bá» cÃ¡c tá»«, cá»¥m tá»« dÆ° thá»«a hoáº·c khÃ´ng liÃªn quan
    irrelevant_phrases = [
        r"\bá»«m\b", r"\bÃ \b", r"\báº­y\b", r"\bháº£\b", r"\buhm\b", r"\buh\b", r"\bhmm\b",
        r"((xin )?chÃ o (cÃ¡c )?(báº¡n|anh|chá»‹|quÃ½ vá»‹))",
        r"cáº£m Æ¡n (cÃ¡c )?(báº¡n|anh|chá»‹|quÃ½ vá»‹) Ä‘Ã£ xem video",
        r"Ä‘á»«ng quÃªn like vÃ  subscribe",
        r"hÃ£y báº¥m like vÃ  Ä‘Äƒng kÃ½ kÃªnh",
        r"nhá»› Ä‘Äƒng kÃ½ kÃªnh nhÃ©",
        r"chÃ o má»«ng báº¡n Ä‘áº¿n vá»›i",
        r"Ä‘á»«ng quÃªn Ä‘Äƒng kÃ½ kÃªnh",
        r"hÃ£y like vÃ  subscribe",
        r"chia sáº» video nÃ y",
        r"cáº£m Æ¡n cÃ¡c báº¡n Ä‘Ã£ xem",
        r"mÃ¬nh sáº½ gáº·p láº¡i cÃ¡c báº¡n trong video sau"
    ]
    
    # Lá»c bá» nhá»¯ng cá»¥m tá»« khÃ´ng liÃªn quan
    for phrase in irrelevant_phrases:
        text = re.sub(phrase, "", text, flags=re.IGNORECASE)
    
    # XÃ³a khoáº£ng tráº¯ng dÆ° thá»«a sau khi lá»c
    text = re.sub(r'\s+', ' ', text).strip()
    
    # TÃ¡ch vÃ  lá»c tá»«ng cÃ¢u
    try:
        sentences = sent_tokenize(text)
        relevant_sentences = []
        
        for sentence in sentences:
            # Bá» qua cÃ¡c cÃ¢u quÃ¡ ngáº¯n
            if len(sentence.strip()) < 10:
                continue
                
            # Bá» qua cÃ¢u khÃ´ng cÃ³ thÃ´ng tin Ä‘Ã¡ng ká»ƒ
            words = sentence.lower().split()
            if len(set(words)) < 3:  # Ãt tá»« duy nháº¥t
                continue
                
            relevant_sentences.append(sentence)
        
        return ". ".join(relevant_sentences)
    except:
        return text

def simple_sent_tokenize(text):
    """HÃ m tÃ¡ch cÃ¢u Ä‘Æ¡n giáº£n thay tháº¿ cho NLTK sent_tokenize"""
    if not text:
        return []
        
    # TÃ¡ch theo cÃ¡c dáº¥u cÃ¢u thÆ°á»ng gáº·p
    text = text.replace('!', '.').replace('?', '.')
    # TÃ¡ch thÃ nh cÃ¡c cÃ¢u
    sentences = [s.strip() for s in text.split('.') if s.strip()]
    return sentences

def textrank_scores(similarity_matrix, d=0.85, iterations=50):
    """
    TÃ­nh Ä‘iá»ƒm TextRank cho cÃ¡c cÃ¢u dá»±a trÃªn ma tráº­n tÆ°Æ¡ng Ä‘á»“ng
    d: há»‡ sá»‘ giáº£m (damping factor)
    iterations: sá»‘ vÃ²ng láº·p Ä‘á»ƒ há»™i tá»¥
    """
    n = len(similarity_matrix)
    if n == 0:
        return np.array([])
    
    # Chuáº©n hÃ³a ma tráº­n tÆ°Æ¡ng Ä‘á»“ng
    for i in range(n):
        if sum(similarity_matrix[i]) != 0:
            similarity_matrix[i] = similarity_matrix[i] / sum(similarity_matrix[i])
    
    # Khá»Ÿi táº¡o vector Ä‘iá»ƒm ban Ä‘áº§u
    scores = np.ones(n) / n
    
    # Láº·p cho Ä‘áº¿n khi há»™i tá»¥
    for _ in range(iterations):
        new_scores = (1 - d) / n + d * (similarity_matrix.T @ scores)
        # Kiá»ƒm tra há»™i tá»¥
        if np.allclose(new_scores, scores, rtol=1e-6):
            break
        scores = new_scores
    
    return scores

def summarize_transcript(transcript, ratio=0.3):
    """TÃ³m táº¯t vÄƒn báº£n sá»­ dá»¥ng TF-IDF Ä‘á»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c cÃ¢u quan trá»ng nháº¥t"""
    print("Äang tÃ³m táº¯t ná»™i dung vÄƒn báº£n...")
    
    # Kiá»ƒm tra Ä‘áº§u vÃ o
    if not transcript or len(transcript) < 10:
        return transcript
    
    # LÃ m sáº¡ch vÄƒn báº£n
    clean_text = clean_transcript(transcript)
    
    # TÃ¡ch vÄƒn báº£n thÃ nh cÃ¡c cÃ¢u
    try:
        sentences = nltk.sent_tokenize(clean_text)
    except Exception as e:
        print(f"Lá»—i NLTK: {e}, sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p tÃ¡ch cÃ¢u Ä‘Æ¡n giáº£n")
        sentences = simple_sent_tokenize(clean_text)
    
    if len(sentences) < 3:
        return clean_text
    
    # TÃ­nh Ä‘iá»ƒm TF-IDF cho má»—i cÃ¢u
    try:
        vectorizer = TfidfVectorizer(min_df=1)  # Äáº·t min_df=1 Ä‘á»ƒ trÃ¡nh lá»—i vá»›i vÄƒn báº£n ngáº¯n
        tfidf_matrix = vectorizer.fit_transform(sentences)
        sentence_scores = np.sum(tfidf_matrix.toarray(), axis=1)
    except ValueError as e:
        print(f"KhÃ´ng thá»ƒ phÃ¢n tÃ­ch TF-IDF: {e}, sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p tÃ³m táº¯t Ä‘Æ¡n giáº£n...")
        num_sentences = max(1, int(len(sentences) * ratio))
        return ". ".join(sentences[:num_sentences])
    
    # Sáº¯p xáº¿p cÃ¢u theo Ä‘iá»ƒm sá»‘ vÃ  chá»n ra top N% cÃ¢u quan trá»ng nháº¥t
    num_sentences = max(1, int(len(sentences) * ratio))
    ranked_sentences = [(score, i) for i, score in enumerate(sentence_scores)]
    ranked_sentences.sort(reverse=True)
    selected_indices = [i for _, i in ranked_sentences[:num_sentences]]
    selected_indices.sort()  # Sáº¯p xáº¿p láº¡i theo thá»© tá»± gá»‘c
    
    # Táº¡o vÄƒn báº£n tÃ³m táº¯t
    summary = ". ".join([sentences[i] for i in selected_indices])
    print(f"ğŸ“ ÄÃ£ tÃ³m táº¯t tá»« {len(sentences)} cÃ¢u xuá»‘ng {num_sentences} cÃ¢u ({ratio*100:.0f}%)")
    return summary

def enhanced_summarize_transcript(text, ratio=0.3):
    """TÃ³m táº¯t transcript vá»›i phÆ°Æ¡ng phÃ¡p TF-IDF vÃ  TextRank cáº£i tiáº¿n"""
    if not text or len(text.split()) < 10:
        return text
        
    try:
        # TÃ¡ch vÄƒn báº£n thÃ nh cÃ¡c cÃ¢u
        try:
            sentences = nltk.sent_tokenize(text)
        except:
            sentences = simple_sent_tokenize(text)
        
        # Náº¿u sá»‘ cÃ¢u quÃ¡ Ã­t, tráº£ vá» nguyÃªn vÄƒn báº£n
        if len(sentences) <= 3:
            return text
        
        # Sá»­ dá»¥ng TF-IDF Ä‘á»ƒ trá»ng sá»‘ tá»«
        try:
            # Thá»­ vá»›i cÃ¡c tham sá»‘ máº·c Ä‘á»‹nh
            vectorizer = TfidfVectorizer(max_df=0.95, min_df=1)  # Giáº£m min_df xuá»‘ng 1
            X = vectorizer.fit_transform(sentences)
        except ValueError as e:
            print(f"Lá»—i TF-IDF: {e}")
            # Náº¿u váº«n lá»—i, tráº£ vá» phÆ°Æ¡ng phÃ¡p Ä‘Æ¡n giáº£n
            return simple_summarize(text, ratio)
        
        # TÃ­nh toÃ¡n similarity matrix
        similarity_matrix = cosine_similarity(X)
        
        # Ãp dá»¥ng TextRank vá»›i networkx
        try:
            nx_graph = nx.from_numpy_array(similarity_matrix)
            scores = nx.pagerank(nx_graph)
            
            # Sáº¯p xáº¿p cÃ¢u theo Ä‘iá»ƒm vÃ  chá»n top N cÃ¢u
            ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)
            
            # Sá»‘ lÆ°á»£ng cÃ¢u cáº§n láº¥y
            num_sentences = max(1, int(len(sentences) * ratio))
            
            # Chá»n top N cÃ¢u vÃ  sáº¯p xáº¿p láº¡i theo thá»© tá»± xuáº¥t hiá»‡n gá»‘c
            top_sentence_indices = [sentences.index(ranked_sentences[i][1]) for i in range(min(num_sentences, len(ranked_sentences)))]
            top_sentence_indices.sort()
            
            # Táº¡o tÃ³m táº¯t
            summary = " ".join([sentences[i] for i in top_sentence_indices])
            
            return summary
        except Exception as e:
            print(f"Lá»—i TextRank: {e}")
            # Fallback to simpler TF-IDF method
            return summarize_transcript(text, ratio)
    except Exception as e:
        print(f"Lá»—i khi tÃ³m táº¯t: {e}")
        # Fallback to simplest method
        return simple_summarize(text, ratio)

def simple_summarize(text, ratio=0.3):
    """PhÆ°Æ¡ng phÃ¡p tÃ³m táº¯t Ä‘Æ¡n giáº£n khi phÆ°Æ¡ng phÃ¡p chÃ­nh gáº·p lá»—i"""
    if not text:
        return ""
        
    sentences = text.split('.')
    num_sentences = max(1, int(len(sentences) * ratio))
    return '. '.join(sentences[:num_sentences]) + ('.' if not sentences[0].endswith('.') else '')

def summarize_transcript_with_textrank(text, ratio=0.3):
    """TÃ³m táº¯t vÄƒn báº£n sá»­ dá»¥ng thuáº­t toÃ¡n TextRank"""
    if not text or len(text.split()) < 10:
        return text
        
    # TÃ¡ch vÄƒn báº£n thÃ nh cÃ¡c cÃ¢u
    try:
        sentences = nltk.sent_tokenize(text)
    except Exception as e:
        print(f"Lá»—i khi tÃ¡ch cÃ¢u vá»›i NLTK: {e}")
        sentences = simple_sent_tokenize(text)
    
    if len(sentences) <= 3:
        return text
    
    # Táº¡o ma tráº­n tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¡c cÃ¢u
    try:
        vectorizer = TfidfVectorizer()
        sentence_vectors = vectorizer.fit_transform(sentences)
        similarity_matrix = cosine_similarity(sentence_vectors)
        
        # Ãp dá»¥ng thuáº­t toÃ¡n PageRank (TextRank)
        graph = nx.from_numpy_array(similarity_matrix)
        scores = nx.pagerank(graph)
        
        # Sáº¯p xáº¿p cÃ¡c cÃ¢u theo Ä‘iá»ƒm sá»‘
        ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)
        
        # Chá»n sá»‘ cÃ¢u dá»±a theo tá»‰ lá»‡
        num_sentences = max(1, int(len(sentences) * ratio))
        
        # Láº¥y cÃ¡c cÃ¢u cÃ³ Ä‘iá»ƒm cao vÃ  sáº¯p xáº¿p láº¡i theo thá»© tá»± gá»‘c
        top_sentence_indices = [sentences.index(ranked_sentences[i][1]) for i in range(min(num_sentences, len(ranked_sentences)))]
        top_sentence_indices.sort()
        
        # Táº¡o tÃ³m táº¯t
        summary = " ".join([sentences[i] for i in top_sentence_indices])
        return summary
        
    except Exception as e:
        print(f"Lá»—i khi Ã¡p dá»¥ng TextRank: {e}")
        # Sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p dá»± phÃ²ng
        return summarize_transcript(text, ratio)

def validate_summary(summary):
    """Kiá»ƒm tra vÃ  lÃ m sáº¡ch báº£n tÃ³m táº¯t"""
    if not summary:
        return ""
    
    # XÃ³a cÃ¡c dÃ²ng trá»‘ng
    summary = re.sub(r'\n\s*\n', '\n\n', summary)
    
    # Äáº£m báº£o cÃ¡c cÃ¢u Ä‘Æ°á»£c viáº¿t hoa chá»¯ cÃ¡i Ä‘áº§u
    sentences = re.split(r'(?<=[.!?])\s+', summary)
    capitalized_sentences = []
    
    for sentence in sentences:
        if sentence:
            # Viáº¿t hoa chá»¯ cÃ¡i Ä‘áº§u tiÃªn
            capitalized = sentence[0].upper() + sentence[1:] if len(sentence) > 1 else sentence.upper()
            capitalized_sentences.append(capitalized)
    
    # Gá»™p láº¡i cÃ¡c cÃ¢u
    clean_summary = " ".join(capitalized_sentences)
    
    # Äáº£m báº£o káº¿t thÃºc báº±ng dáº¥u cháº¥m
    if clean_summary and clean_summary[-1] not in ['.', '!', '?']:
        clean_summary += '.'
    
    return clean_summary
