import os
import cv2
import numpy as np
import torch
import subprocess
import shutil
from tqdm import tqdm
from moviepy.editor import VideoFileClip, concatenate_videoclips

# Add new imports for optimized video processing
try:
    import decord
    from decord import VideoReader, cpu, gpu
    DECORD_AVAILABLE = True
    
    # Bu·ªôc s·ª≠ d·ª•ng GPU n·∫øu c√≥
    if torch.cuda.is_available():
        try:
            decord.bridge.set_bridge('torch')
            print("‚úÖ Decord s·∫Ω s·ª≠ d·ª•ng GPU v·ªõi PyTorch bridge")
            # Kh·ªüi t·∫°o GPU context s·ªõm ƒë·ªÉ ki·ªÉm tra
            test_ctx = decord.gpu(0)
            print("‚úÖ Decord GPU context kh·ªüi t·∫°o th√†nh c√¥ng")
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ kh·ªüi t·∫°o Decord GPU: {e}")
            decord.bridge.set_bridge('native')
            print("‚ö†Ô∏è Fallback to native bridge")
    else:
        decord.bridge.set_bridge('native')
        print("‚úÖ Decord s·∫Ω s·ª≠ d·ª•ng CPU v·ªõi native bridge")
except ImportError:
    DECORD_AVAILABLE = False
    print("‚ö†Ô∏è Decord library not found. Using slower OpenCV for frame extraction. Install with: pip install decord")

def detect_scene_changes(video_path, threshold=30, min_scene_duration=1.0):
    """Ph√°t hi·ªán s·ª± thay ƒë·ªïi c·∫£nh trong video d·ª±a tr√™n s·ª± kh√°c bi·ªát gi·ªØa c√°c frame"""
    print(f"Ph√¢n t√≠ch s·ª± thay ƒë·ªïi c·∫£nh trong video {video_path}...")
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"Kh√¥ng th·ªÉ m·ªü file video: {video_path}")
        return []
        
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    prev_frame = None
    scene_changes = []
    min_frames_between_scenes = int(fps * min_scene_duration)
    last_scene_frame = -min_frames_between_scenes
    
    # Skip frames to speed up processing
    frame_skip = max(1, int(fps / 4))  # Process ~4 frames per second
    
    for frame_idx in tqdm(range(0, total_frames, frame_skip), desc="Ph√°t hi·ªán thay ƒë·ªïi c·∫£nh"):
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        
        if not ret:
            break
            
        # Convert to grayscale and apply blur to reduce noise
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        gray = cv2.GaussianBlur(gray, (21, 21), 0)
        
        if prev_frame is None:
            prev_frame = gray
            continue
            
        # Calculate difference between frames
        frame_diff = cv2.absdiff(prev_frame, gray)
        _, thresh = cv2.threshold(frame_diff, 25, 255, cv2.THRESH_BINARY)
        
        # Calculate percentage of changed pixels
        change_percent = np.count_nonzero(thresh) * 100 / thresh.size
        
        # Detect significant changes
        if change_percent > threshold and (frame_idx - last_scene_frame) > min_frames_between_scenes:
            time_sec = frame_idx / fps
            scene_changes.append(time_sec)
            last_scene_frame = frame_idx
            
        prev_frame = gray
        
    cap.release()
    print(f"T√¨m th·∫•y {len(scene_changes)} thay ƒë·ªïi c·∫£nh.")
    return scene_changes

def detect_motion_activity(video_path, window_size=15, activity_threshold=0.2):
    """Ph√°t hi·ªán c√°c ƒëo·∫°n c√≥ nhi·ªÅu chuy·ªÉn ƒë·ªông trong video"""
    print(f"Ph√¢n t√≠ch c√°c ƒëo·∫°n c√≥ chuy·ªÉn ƒë·ªông m·∫°nh trong video {video_path}...")
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"Kh√¥ng th·ªÉ m·ªü file video: {video_path}")
        return []
        
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    prev_frame = None
    motion_scores = []
    
    # Skip frames to speed up processing
    frame_skip = max(1, int(fps / 4))  # Process ~4 frames per second
    
    for frame_idx in tqdm(range(0, total_frames, frame_skip), desc="Ph√¢n t√≠ch chuy·ªÉn ƒë·ªông"):
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        
        if not ret:
            break
            
        # Convert to grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        if prev_frame is None:
            prev_frame = gray
            motion_scores.append(0)
            continue
            
        # Calculate optical flow using Farneback method
        flow = cv2.calcOpticalFlowFarneback(prev_frame, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Calculate motion magnitude
        magnitude, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        motion_score = np.mean(magnitude)
        motion_scores.append(motion_score)
        
        prev_frame = gray
        
    cap.release()
    
    # Apply moving average to smooth motion scores
    smoothed_scores = np.convolve(motion_scores, np.ones(window_size)/window_size, mode='same')
    
    # Find segments with high motion
    high_motion_segments = []
    in_high_motion = False
    start_time = 0
    
    for i, score in enumerate(smoothed_scores):
        time_sec = i * frame_skip / fps
        
        if score > activity_threshold and not in_high_motion:
            in_high_motion = True
            start_time = time_sec
        elif score <= activity_threshold and in_high_motion:
            in_high_motion = False
            high_motion_segments.append((start_time, time_sec))
    
    # Add the last segment if needed
    if in_high_motion:
        high_motion_segments.append((start_time, total_frames / fps))
    
    # Merge segments that are close to each other
    merged_segments = []
    merge_threshold = 2.0  # seconds
    
    for segment in high_motion_segments:
        if not merged_segments or segment[0] - merged_segments[-1][1] > merge_threshold:
            merged_segments.append(list(segment))
        else:
            merged_segments[-1][1] = segment[1]
    
    print(f"T√¨m th·∫•y {len(merged_segments)} ƒëo·∫°n c√≥ chuy·ªÉn ƒë·ªông m·∫°nh.")
    return merged_segments

def detect_climax_scenes(video_path, threshold_multiplier=1.5, window_size=30):
    """Ph√°t hi·ªán c√°c c·∫£nh cao tr√†o d·ª±a tr√™n chuy·ªÉn ƒë·ªông v√† thay ƒë·ªïi c·∫£nh"""
    print("ƒêang ph√¢n t√≠ch video ƒë·ªÉ t√¨m c·∫£nh cao tr√†o...")
    
    # Ph√°t hi·ªán chuy·ªÉn ƒë·ªông
    motion_segments = detect_motion_activity(video_path, window_size=window_size, 
                                           activity_threshold=0.3)
    
    # Ph√°t hi·ªán thay ƒë·ªïi c·∫£nh
    scene_changes = detect_scene_changes(video_path, threshold=35)
    
    # K·∫øt h·ª£p th√¥ng tin ƒë·ªÉ t√¨m c·∫£nh cao tr√†o
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    cap.release()
    
    # T·∫°o m·∫£ng ƒëi·ªÉm cao tr√†o theo th·ªùi gian
    climax_score = np.zeros(int(total_frames / fps) + 1)
    
    # Th√™m ƒëi·ªÉm cho ƒëo·∫°n c√≥ chuy·ªÉn ƒë·ªông m·∫°nh
    for start, end in motion_segments:
        start_idx = int(start)
        end_idx = int(end) + 1
        climax_score[start_idx:end_idx] += 1
    
    # Th√™m ƒëi·ªÉm cho th·ªùi ƒëi·ªÉm c√≥ thay ƒë·ªïi c·∫£nh
    for time in scene_changes:
        idx = int(time)
        if idx < len(climax_score):
            # Th√™m ƒëi·ªÉm v√†o kho·∫£ng 5 gi√¢y xung quanh thay ƒë·ªïi c·∫£nh
            start_idx = max(0, idx - 2)
            end_idx = min(len(climax_score), idx + 3)
            climax_score[start_idx:end_idx] += 0.5
    
    # L√†m m·ªãn ƒëi·ªÉm b·∫±ng moving average
    window = np.ones(window_size) / window_size
    smoothed_score = np.convolve(climax_score, window, mode='same')
    
    # T√¨m ng∆∞·ª°ng ƒë·ªÉ x√°c ƒë·ªãnh cao tr√†o
    mean_score = np.mean(smoothed_score)
    threshold = mean_score * threshold_multiplier
    
    # X√°c ƒë·ªãnh c√°c ƒëo·∫°n cao tr√†o
    climax_segments = []
    in_climax = False
    start_time = 0
    
    for i, score in enumerate(smoothed_score):
        if score > threshold and not in_climax:
            in_climax = True
            start_time = i
        elif (score <= threshold or i == len(smoothed_score) - 1) and in_climax:
            in_climax = False
            # Ch·ªâ l·∫•y c√°c ƒëo·∫°n ƒë·ªß d√†i (√≠t nh·∫•t 3 gi√¢y)
            if i - start_time >= 3:
                climax_segments.append([start_time, i])
    
    print(f"T√¨m th·∫•y {len(climax_segments)} c·∫£nh cao tr√†o.")
    return climax_segments

def extract_climax_scenes(video_path, output_path, buffer_time=2.0):
    """Tr√≠ch xu·∫•t c√°c c·∫£nh cao tr√†o t·ª´ video"""
    climax_segments = detect_climax_scenes(video_path)
    
    if not climax_segments:
        print("Kh√¥ng t√¨m th·∫•y c·∫£nh cao tr√†o n√†o.")
        return False
    
    # Th√™m buffer time
    for segment in climax_segments:
        segment[0] = max(0, segment[0] - buffer_time)
        segment[1] += buffer_time
    
    # Gh√©p c√°c ƒëo·∫°n video
    clip = VideoFileClip(video_path)
    clips = [clip.subclip(start, end) for start, end in climax_segments]
    
    if clips:
        final_clip = concatenate_videoclips(clips)
        final_clip.write_videofile(output_path, codec="libx264")
        print(f"‚úÖ ƒê√£ xu·∫•t c√°c c·∫£nh cao tr√†o v√†o: {output_path}")
        return True
    else:
        print("‚ùå Kh√¥ng th·ªÉ t·∫°o video c·∫£nh cao tr√†o.")
        return False

def detect_motion_with_object_tracking(video_path, model, device, window_size=15, activity_threshold=0.2):
    """
    Ph√°t hi·ªán c√°c ƒëo·∫°n c√≥ nhi·ªÅu chuy·ªÉn ƒë·ªông c·ªßa nh√¢n v·∫≠t ch√≠nh trong video
    K·∫øt h·ª£p object detection v√† optical flow ƒë·ªÉ tr√°nh nh·∫≠n nh·∫ßm chuy·ªÉn ƒë·ªông m√°y quay
    """
    print(f"Ph√¢n t√≠ch chuy·ªÉn ƒë·ªông c·ªßa nh√¢n v·∫≠t ch√≠nh trong video {video_path}...")
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"Kh√¥ng th·ªÉ m·ªü file video: {video_path}")
        return []
        
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    prev_frame = None
    motion_scores = []
    object_motion_scores = []  # ƒêi·ªÉm chuy·ªÉn ƒë·ªông c·ªßa ƒë·ªëi t∆∞·ª£ng
    camera_motion_scores = []  # ƒêi·ªÉm chuy·ªÉn ƒë·ªông c·ªßa camera
    
    # Chu·∫©n b·ªã m√¥ h√¨nh object detection n·∫øu kh√¥ng truy·ªÅn v√†o
    if model is None:
        try:
            model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)
            model.to(device)
            model.eval()
        except Exception as e:
            print(f"Kh√¥ng th·ªÉ t·∫£i m√¥ h√¨nh YOLOv5: {e}")
            print("S·ª≠ d·ª•ng ch·ªâ optical flow m√† kh√¥ng c√≥ object detection")
            model = None
    
    # Skip frames to speed up processing
    frame_skip = max(1, int(fps / 4))  # Process ~4 frames per second
    
    # L∆∞u tr·ªØ v·ªã tr√≠ ƒë·ªëi t∆∞·ª£ng ·ªü frame tr∆∞·ªõc
    prev_objects = []
    
    for frame_idx in tqdm(range(0, total_frames, frame_skip), desc="Ph√¢n t√≠ch chuy·ªÉn ƒë·ªông"):
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        
        if not ret:
            break
            
        # Convert to grayscale for optical flow
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng (ng∆∞·ªùi) trong frame hi·ªán t·∫°i
        current_objects = []
        if model is not None:
            try:
                # Resize frame for model
                frame_resized = cv2.resize(frame, (640, 640))
                
                # Run object detection
                results = model(frame_resized)
                detections = results.xyxy[0].cpu().numpy()
                
                # Filter for persons (class 0)
                for detection in detections:
                    if int(detection[5]) == 0 and float(detection[4]) > 0.3:
                        # Convert bounding box to original frame size
                        x1, y1, x2, y2 = detection[:4]
                        h, w = frame.shape[:2]
                        
                        # Calculate bbox center for tracking movement
                        center_x = (x1 + x2) / 2
                        center_y = (y1 + y2) / 2
                        current_objects.append((center_x, center_y, x2-x1, y2-y1))
            except Exception as e:
                print(f"L·ªói khi ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng: {e}")
        
        if prev_frame is None:
            prev_frame = gray
            prev_objects = current_objects
            motion_scores.append(0)
            object_motion_scores.append(0)
            camera_motion_scores.append(0)
            continue
            
        # Calculate optical flow using Farneback method
        flow = cv2.calcOpticalFlowFarneback(prev_frame, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Calculate overall motion magnitude
        magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        overall_motion = np.mean(magnitude)
        motion_scores.append(overall_motion)
        
        # T√°ch bi·ªát chuy·ªÉn ƒë·ªông ƒë·ªëi t∆∞·ª£ng v√† chuy·ªÉn ƒë·ªông camera
        object_motion = 0
        camera_motion = overall_motion  # Gi·∫£ s·ª≠ ban ƒë·∫ßu t·∫•t c·∫£ l√† camera motion
        
        # N·∫øu c√≥ ƒë·ªëi t∆∞·ª£ng ƒë∆∞·ª£c ph√°t hi·ªán, t√≠nh to√°n chuy·ªÉn ƒë·ªông c·ªßa ch√∫ng
        if current_objects:
            # T√≠nh t·ªëc ƒë·ªô di chuy·ªÉn c·ªßa c√°c ƒë·ªëi t∆∞·ª£ng
            if prev_objects:
                # T√≠nh kho·∫£ng c√°ch trung b√¨nh di chuy·ªÉn c·ªßa c√°c ƒë·ªëi t∆∞·ª£ng
                total_movement = 0
                matched_objects = 0
                
                # Gh√©p c·∫∑p ƒë·ªëi t∆∞·ª£ng gi·ªØa c√°c frame d·ª±a tr√™n IOU ho·∫∑c kho·∫£ng c√°ch
                for curr_obj in current_objects:
                    best_match = None
                    best_dist = float('inf')
                    
                    for prev_obj in prev_objects:
                        dist = np.sqrt((curr_obj[0] - prev_obj[0])**2 + (curr_obj[1] - prev_obj[1])**2)
                        if dist < best_dist:
                            best_dist = dist
                            best_match = prev_obj
                    
                    if best_match and best_dist < max(frame.shape) * 0.2:  # Ng∆∞·ª°ng h·ª£p l√Ω cho v·∫≠t th·ªÉ di chuy·ªÉn
                        total_movement += best_dist
                        matched_objects += 1
                
                if matched_objects > 0:
                    object_motion = total_movement / matched_objects
                    
                    # ∆Ø·ªõc t√≠nh chuy·ªÉn ƒë·ªông camera b·∫±ng c√°ch lo·∫°i b·ªè chuy·ªÉn ƒë·ªông ƒë·ªëi t∆∞·ª£ng
                    # C√†ng nhi·ªÅu ƒë·ªëi t∆∞·ª£ng di chuy·ªÉn, ∆∞·ªõc t√≠nh c√†ng ch√≠nh x√°c
                    if matched_objects >= 2:
                        camera_motion = max(0, overall_motion - object_motion * 0.7)
            
        object_motion_scores.append(object_motion)
        camera_motion_scores.append(camera_motion)
        
        prev_frame = gray
        prev_objects = current_objects
        
    cap.release()
    
    # C·∫£i thi·ªán: T√≠nh to√°n t·ªëc ƒë·ªô thay ƒë·ªïi khung h√¨nh
    frame_change_rate = []
    for i in range(1, len(motion_scores)):
        frame_change_rate.append(abs(motion_scores[i] - motion_scores[i-1]))
    
    # Th√™m gi√° tr·ªã 0 ·ªü ƒë·∫ßu ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªô d√†i vector
    frame_change_rate.insert(0, 0)
    
    # K·∫øt h·ª£p c√°c ch·ªâ s·ªë ƒë·ªÉ x√°c ƒë·ªãnh c·∫£nh cao tr√†o
    combined_scores = []
    for i in range(len(motion_scores)):
        # C√¥ng th·ª©c k·∫øt h·ª£p: 
        # - Coi tr·ªçng chuy·ªÉn ƒë·ªông ƒë·ªëi t∆∞·ª£ng
        # - Gi·∫£m tr·ªçng s·ªë cho chuy·ªÉn ƒë·ªông camera
        # - Coi tr·ªçng t·ªëc ƒë·ªô thay ƒë·ªïi khung h√¨nh (ƒë·∫∑c tr∆∞ng c·ªßa c·∫£nh cao tr√†o)
        if i < len(object_motion_scores) and i < len(camera_motion_scores) and i < len(frame_change_rate):
            object_weight = 2.0
            camera_penalty = 0.5
            change_rate_weight = 1.5
            
            score = (object_motion_scores[i] * object_weight - 
                    camera_motion_scores[i] * camera_penalty + 
                    frame_change_rate[i] * change_rate_weight)
            
            # ƒê·∫£m b·∫£o kh√¥ng c√≥ gi√° tr·ªã √¢m
            score = max(0, score)
            combined_scores.append(score)
        else:
            combined_scores.append(0)
    
    # Apply moving average to smooth scores
    smoothed_scores = np.convolve(combined_scores, np.ones(window_size)/window_size, mode='same')
    
    # Find segments with significant motion
    high_motion_segments = []
    in_high_motion = False
    start_time = 0
    
    # T√≠nh ng∆∞·ª°ng th√≠ch ·ª©ng
    mean_score = np.mean(smoothed_scores)
    std_score = np.std(smoothed_scores)
    adaptive_threshold = mean_score + std_score * 0.8  # ƒêi·ªÅu ch·ªânh h·ªá s·ªë n√†y theo ƒë·ªô nh·∫°y mong mu·ªën
    print(f"Ng∆∞·ª°ng ph√°t hi·ªán chuy·ªÉn ƒë·ªông th√≠ch ·ª©ng: {adaptive_threshold:.4f}")
    
    # S·ª≠ d·ª•ng ng∆∞·ª°ng th√≠ch ·ª©ng ho·∫∑c ng∆∞·ª°ng t·ªëi thi·ªÉu
    final_threshold = max(activity_threshold, adaptive_threshold)
    
    for i, score in enumerate(smoothed_scores):
        time_sec = i * frame_skip / fps
        
        if score > final_threshold and not in_high_motion:
            in_high_motion = True
            start_time = time_sec
        elif (score <= final_threshold * 0.7 or i == len(smoothed_scores) - 1) and in_high_motion:
            in_high_motion = False
            # Ch·ªâ l·∫•y c√°c ƒëo·∫°n ƒë·ªß d√†i (√≠t nh·∫•t 2 gi√¢y)
            if time_sec - start_time >= 2:
                high_motion_segments.append((start_time, time_sec))
    
    # Merge segments that are close to each other
    merged_segments = []
    merge_threshold = 3.0  # seconds
    
    for segment in high_motion_segments:
        if not merged_segments or segment[0] - merged_segments[-1][1] > merge_threshold:
            merged_segments.append(list(segment))
        else:
            merged_segments[-1][1] = segment[1]
    
    print(f"T√¨m th·∫•y {len(merged_segments)} ƒëo·∫°n c√≥ chuy·ªÉn ƒë·ªông m·∫°nh c·ªßa nh√¢n v·∫≠t ch√≠nh.")
    return merged_segments

def detect_climax_scenes_improved(video_path, model, device, threshold_multiplier=1.3, window_size=30):
    """
    Ph√°t hi·ªán c√°c c·∫£nh cao tr√†o d·ª±a tr√™n nhi·ªÅu ti√™u ch√≠ ph·ª©c h·ª£p:
    1. Chuy·ªÉn ƒë·ªông c·ªßa nh√¢n v·∫≠t ch√≠nh (theo d√µi ƒë·ªëi t∆∞·ª£ng)
    2. T·ªëc ƒë·ªô thay ƒë·ªïi khung h√¨nh (quan tr·ªçng cho c·∫£nh h√†nh ƒë·ªông)
    3. Thay ƒë·ªïi c·∫£nh nhanh (c·∫Øt nhanh gi·ªØa c√°c c·∫£nh)
    4. T·ª∑ l·ªá c·∫£nh (∆∞u ti√™n c·∫≠n c·∫£nh, trung c·∫£nh)
    """
    print("ƒêang ph√¢n t√≠ch video ƒë·ªÉ t√¨m c·∫£nh cao tr√†o...")
    
    # Ph√°t hi·ªán chuy·ªÉn ƒë·ªông k·∫øt h·ª£p theo d√µi ƒë·ªëi t∆∞·ª£ng
    motion_segments = detect_motion_with_object_tracking(
        video_path, model, device, window_size=window_size, activity_threshold=0.25)
    
    # Ph√°t hi·ªán thay ƒë·ªïi c·∫£nh (gi·∫£m threshold ƒë·ªÉ nh·∫°y h∆°n v·ªõi thay ƒë·ªïi c·∫£nh)
    scene_changes = detect_scene_changes(video_path, threshold=30, min_scene_duration=0.8)
    
    # Ph√¢n t√≠ch m·∫≠t ƒë·ªô thay ƒë·ªïi c·∫£nh
    scene_density = analyze_scene_change_density(scene_changes, window_size=15)
    
    # K·∫øt h·ª£p th√¥ng tin ƒë·ªÉ t√¨m c·∫£nh cao tr√†o
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps
    cap.release()
    
    # T·∫°o m·∫£ng ƒëi·ªÉm cao tr√†o theo th·ªùi gian
    duration_seconds = int(duration) + 1
    climax_score = np.zeros(duration_seconds)
    
    # Th√™m ƒëi·ªÉm cho ƒëo·∫°n c√≥ chuy·ªÉn ƒë·ªông m·∫°nh c·ªßa nh√¢n v·∫≠t
    for start, end in motion_segments:
        start_idx = int(start)
        end_idx = min(int(end) + 1, duration_seconds)
        climax_score[start_idx:end_idx] += 2.0  # Tr·ªçng s·ªë cao h∆°n
    
    # Th√™m ƒëi·ªÉm cho th·ªùi ƒëi·ªÉm c√≥ m·∫≠t ƒë·ªô thay ƒë·ªïi c·∫£nh cao
    for i, density in enumerate(scene_density):
        if i < duration_seconds:
            # M·∫≠t ƒë·ªô thay ƒë·ªïi c·∫£nh cao th∆∞·ªùng l√† d·∫•u hi·ªáu c·ªßa c·∫£nh h√†nh ƒë·ªông
            climax_score[i] += density * 1.5
    
    # L√†m m·ªãn ƒëi·ªÉm b·∫±ng moving average
    window = np.ones(window_size) / window_size
    smoothed_score = np.convolve(climax_score, window, mode='same')
    
    # T√≠nh ng∆∞·ª°ng th√≠ch ·ª©ng
    non_zero_scores = smoothed_score[smoothed_score > 0]
    if len(non_zero_scores) > 0:
        mean_score = np.mean(non_zero_scores)
        threshold = mean_score * threshold_multiplier
    else:
        # Fallback n·∫øu kh√¥ng c√≥ segment n√†o c√≥ ƒëi·ªÉm
        mean_score = 0.1
        threshold = 0.15
    
    print(f"ƒêi·ªÉm trung b√¨nh: {mean_score:.3f}, Ng∆∞·ª°ng x√°c ƒë·ªãnh cao tr√†o: {threshold:.3f}")
    
    # X√°c ƒë·ªãnh c√°c ƒëo·∫°n cao tr√†o v·ªõi kh·∫£ nƒÉng ch·ªãu ƒë·ª±ng t·ªët h∆°n
    climax_segments = []
    in_climax = False
    start_time = 0
    low_count = 0  # ƒê·∫øm s·ªë l∆∞·ª£ng frame li√™n ti·∫øp d∆∞·ªõi ng∆∞·ª°ng
    
    for i, score in enumerate(smoothed_score):
        if score > threshold and not in_climax:
            in_climax = True
            start_time = i
            low_count = 0
        elif score > threshold * 0.7 and in_climax:
            # Reset b·ªô ƒë·∫øm n·∫øu ƒëi·ªÉm v·∫´n cao
            low_count = 0
        elif score <= threshold * 0.7 and in_climax:
            low_count += 1
            # Ch·ªâ k·∫øt th√∫c ƒëo·∫°n cao tr√†o n·∫øu ƒëi·ªÉm th·∫•p li√™n t·ª•c
            if low_count >= 5 or i == len(smoothed_score) - 1:
                in_climax = False
                # Ch·ªâ l·∫•y c√°c ƒëo·∫°n ƒë·ªß d√†i (√≠t nh·∫•t 3 gi√¢y)
                if i - start_time >= 3:
                    climax_segments.append([start_time, i-low_count+1])
                low_count = 0
    
    # Th√™m ƒëo·∫°n cao tr√†o cu·ªëi c√πng n·∫øu k·∫øt th√∫c video trong tr·∫°ng th√°i cao tr√†o
    if in_climax and duration_seconds - start_time >= 3:
        climax_segments.append([start_time, duration_seconds-1])
    
    print(f"T√¨m th·∫•y {len(climax_segments)} c·∫£nh cao tr√†o.")
    return climax_segments

def analyze_scene_change_density(scene_changes, window_size=15):
    """Ph√¢n t√≠ch m·∫≠t ƒë·ªô thay ƒë·ªïi c·∫£nh theo th·ªùi gian"""
    if not scene_changes:
        return []
        
    max_time = int(scene_changes[-1]) + window_size
    density = np.zeros(max_time)
    
    # T·∫°o h√†m m·∫≠t ƒë·ªô Gaussian cho t·ª´ng thay ƒë·ªïi c·∫£nh
    for change_time in scene_changes:
        time_int = int(change_time)
        # Th√™m gi√° tr·ªã Gaussian xung quanh ƒëi·ªÉm thay ƒë·ªïi c·∫£nh
        for i in range(max(0, time_int - window_size//2), min(max_time, time_int + window_size//2 + 1)):
            # Gaussian weight based on distance
            weight = np.exp(-0.5 * ((i - time_int) / (window_size/5))**2)
            density[i] += weight
    
    # Normalize
    if np.max(density) > 0:
        density = density / np.max(density)
        
    return density

def extract_climax_scenes_improved(video_path, output_path, model, device, buffer_time=2.0, threshold_multiplier=1.0):
    climax_segments = detect_climax_scenes_improved(video_path, model, device, threshold_multiplier)
    
    if not climax_segments:
        print("Kh√¥ng t√¨m th·∫•y c·∫£nh cao tr√†o")
        return False, []
    
    # Th√™m buffer time v√† ƒë·∫£m b·∫£o th·ªùi l∆∞·ª£ng h·ª£p l√Ω
    clip = VideoFileClip(video_path)
    duration = clip.duration
    
    for i in range(len(climax_segments)):
        start, end = climax_segments[i]
        start = max(0, start - buffer_time)
        end = min(duration, end + buffer_time)
        climax_segments[i] = [start, end]
    
    # Gh√©p c√°c ƒëo·∫°n video
    clips = []
    for i, (start, end) in enumerate(climax_segments):
        print(f"C·∫£nh cao tr√†o {i+1}: {start:.1f}s - {end:.1f}s")
        clips.append(clip.subclip(start, end))
    
    if clips:
        print(f"T·∫°o video v·ªõi {len(clips)} c·∫£nh cao tr√†o...")
        final_video = concatenate_videoclips(clips)
        final_video.write_videofile(output_path, codec="libx264")
        final_video.close()
        clip.close()
        
        return True, climax_segments
    else:
        print("Kh√¥ng th·ªÉ t·∫°o video t·ª´ c√°c c·∫£nh cao tr√†o")
        clip.close()
        return False, []

def detect_scene_changes_optimized(video_path, threshold=25):
    """Phi√™n b·∫£n t·ªëi ∆∞u c·ªßa ph√°t hi·ªán thay ƒë·ªïi c·∫£nh"""
    print(f"Ph√°t hi·ªán thay ƒë·ªïi c·∫£nh (phi√™n b·∫£n t·ªëi ∆∞u)...")
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"Kh√¥ng th·ªÉ m·ªü file video: {video_path}")
        return []
        
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    prev_frame = None
    scene_changes = []
    
    # TƒÉng frame_skip ƒë·ªÉ ch·ªâ x·ª≠ l√Ω 1 frame m·ªói gi√¢y 
    frame_skip = max(1, int(fps))
    
    for frame_idx in tqdm(range(0, total_frames, frame_skip), desc="Ph√°t hi·ªán thay ƒë·ªïi c·∫£nh"):
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        
        if not ret:
            break
            
        # Gi·∫£m k√≠ch th∆∞·ªõc frame ƒë·ªÉ tƒÉng t·ªëc x·ª≠ l√Ω
        small_frame = cv2.resize(frame, (160, 90))  # Gi·∫£m ƒë·ªô ph√¢n gi·∫£i h∆°n n·ªØa
        
        # Convert to grayscale and apply blur to reduce noise
        gray = cv2.cvtColor(small_frame, cv2.COLOR_BGR2GRAY)
        
        if prev_frame is None:
            prev_frame = gray
            continue
            
        # Calculate difference between frames - kh√¥ng c·∫ßn threshold
        frame_diff = cv2.absdiff(prev_frame, gray)
        
        # Calculate percentage of changed pixels directly from diff
        change_percent = np.mean(frame_diff) * 100 / 255
        
        # Detect significant changes
        if change_percent > threshold:
            time_sec = frame_idx / fps
            scene_changes.append(time_sec)
            
        prev_frame = gray
        
    cap.release()
    print(f"T√¨m th·∫•y {len(scene_changes)} thay ƒë·ªïi c·∫£nh.")
    return scene_changes

def detect_motion_activity_optimized(video_path, window_size=15, activity_threshold=0.2, device="cuda"):
    """
    Phi√™n b·∫£n t·ªëi ∆∞u c·ªßa ph√°t hi·ªán chuy·ªÉn ƒë·ªông s·ª≠ d·ª•ng:
    1. Frame skip cao h∆°n (1 frame/gi√¢y)
    2. Frame Difference thay cho Optical Flow
    3. PyTorch tensor x·ª≠ l√Ω tr√™n GPU
    """
    print(f"Ph√¢n t√≠ch chuy·ªÉn ƒë·ªông v·ªõi ph∆∞∆°ng ph√°p t·ªëi ∆∞u...")
    
    import torch
    
    # M·ªü video
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"Kh√¥ng th·ªÉ m·ªü file video: {video_path}")
        return []
        
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps
    
    # TƒÉng frame_skip ƒë·ªÉ ch·ªâ x·ª≠ l√Ω 1 frame m·ªói gi√¢y
    frame_skip = max(1, int(fps))  # ƒê√¢y l√† thay ƒë·ªïi ch√≠nh - x·ª≠ l√Ω 1 frame/gi√¢y
    print(f"Frame skip: {frame_skip} (1 frame m·ªói gi√¢y)")
    
    # X√°c ƒë·ªãnh s·ªë l∆∞·ª£ng frame c·∫ßn x·ª≠ l√Ω v√† t·∫°o progress bar
    num_frames = int(total_frames / frame_skip)
    
    prev_frame = None
    motion_scores = []
    
    # T·∫°o m·ªôt tensor r·ªóng tr√™n device ƒë·ªÉ ki·ªÉm tra xem GPU c√≥ s·∫µn s√†ng kh√¥ng
    try:
        if device == "cuda" and torch.cuda.is_available():
            # Kh·ªüi t·∫°o tensor r·ªóng ƒë·ªÉ x√°c nh·∫≠n GPU ho·∫°t ƒë·ªông
            test_tensor = torch.zeros((1,), device=device)
            del test_tensor  # Gi·∫£i ph√≥ng ngay
            print("‚úÖ S·ª≠ d·ª•ng GPU ƒë·ªÉ x·ª≠ l√Ω h√¨nh ·∫£nh")
        else:
            device = "cpu"
            print("‚ö†Ô∏è Kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c GPU, s·ª≠ d·ª•ng CPU")
    except Exception as e:
        device = "cpu"
        print(f"‚ö†Ô∏è L·ªói khi ki·ªÉm tra GPU: {e}, s·ª≠ d·ª•ng CPU")
    
    # X·ª≠ l√Ω t·ª´ng frame
    for frame_idx in tqdm(range(0, total_frames, frame_skip), desc="Ph√¢n t√≠ch chuy·ªÉn ƒë·ªông"):
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        
        if not ret:
            break
            
        # Gi·∫£m k√≠ch th∆∞·ªõc frame ƒë·ªÉ tƒÉng t·ªëc x·ª≠ l√Ω
        small_frame = cv2.resize(frame, (320, 180))
        
        # Chuy·ªÉn sang grayscale
        gray = cv2.cvtColor(small_frame, cv2.COLOR_BGR2GRAY)
        
        if prev_frame is None:
            prev_frame = gray
            motion_scores.append(0)
            continue
        
        try:
            if device == "cuda":
                # Chuy·ªÉn frame sang PyTorch tensor tr√™n GPU
                prev_tensor = torch.tensor(prev_frame, dtype=torch.float32, device=device)
                curr_tensor = torch.tensor(gray, dtype=torch.float32, device=device)
                
                # T√≠nh frame difference b·∫±ng PyTorch
                diff_tensor = torch.abs(curr_tensor - prev_tensor)
                motion_score = torch.mean(diff_tensor).item()
            else:
                # S·ª≠ d·ª•ng numpy cho CPU processing
                frame_diff = cv2.absdiff(prev_frame, gray)
                motion_score = np.mean(frame_diff)
        except Exception as e:
            print(f"L·ªói khi x·ª≠ l√Ω frame {frame_idx}: {e}")
            # Fallback to CPU
            frame_diff = cv2.absdiff(prev_frame, gray)
            motion_score = np.mean(frame_diff)
            
        # ƒêi·ªÅu ch·ªânh h·ªá s·ªë ƒë·ªÉ motion score t∆∞∆°ng ƒë∆∞∆°ng v·ªõi ph∆∞∆°ng ph√°p optical flow
        motion_score *= 5.0  # ƒêi·ªÅu ch·ªânh h·ªá s·ªë ƒë·ªÉ c√≥ thang ƒëi·ªÉm t∆∞∆°ng t·ª±
        motion_scores.append(motion_score)
        
        prev_frame = gray
        
        # Gi·∫£i ph√≥ng b·ªô nh·ªõ GPU ƒë·ªãnh k·ª≥
        if device == "cuda" and frame_idx % 100 == 0:
            torch.cuda.empty_cache()
    
    cap.release()
    
    # Clean up CUDA memory
    if device == "cuda":
        torch.cuda.empty_cache()
    
    # Apply moving average to smooth motion scores
    smoothed_scores = np.convolve(motion_scores, np.ones(window_size)/window_size, mode='same')
    
    # Find segments with high motion
    high_motion_segments = []
    in_high_motion = False
    start_time = 0
    
    for i, score in enumerate(smoothed_scores):
        time_sec = i * frame_skip / fps
        
        if score > activity_threshold and not in_high_motion:
            in_high_motion = True
            start_time = time_sec
        elif score <= activity_threshold and in_high_motion:
            in_high_motion = False
            high_motion_segments.append((start_time, time_sec))
    
    # Add the last segment if needed
    if in_high_motion:
        high_motion_segments.append((start_time, duration))
    
    # Merge segments that are close to each other
    merged_segments = []
    merge_threshold = 3.0  # seconds
    
    for segment in high_motion_segments:
        if not merged_segments or segment[0] - merged_segments[-1][1] > merge_threshold:
            merged_segments.append(list(segment))
        else:
            merged_segments[-1][1] = segment[1]
    
    print(f"T√¨m th·∫•y {len(merged_segments)} ƒëo·∫°n c√≥ chuy·ªÉn ƒë·ªông m·∫°nh.")
    return merged_segments

def detect_climax_scenes_fast(video_path, model=None, device="cuda", threshold_multiplier=1.0):
    """Phi√™n b·∫£n t·ªëi ∆∞u v√† nhanh h∆°n c·ªßa ph√°t hi·ªán c·∫£nh cao tr√†o"""
    print("ƒêang ph√¢n t√≠ch video ƒë·ªÉ t√¨m c·∫£nh cao tr√†o (phi√™n b·∫£n t·ªëi ∆∞u)...")
    
    # Ki·ªÉm tra c√≥ s·ª≠ d·ª•ng GPU kh√¥ng
    use_cuda = device == "cuda" and torch.cuda.is_available()
    if use_cuda:
        print("‚úÖ S·ª≠ d·ª•ng GPU ƒë·ªÉ tƒÉng t·ªëc ph√°t hi·ªán c·∫£nh cao tr√†o")
    
    # 1. Ph√°t hi·ªán chuy·ªÉn ƒë·ªông v·ªõi ph∆∞∆°ng ph√°p t·ªëi ∆∞u
    motion_segments = detect_motion_activity_optimized(
        video_path, window_size=15, activity_threshold=0.3, device=device)
    
    # 2. Ph√°t hi·ªán thay ƒë·ªïi c·∫£nh v·ªõi ph∆∞∆°ng ph√°p t·ªëi ∆∞u
    # TƒÉng frame_skip ƒë·ªÉ ch·ªâ ki·ªÉm tra √≠t frame h∆°n
    scene_changes = detect_scene_changes_optimized(video_path, threshold=25)
    
    # K·∫øt h·ª£p th√¥ng tin ƒë·ªÉ t√¨m c·∫£nh cao tr√†o
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps
    cap.release()
    
    # Chuy·ªÉn ƒë·ªïi k·∫øt qu·∫£ th√†nh c·∫£nh cao tr√†o
    climax_segments = []
    
    # ∆Øu ti√™n c√°c ƒëo·∫°n c√≥ chuy·ªÉn ƒë·ªông t·ª´ ph√¢n t√≠ch motion
    if motion_segments:
        print(f"S·ª≠ d·ª•ng {len(motion_segments)} ƒëo·∫°n chuy·ªÉn ƒë·ªông l√†m c·∫£nh cao tr√†o")
        climax_segments = [segment for segment in motion_segments]
    
    # N·∫øu kh√¥ng c√≥ ƒëo·∫°n chuy·ªÉn ƒë·ªông, d·ª±a v√†o scene changes
    elif scene_changes:
        print("D·ª±a v√†o scene changes ƒë·ªÉ x√°c ƒë·ªãnh c·∫£nh cao tr√†o")
        prev_time = 0
        
        for time in scene_changes:
            # T·∫°o ƒëo·∫°n ng·∫Øn (5 gi√¢y) xung quanh m·ªói scene change
            start_time = max(0, time - 2)
            end_time = min(duration, time + 3)
            
            # Ch·ªâ l·∫•y n·∫øu ƒëo·∫°n ƒë·ªß d√†i v√† kh√¥ng ch·ªìng l·∫•p v·ªõi ƒëo·∫°n tr∆∞·ªõc
            if end_time - start_time >= 2 and start_time > prev_time:
                climax_segments.append([start_time, end_time])
                prev_time = end_time
    
    # N·∫øu v·∫´n kh√¥ng t√¨m th·∫•y, l·∫•y m·ªôt s·ªë ƒëo·∫°n ƒë·ªÅu trong video
    if not climax_segments:
        print("Kh√¥ng t√¨m th·∫•y c·∫£nh cao tr√†o, l·∫•y m·ªôt s·ªë ƒëo·∫°n ƒë·ªÅu trong video")
        segment_duration = min(15, max(duration / 10, 5))
        
        # L·∫•y t·ªëi ƒëa 5 ƒëo·∫°n ƒë·ªÅu
        for i in range(5):
            start_time = duration * (i + 1) / 6
            end_time = min(duration, start_time + segment_duration)
            if end_time - start_time > 3:
                climax_segments.append([start_time, end_time])
    
    print(f"T√¨m th·∫•y {len(climax_segments)} c·∫£nh cao tr√†o.")
    return climax_segments

def extract_climax_scenes_optimized(video_path, output_path, model=None, device="cuda", buffer_time=2.0):
    """Phi√™n b·∫£n t·ªëi ∆∞u c·ªßa tr√≠ch xu·∫•t c·∫£nh cao tr√†o"""
    # Ph√°t hi·ªán cao tr√†o v·ªõi thu·∫≠t to√°n t·ªëi ∆∞u
    climax_segments = detect_climax_scenes_fast(video_path, model, device)
    
    if not climax_segments:
        print("Kh√¥ng t√¨m th·∫•y c·∫£nh cao tr√†o n√†o.")
        return False
    
    # Th√™m buffer time v√† ƒë·∫£m b·∫£o th·ªùi l∆∞·ª£ng h·ª£p l√Ω
    clip = VideoFileClip(video_path)
    duration = clip.duration
    
    # Process segments
    for segment in climax_segments:
        segment[0] = max(0, segment[0] - buffer_time)
        segment[1] = min(duration, segment[1] + buffer_time)
    
    # Merge segments that are close to each other
    merged_segments = []
    merge_threshold = 4.0  # seconds
    
    for segment in sorted(climax_segments, key=lambda x: x[0]):
        if not merged_segments or segment[0] - merged_segments[-1][1] > merge_threshold:
            merged_segments.append(segment)
        else:
            merged_segments[-1][1] = max(merged_segments[-1][1], segment[1])
    
    # Calculate total extract duration
    total_duration = sum(end - start for start, end in merged_segments)
    percentage = (total_duration / duration) * 100
    print(f"T·ªïng th·ªùi l∆∞·ª£ng c·∫£nh cao tr√†o: {total_duration:.1f}s ({percentage:.1f}% c·ªßa video g·ªëc)")
    
    # Extract video segments with CUDA acceleration if available
    if device == "cuda" and torch.cuda.is_available():
        print("üöÄ S·ª≠ d·ª•ng GPU ƒë·ªÉ tƒÉng t·ªëc xu·∫•t video")
        return fast_extract_segments_cuda(video_path, output_path, merged_segments)
    else:
        # Extract video segments with standard method
        clips = []
        for i, (start, end) in enumerate(merged_segments):
            print(f"ƒêo·∫°n cao tr√†o {i+1}/{len(merged_segments)}: {start:.1f}s - {end:.1f}s (ƒë·ªô d√†i {end-start:.1f}s)")
            try:
                clips.append(clip.subclip(start, end))
            except Exception as e:
                print(f"L·ªói khi c·∫Øt ƒëo·∫°n {start}-{end}: {e}")
        
        if clips:
            try:
                final_clip = concatenate_videoclips(clips)
                final_clip.write_videofile(output_path, codec="libx264", threads=4)
                print(f"‚úÖ ƒê√£ xu·∫•t c√°c c·∫£nh cao tr√†o v√†o: {output_path}")
                clip.close()
                return True
            except Exception as e:
                print(f"‚ùå L·ªói khi t·∫°o video c·∫£nh cao tr√†o: {e}")
                clip.close()
                return False
        else:
            print("‚ùå Kh√¥ng th·ªÉ t·∫°o video c·∫£nh cao tr√†o.")
            clip.close()
            return False

def extract_segments_to_video(video_path, output_path, segments, buffer_time=0.0):
    """Tr√≠ch xu·∫•t c√°c ƒëo·∫°n th·ªùi gian c·ª• th·ªÉ t·ª´ video"""
    if not segments:
        print("Kh√¥ng c√≥ ƒëo·∫°n n√†o ƒë·ªÉ tr√≠ch xu·∫•t")
        return False
        
    try:
        clip = VideoFileClip(video_path)
        clips = []
        
        for start, end in segments:
            # Th√™m buffer_time v√† ƒë·∫£m b·∫£o th·ªùi gian h·ª£p l·ªá
            start_time = max(0, start - buffer_time)
            end_time = min(clip.duration, end + buffer_time)
            
            if end_time - start_time >= 1.0:  # Ch·ªâ l·∫•y ƒëo·∫°n d√†i h∆°n 1 gi√¢y
                try:
                    segment_clip = clip.subclip(start_time, end_time)
                    clips.append(segment_clip)
                except Exception as e:
                    print(f"L·ªói khi c·∫Øt ƒëo·∫°n {start_time}-{end_time}: {e}")
        
        if clips:
            final_clip = concatenate_videoclips(clips)
            final_clip.write_videofile(output_path, codec="libx264", threads=4)
            final_clip.close()
            clip.close()
            print(f"‚úÖ ƒê√£ tr√≠ch xu·∫•t {len(clips)} ƒëo·∫°n th√†nh video: {output_path}")
            return True
        else:
            clip.close()
            print("‚ö†Ô∏è Kh√¥ng c√≥ ƒëo·∫°n n√†o h·ª£p l·ªá ƒë·ªÉ tr√≠ch xu·∫•t")
            return False
            
    except Exception as e:
        print(f"‚ùå L·ªói khi tr√≠ch xu·∫•t ƒëo·∫°n video: {e}")
        return False

def fast_motion_detection(video_path, frame_rate=1, device='cuda'):
    """Entry point for fast motion detection - uses Decord if available, OpenCV otherwise"""
    if DECORD_AVAILABLE and (device == 'cuda' or torch.cuda.is_available()):
        print("üöÄ S·ª≠ d·ª•ng Decord ƒë·ªÉ tƒÉng t·ªëc ph√°t hi·ªán chuy·ªÉn ƒë·ªông")
        # Remove the recursive call and implement the decord detection inline
        return _fast_motion_detection_with_decord_impl(video_path, frame_rate, device)
    
    print("‚ö° Ph√¢n t√≠ch chuy·ªÉn ƒë·ªông v·ªõi ph∆∞∆°ng ph√°p si√™u t·ªëc (OpenCV)...")
    
    # Original implementation as fallback
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return []
    
    # ...existing code (rest of the OpenCV implementation)...
    
def _fast_motion_detection_with_decord_impl(video_path, frame_rate=1, device="cuda"):
    """Internal implementation for Decord-based motion detection"""
    print("‚ö° Analyzing motion with optimized method using Decord...")
    
    # Extract frames using Decord
    device_id = 0 if device == "cuda" else -1
    frames, timestamps = extract_frames_with_decord(video_path, sample_rate=frame_rate, device_id=device_id)
    
    # Fall back to OpenCV if Decord fails
    if frames is None:
        print("Falling back to standard frame extraction using OpenCV...")
        # Directly use OpenCV version without recursion
        return _fast_motion_detection_opencv_impl(video_path, frame_rate)
    
    # Process frames for motion detection
    # ...existing code (rest of the Decord implementation)...

def _fast_motion_detection_opencv_impl(video_path, frame_rate=1):
    """OpenCV implementation of motion detection without any recursion"""
    print("‚ö° Ph√¢n t√≠ch chuy·ªÉn ƒë·ªông v·ªõi ph∆∞∆°ng ph√°p si√™u t·ªëc (OpenCV)...")
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return []
    
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps
    
    # Ch·ªâ l·∫•y m·∫´u 1 frame m·ªói frame_rate gi√¢y
    frame_skip = int(fps * frame_rate)
    prev_frame = None
    motion_scores = []
    timestamps = []
    
    for frame_idx in tqdm(range(0, total_frames, frame_skip), desc="Ph√°t hi·ªán chuy·ªÉn ƒë·ªông"):
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        
        if not ret:
            break
            
        # Gi·∫£m ƒë·ªô ph√¢n gi·∫£i ƒë·ªÉ tƒÉng t·ªëc
        small_frame = cv2.resize(frame, (160, 90))
        gray = cv2.cvtColor(small_frame, cv2.COLOR_BGR2GRAY)
        
        if prev_frame is None:
            prev_frame = gray
            motion_scores.append(0)
            timestamps.append(frame_idx / fps)
            continue
        
        # Frame difference thay v√¨ optical flow - nhanh h∆°n ~10x
        frame_diff = cv2.absdiff(prev_frame, gray)
        motion_score = np.mean(frame_diff) / 255.0 * 10  # Normalize v√† scale
        motion_scores.append(motion_score)
        timestamps.append(frame_idx / fps)
        
        prev_frame = gray
        
    cap.release()
    
    # Ph√°t hi·ªán c√°c ƒëo·∫°n c√≥ chuy·ªÉn ƒë·ªông m·∫°nh
    threshold = np.mean(motion_scores) * 1.5  # Adaptive threshold
    if threshold < 0.3:  # Minimum threshold
        threshold = 0.3
        
    segments = []
    in_motion = False
    start_time = 0
    
    for i, score in enumerate(motion_scores):
        if score > threshold and not in_motion:
            in_motion = True
            start_time = timestamps[i]
        elif score <= threshold * 0.7 and in_motion:
            in_motion = False
            if timestamps[i] - start_time >= 1.5:  # ƒêo·∫°n d√†i √≠t nh·∫•t 1.5s
                segments.append([start_time, timestamps[i]])
                
    if in_motion:
        segments.append([start_time, duration])
        
    # Gh√©p c√°c ƒëo·∫°n g·∫ßn nhau
    if not segments:
        return []
        
    merged = [segments[0]]
    for current in segments[1:]:
        previous = merged[-1]
        if current[0] - previous[1] < 3.0:  # 3s gap
            previous[1] = current[1]
        else:
            merged.append(current)
            
    print(f"T√¨m th·∫•y {len(merged)} ƒëo·∫°n chuy·ªÉn ƒë·ªông")
    return merged

def fast_motion_detection_with_decord(video_path, frame_rate=1, device="cuda"):
    """Optimized motion detection using Decord for frame extraction"""
    # Call the internal implementation directly to avoid recursion
    return _fast_motion_detection_with_decord_impl(video_path, frame_rate, device)
    
def check_cuda_ffmpeg_support():
    """Ki·ªÉm tra h·ªó tr·ª£ CUDA v√† NVENC trong FFmpeg"""
    try:
        # Check CUDA
        result = subprocess.run(
            ["ffmpeg", "-hwaccels"], 
            stdout=subprocess.PIPE, 
            stderr=subprocess.PIPE
        )
        output = result.stdout.decode() + result.stderr.decode()
        cuda_available = "cuda" in output.lower()
        
        # Check NVENC
        result = subprocess.run(
            ["ffmpeg", "-encoders"], 
            stdout=subprocess.PIPE, 
            stderr=subprocess.PIPE
        )
        output = result.stdout.decode()
        nvenc_available = "h264_nvenc" in output
        
        return cuda_available, nvenc_available
    except Exception as e:
        print(f"L·ªói khi ki·ªÉm tra FFmpeg: {e}")
        return False, False

def extract_video_segment_ffmpeg(input_path, output_path, start_time, duration, use_cuda=True):
    """Tr√≠ch xu·∫•t m·ªôt ƒëo·∫°n video s·ª≠ d·ª•ng FFmpeg - nhanh h∆°n MoviePy 2-3x"""
    import subprocess
    
    try:
        # Ki·ªÉm tra h·ªó tr·ª£ CUDA v√† NVENC
        if use_cuda:
            cuda_available, nvenc_available = check_cuda_ffmpeg_support()
        else:
            cuda_available, nvenc_available = False, False
            
        cmd = [
            "ffmpeg", "-y",
            "-ss", str(start_time),
            "-i", input_path,
            "-t", str(duration)
        ]
        
        # Th√™m t√πy ch·ªçn CUDA n·∫øu kh·∫£ d·ª•ng
        if cuda_available:
            cmd.extend(["-hwaccel", "cuda"])
            
        # S·ª≠ d·ª•ng NVENC n·∫øu kh·∫£ d·ª•ng, n·∫øu kh√¥ng th√¨ d√πng preset ultrafast
        if nvenc_available:
            cmd.extend(["-c:v", "h264_nvenc", "-preset", "p1", "-tune", "hq"])
        else:
            cmd.extend(["-c:v", "libx264", "-preset", "ultrafast"])
            
        cmd.extend(["-c:a", "aac", output_path])
        
        subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        return os.path.exists(output_path)
    except Exception as e:
        print(f"L·ªói khi tr√≠ch xu·∫•t video: {e}")
        return False

def fast_extract_segments(input_path, output_path, segments, use_cuda=True):
    """Tr√≠ch xu·∫•t v√† gh√©p nhi·ªÅu ƒëo·∫°n video nhanh ch√≥ng v·ªõi FFmpeg"""
    try:
        if not segments:
            print("Kh√¥ng c√≥ ƒëo·∫°n n√†o ƒë·ªÉ tr√≠ch xu·∫•t")
            return False
            
        clip = VideoFileClip(input_path)
        clips = []
        
        for start, end in segments:
            clips.append(clip.subclip(max(0, start), min(clip.duration, end)))
        
        if not clips:
            print("Kh√¥ng c√≥ ƒëo·∫°n n√†o h·ª£p l·ªá ƒë·ªÉ gh√©p")
            return False
            
        final_clip = concatenate_videoclips(clips)
        final_clip.write_videofile(output_path, codec="libx264")
        clip.close()
        final_clip.close()
        
        return True
    except Exception as e:
        print(f"L·ªói khi tr√≠ch xu·∫•t ƒëo·∫°n video: {e}")
        return False

def extract_frames_with_decord(video_path, sample_rate=1, device_id=0):
    """Extract frames using Decord - much faster than OpenCV"""
    if not DECORD_AVAILABLE:
        print("Decord not available, falling back to OpenCV")
        return None, None, None
    
    try:
        # Use CPU context if device_id is -1
        if device_id < 0:
            print("S·ª≠ d·ª•ng Decord v·ªõi CPU context")
            ctx = decord.cpu()
        else:
            # Force GPU usage if available
            if torch.cuda.is_available():
                print(f"S·ª≠ d·ª•ng Decord v·ªõi GPU context (device {device_id})")
                try:
                    ctx = decord.gpu(device_id)
                except Exception as e:
                    print(f"L·ªói khi t·∫°o GPU context: {e}")
                    print("Fallback to CPU context")
                    ctx = decord.cpu()
            else:
                print("GPU kh√¥ng kh·∫£ d·ª•ng, s·ª≠ d·ª•ng CPU context")
                ctx = decord.cpu()
        
        vr = decord.VideoReader(video_path, ctx=ctx)
        
        fps = vr.get_avg_fps()
        frame_count = len(vr)
        duration = frame_count / fps
        
        # Calculate frames to extract (1 frame per second)
        frame_indices = list(range(0, frame_count, int(fps * sample_rate)))
        timestamps = [idx / fps for idx in frame_indices]
        
        # Extract the frames in batches to prevent memory issues
        batch_size = 16
        frames = []
        
        for i in tqdm(range(0, len(frame_indices), batch_size), desc="Extracting frames"):
            batch_indices = frame_indices[i:i+batch_size]
            batch_frames = vr.get_batch(batch_indices).asnumpy()
            
            # Convert to BGR (OpenCV format) if needed for consistency with other code
            for frame in batch_frames:
                frames.append(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))
            
            # Release memory explicitly
            del batch_frames
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        return frames, timestamps, fps
        
    except Exception as e:
        print(f"Error using Decord: {e}")
        print("Falling back to OpenCV for frame extraction")
        return None, None, None

def extract_video_with_cuda(input_path, output_path, start_time, duration):
    """Extract video segment using CUDA acceleration with FFmpeg"""
    try:
        # Check if FFmpeg is available with CUDA support
        result = subprocess.run(
            ["ffmpeg", "-hwaccels"], 
            stdout=subprocess.PIPE, 
            stderr=subprocess.PIPE
        )
        
        cuda_available = "cuda" in result.stdout.decode().lower() or "cuda" in result.stderr.decode().lower()
        nvenc_available = False
        
        # Check if NVENC is available
        result = subprocess.run(
            ["ffmpeg", "-encoders"], 
            stdout=subprocess.PIPE, 
            stderr=subprocess.PIPE
        )
        output = result.stdout.decode()
        if "h264_nvenc" in output:
            nvenc_available = True
        
        cmd = ["ffmpeg", "-y"]
        
        # Add seek before input (faster)
        cmd.extend(["-ss", str(start_time)])
        
        # Add input
        cmd.extend(["-i", input_path])
        
        # Add duration
        cmd.extend(["-t", str(duration)])
        
        # Use hardware acceleration if available
        if cuda_available:
            cmd.extend(["-hwaccel", "cuda"])
            
        # Use NVENC encoder if available
        if nvenc_available:
            cmd.extend(["-c:v", "h264_nvenc", "-preset", "p1", "-tune", "hq"])
        else:
            cmd.extend(["-c:v", "libx264", "-preset", "ultrafast"])
            
        # Audio codec
        cmd.extend(["-c:a", "aac"])
        
        # Output file
        cmd.append(output_path)
        
        # Run FFmpeg
        subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        
        return os.path.exists(output_path)
    
    except Exception as e:
        print(f"Error using FFmpeg with CUDA: {e}")
        # Fall back to CPU version
        return extract_video_segment_ffmpeg(input_path, output_path, start_time, duration)

def fast_extract_segments_cuda(input_path, output_path, segments):
    """Extract and concatenate video segments using CUDA acceleration when possible"""
    if not segments:
        print("Kh√¥ng c√≥ ƒëo·∫°n n√†o ƒë·ªÉ tr√≠ch xu·∫•t")
        return False
        
    try:
        # Ki·ªÉm tra xem c√≥ C++ extensions hay kh√¥ng
        from utils.gpu_utils import try_load_cpp_extensions
        cpp_extensions = try_load_cpp_extensions()
        
        # T·∫°o th∆∞ m·ª•c t·∫°m cho video concat
        temp_dir = os.path.join(os.path.dirname(output_path), "temp_segments")
        os.makedirs(temp_dir, exist_ok=True)
        
        segment_files = []
        
        # S·ª≠ d·ª•ng C++ extensions n·∫øu c√≥
        if cpp_extensions and "video_extract" in cpp_extensions:
            print("üöÄ S·ª≠ d·ª•ng C++ extensions ƒë·ªÉ x·ª≠ l√Ω video")
            for i, (start, end) in enumerate(segments):
                segment_file = os.path.join(temp_dir, f"segment_{i:03d}.mp4")
                duration = end - start
                
                try:
                    # S·ª≠ d·ª•ng h√†m C++ ƒë·ªÉ tr√≠ch xu·∫•t ƒëo·∫°n video
                    success = cpp_extensions["video_extract"].extract_segment(
                        input_path, segment_file, start, duration, True)
                    
                    if success and os.path.exists(segment_file) and os.path.getsize(segment_file) > 0:
                        segment_files.append(segment_file)
                    else:
                        print(f"‚ö†Ô∏è Tr√≠ch xu·∫•t segment {i} v·ªõi C++ kh√¥ng th√†nh c√¥ng, d√πng FFmpeg")
                        # Fall back to FFmpeg for this segment
                        cmd = [
                            "ffmpeg", "-y",
                            "-ss", str(start),
                            "-i", input_path,
                            "-t", str(duration),
                            "-c:v", "libx264", "-preset", "ultrafast",
                            "-c:a", "aac", 
                            segment_file
                        ]
                        subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)
                        if os.path.exists(segment_file) and os.path.getsize(segment_file) > 0:
                            segment_files.append(segment_file)
                except Exception as e:
                    print(f"‚ö†Ô∏è L·ªói khi s·ª≠ d·ª•ng C++ extensions: {e}")
                    # Fall back to FFmpeg for this segment
                    cmd = [
                        "ffmpeg", "-y",
                        "-ss", str(start),
                        "-i", input_path,
                        "-t", str(duration),
                        "-c:v", "libx264", "-preset", "ultrafast",
                        "-c:a", "aac", 
                        segment_file
                    ]
                    try:
                        subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)
                        if os.path.exists(segment_file) and os.path.getsize(segment_file) > 0:
                            segment_files.append(segment_file)
                    except subprocess.CalledProcessError:
                        print(f"‚ö†Ô∏è L·ªói khi t·∫°o segment {i}, th·ª≠ ph∆∞∆°ng ph√°p thay th·∫ø")
        else:
            # S·ª≠ d·ª•ng FFmpeg v·ªõi h·ªó tr·ª£ CUDA n·∫øu c√≥ th·ªÉ
            for i, (start, end) in enumerate(segments):
                segment_file = os.path.join(temp_dir, f"segment_{i:03d}.mp4")
                duration = end - start
                
                # S·ª≠ d·ª•ng FFmpeg ƒë·ªÉ tr√≠ch xu·∫•t ƒëo·∫°n video
                cmd = [
                    "ffmpeg", "-y",
                    "-ss", str(start),
                    "-i", input_path,
                    "-t", str(duration),
                    "-c:v", "libx264", "-preset", "ultrafast",
                    "-c:a", "aac", 
                    segment_file
                ]
                try:
                    subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)
                    if os.path.exists(segment_file) and os.path.getsize(segment_file) > 0:
                        segment_files.append(segment_file)
                except subprocess.CalledProcessError:
                    print(f"‚ö†Ô∏è L·ªói khi t·∫°o segment {i}, th·ª≠ ph∆∞∆°ng ph√°p thay th·∫ø")
        
        # Ki·ªÉm tra xem c√≥ segment files n√†o ƒë∆∞·ª£c t·∫°o kh√¥ng
        if not segment_files:
            print("‚ö†Ô∏è Kh√¥ng th·ªÉ tr√≠ch xu·∫•t ƒëo·∫°n video n√†o v·ªõi CUDA")
            print("Chuy·ªÉn sang ph∆∞∆°ng ph√°p thay th·∫ø...")
            return fast_extract_segments_with_moviepy(input_path, output_path, segments)
        
        # Gh√©p c√°c ƒëo·∫°n video
        if len(segment_files) == 1:
            # N·∫øu ch·ªâ c√≥ 1 ƒëo·∫°n, copy tr·ª±c ti·∫øp
            shutil.copy(segment_files[0], output_path)
            print(f"‚úÖ ƒê√£ t·∫°o video th√†nh c√¥ng v·ªõi ph∆∞∆°ng ph√°p tr·ª±c ti·∫øp: {output_path}")
        else:
            # S·ª≠ d·ª•ng video_concat_cpp n·∫øu c√≥
            if cpp_extensions and "video_concat" in cpp_extensions:
                try:
                    success = cpp_extensions["video_concat"].concatenate_videos(segment_files, output_path)
                    if success:
                        print(f"‚úÖ ƒê√£ t·∫°o video th√†nh c√¥ng v·ªõi C++ concat: {output_path}")
                    else:
                        raise Exception("Gh√©p video kh√¥ng th√†nh c√¥ng v·ªõi C++")
                except Exception as e:
                    print(f"‚ö†Ô∏è L·ªói khi gh√©p video v·ªõi C++: {e}")
                    print("Chuy·ªÉn sang ph∆∞∆°ng ph√°p thay th·∫ø v·ªõi FFmpeg...")
                    # Fall back to FFmpeg concat
                    try:
                        # Chu·∫©n b·ªã file danh s√°ch - S·ª≠ d·ª•ng forward slashes cho FFmpeg
                        list_file = os.path.join(temp_dir, "segments.txt")
                        with open(list_file, 'w') as f:
                            for file in segment_files:
                                f.write(f"file '{file.replace(os.sep, '/')}'\n")
                        
                        cmd = [
                            "ffmpeg", "-y",
                            "-f", "concat",
                            "-safe", "0",
                            "-i", list_file,
                            "-c:v", "copy",
                            "-c:a", "copy",
                            output_path
                        ]
                        subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)
                        print(f"‚úÖ ƒê√£ t·∫°o video th√†nh c√¥ng v·ªõi ph∆∞∆°ng ph√°p gh√©p FFmpeg: {output_path}")
                    except subprocess.CalledProcessError as e:
                        print(f"‚ö†Ô∏è L·ªói khi gh√©p video v·ªõi FFmpeg: {e}")
                        print("Chuy·ªÉn sang ph∆∞∆°ng ph√°p thay th·∫ø v·ªõi MoviePy...")
                        return fast_extract_segments_with_moviepy(input_path, output_path, segments)
            else:
                # S·ª≠ d·ª•ng FFmpeg ƒë·ªÉ gh√©p video
                try:
                    # Chu·∫©n b·ªã file danh s√°ch - S·ª≠ d·ª•ng forward slashes cho FFmpeg
                    list_file = os.path.join(temp_dir, "segments.txt")
                    with open(list_file, 'w') as f:
                        for file in segment_files:
                            f.write(f"file '{file.replace(os.sep, '/')}'\n")
                    
                    cmd = [
                        "ffmpeg", "-y",
                        "-f", "concat",
                        "-safe", "0",
                        "-i", list_file,
                        "-c:v", "copy",
                        "-c:a", "copy",
                        output_path
                    ]
                    subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)
                    print(f"‚úÖ ƒê√£ t·∫°o video th√†nh c√¥ng v·ªõi ph∆∞∆°ng ph√°p gh√©p FFmpeg: {output_path}")
                except subprocess.CalledProcessError as e:
                    print(f"‚ö†Ô∏è L·ªói khi gh√©p video: {e}")
                    print("Chuy·ªÉn sang ph∆∞∆°ng ph√°p thay th·∫ø v·ªõi MoviePy...")
                    return fast_extract_segments_with_moviepy(input_path, output_path, segments)
        
        # D·ªçn d·∫πp
        for file in segment_files:
            try:
                os.remove(file)
            except:
                pass
        try:
            if os.path.exists(list_file):
                os.remove(list_file)
            os.rmdir(temp_dir)
        except:
            pass
            
        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:
            print(f"‚úÖ ƒê√£ t·∫°o video th√†nh c√¥ng: {output_path}")
            return True
        else:
            print("‚ùå T·∫°o video th·∫•t b·∫°i v·ªõi CUDA")
            return fast_extract_segments_with_moviepy(input_path, output_path, segments)
            
    except Exception as e:
        print(f"‚ùå L·ªói khi tr√≠ch xu·∫•t video v·ªõi CUDA: {e}")
        # Th·ª≠ l·∫°i v·ªõi ph∆∞∆°ng ph√°p th√¥ng th∆∞·ªùng
        print("Chuy·ªÉn sang ph∆∞∆°ng ph√°p kh√¥ng d√πng CUDA...")
        return fast_extract_segments_with_moviepy(input_path, output_path, segments)

def fast_extract_segments_with_moviepy(input_path, output_path, segments):
    """Fallback method using MoviePy to extract segments"""
    try:
        print("S·ª≠ d·ª•ng MoviePy ƒë·ªÉ x·ª≠ l√Ω video...")
        if not segments:
            print("Kh√¥ng c√≥ ƒëo·∫°n n√†o ƒë·ªÉ tr√≠ch xu·∫•t")
            return False
            
        clip = VideoFileClip(input_path)
        clips = []
        
        for start, end in segments:
            start = max(0, start)
            end = min(clip.duration, end)
            if end - start >= 0.5:  # Ch·ªâ l·∫•y ƒëo·∫°n d√†i √≠t nh·∫•t 0.5 gi√¢y
                clips.append(clip.subclip(start, end))
                
        if not clips:
            print("Kh√¥ng c√≥ ƒëo·∫°n n√†o h·ª£p l·ªá ƒë·ªÉ gh√©p")
            clip.close()
            return False
            
        print(f"ƒêang gh√©p {len(clips)} ƒëo·∫°n video v·ªõi MoviePy...")
        final_clip = concatenate_videoclips(clips)
        final_clip.write_videofile(output_path, codec="libx264", audio_codec="aac", 
                                  threads=4, logger=None)
        clip.close()
        final_clip.close()
        
        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:
            print(f"‚úÖ ƒê√£ t·∫°o video th√†nh c√¥ng v·ªõi MoviePy: {output_path}")
            return True
        else:
            print("‚ùå T·∫°o video th·∫•t b·∫°i")
            return False
    except Exception as e:
        print(f"‚ùå L·ªói khi tr√≠ch xu·∫•t ƒëo·∫°n video v·ªõi MoviePy: {e}")
        return False

def detect_action_with_yolo_batched(video_path, model=None, device="cuda", batch_size=8, frame_rate=1):
    """
    Ph√°t hi·ªán c·∫£nh h√†nh ƒë·ªông s·ª≠ d·ª•ng YOLOv5 v·ªõi x·ª≠ l√Ω batch ƒë·ªÉ t·ªëi ∆∞u h√≥a.
    Ch·ªâ t·∫≠p trung v√†o ph√°t hi·ªán ng∆∞·ªùi v√† ph√¢n t√≠ch chuy·ªÉn ƒë·ªông c·ªßa h·ªç.
    """
    print("üöÄ Ph√°t hi·ªán c·∫£nh h√†nh ƒë·ªông v·ªõi YOLOv5 (x·ª≠ l√Ω batch)...")
    
    # Ki·ªÉm tra device
    if device == "cuda" and not torch.cuda.is_available():
        print("‚ö†Ô∏è CUDA kh√¥ng kh·∫£ d·ª•ng, s·ª≠ d·ª•ng CPU thay th·∫ø")
        device = "cpu"
    
    # Kh·ªüi t·∫°o m√¥ h√¨nh n·∫øu ch∆∞a ƒë∆∞·ª£c truy·ªÅn v√†o
    if model is None:
        try:
            model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)
            model.to(device)
            # Ch·ªâ ph√°t hi·ªán ng∆∞·ªùi (class 0)
            model.classes = [0]  # Ch·ªâ gi·ªØ l·∫°i class "person"
            print("‚úÖ ƒê√£ kh·ªüi t·∫°o YOLOv5 model, ch·ªâ nh·∫≠n di·ªán ng∆∞·ªùi")
        except Exception as e:
            print(f"‚ùå L·ªói khi kh·ªüi t·∫°o YOLOv5: {e}")
            return []
    elif hasattr(model, 'classes'):
        # ƒê·∫£m b·∫£o model ƒë√£ cho c√≥ th·ªÉ nh·∫≠n di·ªán ng∆∞·ªùi
        model.classes = [0]
        print("ƒê√£ c·∫≠p nh·∫≠t model ƒë·ªÉ ch·ªâ nh·∫≠n di·ªán ng∆∞·ªùi")
    
    # Extract frames using Decord for optimal performance if available
    if DECORD_AVAILABLE:
        print("S·ª≠ d·ª•ng Decord ƒë·ªÉ tr√≠ch xu·∫•t frames nhanh h∆°n")
        frames, timestamps = extract_frames_with_decord(video_path, sample_rate=frame_rate, 
                                                     device_id=0 if device == "cuda" else -1)
    else:
        # Fall back to OpenCV
        print("S·ª≠ d·ª•ng OpenCV ƒë·ªÉ tr√≠ch xu·∫•t frames")
        frames, timestamps = extract_frames_with_opencv(video_path, sample_rate=frame_rate)
        
    if not frames:
        print("‚ùå Kh√¥ng th·ªÉ tr√≠ch xu·∫•t frames t·ª´ video")
        return []
        
    print(f"ƒê√£ tr√≠ch xu·∫•t {len(frames)} frames t·ª´ video")
    
    # Process frames in batches
    action_scores = []
    person_counts = []
    
    for i in tqdm(range(0, len(frames), batch_size), desc="Ph√¢n t√≠ch h√†nh ƒë·ªông"):
        batch = frames[i:i+batch_size]
        
        # Run inference on batch
        with torch.no_grad():
            results = model(batch)
            
        # Process detection results for each frame
        for j, frame_result in enumerate(results.xyxy):
            frame_idx = i + j
            if frame_idx >= len(timestamps):
                continue
            
            # Extract people detections
            people = []
            for det in frame_result:
                if int(det[5]) == 0:  # Class 0 is person
                    bbox = det[:4].cpu().numpy()  # x1, y1, x2, y2
                    conf = float(det[4])
                    if conf > 0.3:  # Only keep confident detections
                        people.append(bbox)
            
            # Calculate action score based on number of people and their positions
            person_count = len(people)
            person_counts.append(person_count)
            
            # More sophisticated action score calculation:
            # - More people = higher score
            # - People spread across the frame = higher score (more action)
            action_score = person_count
            
            if person_count > 1:
                # Calculate spatial distribution of people
                centers = []
                for bbox in people:
                    center_x = (bbox[0] + bbox[2]) / 2
                    center_y = (bbox[1] + bbox[3]) / 2
                    centers.append((center_x, center_y))
                
                # Calculate average distance between people
                if len(centers) > 1:
                    distances = []
                    for idx1 in range(len(centers)):
                        for idx2 in range(idx1 + 1, len(centers)):
                            c1 = centers[idx1]
                            c2 = centers[idx2]
                            dist = np.sqrt((c1[0] - c2[0])**2 + (c1[1] - c2[1])**2)
                            distances.append(dist)
                    avg_distance = np.mean(distances) if distances else 0
                    
                    # Normalize by image size
                    h, w = frames[0].shape[:2]
                    max_dist = np.sqrt(h**2 + w**2)
                    normalized_dist = avg_distance / max_dist
                    
                    # Add distance factor to score (0.5-1.5x multiplier)
                    action_score *= (0.5 + normalized_dist)
            
            action_scores.append(action_score)
    
    # Create segments with high action scores
    if not action_scores:
        return []
    
    # Normalize scores
    max_score = max(action_scores) if action_scores else 1
    normalized_scores = [score / max_score for score in action_scores]
    
    # Calculate adaptive threshold
    mean_score = np.mean(normalized_scores)
    std_score = np.std(normalized_scores)
    threshold = mean_score + 0.5 * std_score  # Adaptive threshold
    
    # Find segments with high action
    high_action_segments = []
    in_segment = False
    start_idx = 0
    
    for i, score in enumerate(normalized_scores):
        if score > threshold and not in_segment:
            in_segment = True
            start_idx = i
        elif (score <= threshold * 0.7 or i == len(normalized_scores) - 1) and in_segment:
            in_segment = False
            # Ch·ªâ l·∫•y c√°c ƒëo·∫°n ƒë·ªß d√†i (√≠t nh·∫•t 2 frames)
            if i - start_idx >= 2:
                start_time = timestamps[start_idx]
                end_time = timestamps[i]
                high_action_segments.append([start_time, end_time])
    
    # Cleanup
    if device == "cuda" and torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # Merge nearby segments
    if high_action_segments:
        merged_segments = [high_action_segments[0]]
        merge_threshold = 3.0  # seconds
        
        for segment in high_action_segments[1:]:
            if segment[0] - merged_segments[-1][1] < merge_threshold:
                # Merge with previous segment
                merged_segments[-1][1] = segment[1]
            else:
                merged_segments.append(segment)
                
    return merged_segments

def extract_frames_with_opencv(video_path, sample_rate=1):
    """Extract frames using OpenCV at specified sample rate"""
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"Kh√¥ng th·ªÉ m·ªü video file: {video_path}")
        return [], [], []
    
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    # Extract one frame per second or at specified rate
    frame_interval = int(fps / sample_rate)
    frames = []
    timestamps = []
    
    for frame_idx in tqdm(range(0, total_frames, frame_interval), desc="Tr√≠ch xu·∫•t frames"):
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        
        if not ret:
            break
        
        frames.append(frame)
        timestamps.append(frame_idx / fps)
        
    cap.release()
    return frames, timestamps

def extract_top_action_scenes(video_path, output_path, model=None, device="cuda", 
                            max_scenes=5, max_duration=60, buffer_time=1.0):
    """
    Tr√≠ch xu·∫•t c√°c c·∫£nh h√†nh ƒë·ªông h·∫•p d·∫´n nh·∫•t t·ª´ video
    v√† t·∫°o m·ªôt video ng·∫Øn ch·ª©a nh·ªØng c·∫£nh n√†y.
    """
    print(f"Ph√¢n t√≠ch video ƒë·ªÉ tr√≠ch xu·∫•t t·ªëi ƒëa {max_scenes} c·∫£nh h√†nh ƒë·ªông...")
    
    # Ph√°t hi·ªán c·∫£nh h√†nh ƒë·ªông
    action_segments = detect_action_with_yolo_batched(video_path, model, device)
    
    if not action_segments:
        print("Kh√¥ng t√¨m th·∫•y c·∫£nh h√†nh ƒë·ªông n√†o")
        return False
    
    print(f"T√¨m th·∫•y {len(action_segments)} c·∫£nh h√†nh ƒë·ªông")
    
    # Sort segments by duration (longer = more interesting)
    action_segments.sort(key=lambda x: x[1] - x[0], reverse=True)
    # Merge nearby segments
    # Limit to max_scenes
    selected_segments = action_segments[:max_scenes]
    # Apply buffer time
    clip = VideoFileClip(video_path)
    duration = clip.duration
    
    for i in range(len(selected_segments)):
        start, end = selected_segments[i]
        # Add buffer before and after
        start = max(0, start - buffer_time)
        end = min(duration, end + buffer_time)
        selected_segments[i] = [start, end]
    
    # Ensure we don't exceed max_duration
    total_duration = sum(end - start for start, end in selected_segments)
    if total_duration > max_duration:
        print(f"T·ªïng th·ªùi l∆∞·ª£ng ({total_duration:.1f}s) v∆∞·ª£t qu√° gi·ªõi h·∫°n ({max_duration}s), c·∫Øt b·ªõt...")
        # Keep adding segments until we reach max_duration
        final_segments = []
        current_duration = 0
        
        for start, end in selected_segments:
            segment_duration = end - start
            if current_duration + segment_duration <= max_duration:
                final_segments.append([start, end])
                current_duration += segment_duration
            else:
                # Add partial segment if possible
                remaining = max_duration - current_duration
                if remaining >= 3:  # Only add if at least 3 seconds remain
                    final_segments.append([start, start + remaining])
                break
        selected_segments = final_segments
        
    # Extract and concatenate segments
    if selected_segments:
        clips = []
        for i, (start, end) in enumerate(selected_segments):
            print(f"C·∫£nh {i+1}: {start:.1f}s - {end:.1f}s (ƒë·ªô d√†i: {end-start:.1f}s)")
            clips.append(clip.subclip(start, end))
        
        if clips:
            print(f"ƒêang t·∫°o video highlight v·ªõi {len(clips)} c·∫£nh...")
            final_clip = concatenate_videoclips(clips)
            final_clip.write_videofile(output_path, codec="libx264", threads=4)
            clip.close()
            final_clip.close()
            print(f"‚úÖ ƒê√£ t·∫°o video highlight: {output_path}")
            return True
    
    print("‚ùå Kh√¥ng th·ªÉ t·∫°o video highlight")
    clip.close()
    return False

def detect_motion_with_optical_flow(video_path, sample_rate=2, threshold=0.5, min_segment_duration=2.0):
    """
    Ph√°t hi·ªán chuy·ªÉn ƒë·ªông b·∫±ng Farneback Optical Flow, ph√¢n bi·ªát chuy·ªÉn ƒë·ªông nh√¢n v·∫≠t v√† m√°y quay.
    
    Args:
        video_path: ƒê∆∞·ªùng d·∫´n video c·∫ßn ph√¢n t√≠ch
        sample_rate: S·ªë frame l·∫•y m·∫´u m·ªói gi√¢y
        threshold: Ng∆∞·ª°ng ph√°t hi·ªán chuy·ªÉn ƒë·ªông
        min_segment_duration: Th·ªùi l∆∞·ª£ng t·ªëi thi·ªÉu c·ªßa ƒëo·∫°n chuy·ªÉn ƒë·ªông (gi√¢y)
    
    Returns:
        Danh s√°ch c√°c ƒëo·∫°n th·ªùi gian c√≥ chuy·ªÉn ƒë·ªông m·∫°nh c·ªßa nh√¢n v·∫≠t ch√≠nh
    """
    print("üîç Ph√°t hi·ªán chuy·ªÉn ƒë·ªông n√¢ng cao v·ªõi Optical Flow...")
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"‚ùå Kh√¥ng th·ªÉ m·ªü file video: {video_path}")
        return []
    
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps
    
    # T√≠nh to√°n frame interval ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c sample_rate mong mu·ªën
    frame_interval = max(1, int(fps / sample_rate))
    print(f"ƒêang ph√¢n t√≠ch video: {duration:.1f} gi√¢y, {fps:.1f} fps")
    print(f"L·∫•y m·∫´u {sample_rate} frame/gi√¢y (m·ªói {frame_interval} frame)")
        
    prev_frame = None
    motion_scores = []       # ƒêi·ªÉm chuy·ªÉn ƒë·ªông t·ªïng th·ªÉ
    character_scores = []    # ƒêi·ªÉm chuy·ªÉn ƒë·ªông c·ªßa nh√¢n v·∫≠t (sau khi lo·∫°i b·ªè chuy·ªÉn ƒë·ªông m√°y quay)
    timestamps = []          # Th·ªùi ƒëi·ªÉm c·ªßa m·ªói frame ƒë∆∞·ª£c ph√¢n t√≠ch
    
    for frame_idx in tqdm(range(0, total_frames, frame_interval), desc="Ph√¢n t√≠ch optical flow"):
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        
        if not ret:
            break
        
        # Convert to grayscale
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        # Gi·∫£m nhi·ªÖu
        gray = cv2.GaussianBlur(gray, (5, 5), 0)
        
        if prev_frame is None:
            prev_frame = gray
            timestamps.append(frame_idx / fps)
            motion_scores.append(0)
            character_scores.append(0)
            continue
        
        # Calculate Farneback optical flow
        flow = cv2.calcOpticalFlowFarneback(
            prev_frame, gray, None, 
            pyr_scale=0.5,   # Pyramid scale
            levels=3,        # Number of pyramid levels
            winsize=15,      # Window size
            iterations=3,    # Iterations
            poly_n=5,        # Polynomial size
            poly_sigma=1.2,  # Polynomial standard deviation
            flags=0
        )
        
        # Split flow into horizontal (x) and vertical (y) components
        flow_x, flow_y = flow[:, :, 0], flow[:, :, 1]
        # Calculate magnitude and angle of optical flow
        magnitude, angle = cv2.cartToPolar(flow_x, flow_y)
        
        # 1. Ph√¢n t√≠ch chuy·ªÉn ƒë·ªông to√†n c·ª•c (m√°y quay)
        # L·∫•y trung b√¨nh c·ªßa c√°c vector chuy·ªÉn ƒë·ªông ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng chuy·ªÉn ƒë·ªông m√°y quay
        mean_flow_x = np.mean(flow_x)
        mean_flow_y = np.mean(flow_y)
        
        # 2. Lo·∫°i b·ªè chuy·ªÉn ƒë·ªông m√°y quay t·ª´ optical flow
        # T·∫°o ma tr·∫≠n chuy·ªÉn ƒë·ªông to√†n c·ª•c c√≥ c√πng k√≠ch th∆∞·ªõc v·ªõi frame
        h, w = flow_x.shape
        camera_motion_x = np.ones((h, w)) * mean_flow_x
        camera_motion_y = np.ones((h, w)) * mean_flow_y
        
        # Tr·ª´ chuy·ªÉn ƒë·ªông m√°y quay
        character_flow_x = flow_x - camera_motion_x
        character_flow_y = flow_y - camera_motion_y
        
        # T√≠nh magnitude c·ªßa chuy·ªÉn ƒë·ªông sau khi lo·∫°i b·ªè chuy·ªÉn ƒë·ªông m√°y quay
        character_magnitude, _ = cv2.cartToPolar(character_flow_x, character_flow_y)
        
        # 3. T√≠nh ƒëi·ªÉm chuy·ªÉn ƒë·ªông
        # Chuy·ªÉn ƒë·ªông t·ªïng th·ªÉ
        total_motion = np.mean(magnitude)
        motion_scores.append(total_motion)
        
        # Chuy·ªÉn ƒë·ªông c·ªßa nh√¢n v·∫≠t (sau khi lo·∫°i b·ªè chuy·ªÉn ƒë·ªông m√°y quay)
        character_motion = np.mean(character_magnitude)
        character_scores.append(character_motion)
        
        # L∆∞u timestamp
        timestamps.append(frame_idx / fps)
        
        # C·∫≠p nh·∫≠t frame tr∆∞·ªõc ƒë√≥
        prev_frame = gray
        
    cap.release()
    
    # 4. X√°c ƒë·ªãnh c√°c ƒëo·∫°n c√≥ chuy·ªÉn ƒë·ªông nh√¢n v·∫≠t m·∫°nh
    # T√≠nh ng∆∞·ª°ng th√≠ch nghi
    if not character_scores:
        return []
    
    # L√†m m·ªãn ƒëi·ªÉm chuy·ªÉn ƒë·ªông b·∫±ng moving average
    window_size = min(15, len(character_scores) // 5 + 1)  # Window size t·ªëi ƒëa 15 ho·∫∑c 1/5 s·ªë l∆∞·ª£ng frame
    smoothed_scores = np.convolve(character_scores, np.ones(window_size)/window_size, mode='same')
    
    # T√≠nh ng∆∞·ª°ng th√≠ch nghi d·ª±a tr√™n ph√¢n ph·ªëi ƒëi·ªÉm
    mean_score = np.mean(smoothed_scores)
    std_score = np.std(smoothed_scores)
    adaptive_threshold = mean_score + std_score * threshold
    print(f"Ph√¢n t√≠ch ƒëi·ªÉm chuy·ªÉn ƒë·ªông: mean={mean_score:.4f}, std={std_score:.4f}")
    print(f"Ng∆∞·ª°ng ph√°t hi·ªán chuy·ªÉn ƒë·ªông: {adaptive_threshold:.4f}")
    
    # 5. Nh√≥m c√°c frame li√™n ti·∫øp c√≥ chuy·ªÉn ƒë·ªông m·∫°nh th√†nh c√°c ƒëo·∫°n
    segments = []
    in_motion = False
    start_time = 0
    
    for i, score in enumerate(smoothed_scores):
        if score > adaptive_threshold and not in_motion:
            # B·∫Øt ƒë·∫ßu ƒëo·∫°n m·ªõi
            in_motion = True
            start_time = timestamps[i]
        elif (score <= adaptive_threshold * 0.7 or i == len(smoothed_scores) - 1) and in_motion:
            # K·∫øt th√∫c ƒëo·∫°n hi·ªán t·∫°i
            in_motion = False
            end_time = timestamps[i]
            # Ch·ªâ l∆∞u c√°c ƒëo·∫°n ƒë·ªß d√†i
            if end_time - start_time >= min_segment_duration:
                segments.append([start_time, end_time])
    
    # 6. Gh√©p c√°c ƒëo·∫°n g·∫ßn nhau
    if not segments:
        return []
        
    merged_segments = [segments[0]]
    merge_threshold = 2.0  # seconds
    
    for segment in segments[1:]:
        prev_segment = merged_segments[-1]
        # N·∫øu ƒëo·∫°n m·ªõi b·∫Øt ƒë·∫ßu g·∫ßn v·ªõi k·∫øt th√∫c c·ªßa ƒëo·∫°n tr∆∞·ªõc, gh√©p ch√∫ng l·∫°i
        if segment[0] - prev_segment[1] < merge_threshold:
            prev_segment[1] = segment[1]  # M·ªü r·ªông ƒëo·∫°n tr∆∞·ªõc
        else:
            merged_segments.append(segment)
            
    print(f"‚úÖ T√¨m th·∫•y {len(merged_segments)} ƒëo·∫°n c√≥ chuy·ªÉn ƒë·ªông nh√¢n v·∫≠t m·∫°nh")
    
    # 7. Hi·ªÉn th·ªã th√¥ng tin chi ti·∫øt v·ªÅ c√°c ƒëo·∫°n
    total_motion_duration = sum(end - start for start, end in merged_segments)
    print(f"T·ªïng th·ªùi l∆∞·ª£ng chuy·ªÉn ƒë·ªông: {total_motion_duration:.1f}s ({total_motion_duration/duration*100:.1f}% c·ªßa video)")
    for i, (start, end) in enumerate(merged_segments):
        print(f"  ƒêo·∫°n {i+1}: {start:.1f}s - {end:.1f}s (th·ªùi l∆∞·ª£ng: {end-start:.1f}s)")
        
    return merged_segments
    
def extract_character_motion_segments(video_path, output_path, buffer_time=1.0, sample_rate=2):
    """
    Tr√≠ch xu·∫•t c√°c ƒëo·∫°n video c√≥ chuy·ªÉn ƒë·ªông m·∫°nh c·ªßa nh√¢n v·∫≠t ch√≠nh
    
    Args:
        video_path: ƒê∆∞·ªùng d·∫´n video g·ªëc
        output_path: ƒê∆∞·ªùng d·∫´n video ƒë·∫ßu ra
        buffer_time: Th·ªùi gian th√™m v√†o tr∆∞·ªõc v√† sau m·ªói ƒëo·∫°n (gi√¢y)
        sample_rate: S·ªë frame l·∫•y m·∫´u m·ªói gi√¢y
    
    Returns:
        True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
    """
    print("üé¨ ƒêang ph√¢n t√≠ch v√† tr√≠ch xu·∫•t c√°c c·∫£nh c√≥ chuy·ªÉn ƒë·ªông nh√¢n v·∫≠t...")
    
    # Ph√°t hi·ªán c√°c ƒëo·∫°n c√≥ chuy·ªÉn ƒë·ªông nh√¢n v·∫≠t
    motion_segments = detect_motion_with_optical_flow(
        video_path,
        sample_rate=sample_rate,
        threshold=0.6,  # Ng∆∞·ª°ng ph√°t hi·ªán (s·ªë l·∫ßn std)
        min_segment_duration=1.5  # ƒê·ªô d√†i t·ªëi thi·ªÉu c·ªßa ƒëo·∫°n (gi√¢y)
    )
    
    if not motion_segments:
        print("‚ùå Kh√¥ng t√¨m th·∫•y ƒëo·∫°n chuy·ªÉn ƒë·ªông n√†o ƒë·ªß m·∫°nh")
        return False
    
    # Th√™m buffer time
    clip = VideoFileClip(video_path)
    duration = clip.duration
    
    buffered_segments = []
    for start, end in motion_segments:
        buffered_start = max(0, start - buffer_time)
        buffered_end = min(duration, end + buffer_time)
        buffered_segments.append([buffered_start, buffered_end])
    
    # Tr√≠ch xu·∫•t v√† gh√©p c√°c ƒëo·∫°n
    try:
        # S·ª≠ d·ª•ng CUDA n·∫øu c√≥
        if torch.cuda.is_available():
            print("üöÄ S·ª≠ d·ª•ng CUDA ƒë·ªÉ tƒÉng t·ªëc xu·∫•t video")
            success = fast_extract_segments_cuda(video_path, output_path, buffered_segments)
        else:
            # D√πng moviepy
            clips = []
            for i, (start, end) in enumerate(buffered_segments):
                print(f"Tr√≠ch xu·∫•t ƒëo·∫°n {i+1}/{len(buffered_segments)}: {start:.1f}s - {end:.1f}s")
                clips.append(clip.subclip(start, end))
            
            if clips:
                final_clip = concatenate_videoclips(clips)
                final_clip.write_videofile(output_path, codec="libx264", threads=4)
                final_clip.close()
                success = True
            else:
                success = False
        clip.close()
        return success
    except Exception as e:
        print(f"‚ùå L·ªói khi tr√≠ch xu·∫•t video: {e}")
        clip.close()
        return False

def visualize_optical_flow(video_path, output_path, sample_rate=1):
    """
    T·∫°o video hi·ªÉn th·ªã optical flow ƒë·ªÉ ph√¢n bi·ªát chuy·ªÉn ƒë·ªông m√°y quay v√† nh√¢n v·∫≠t
    
    Args:
        video_path: ƒê∆∞·ªùng d·∫´n video c·∫ßn ph√¢n t√≠ch
        output_path: ƒê∆∞·ªùng d·∫´n video k·∫øt qu·∫£ (hi·ªÉn th·ªã optical flow)
        sample_rate: S·ªë frame l·∫•y m·∫´u m·ªói gi√¢y
    
    Returns:
        True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
    """
    print("üîÑ ƒêang t·∫°o video hi·ªÉn th·ªã optical flow...")
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"‚ùå Kh√¥ng th·ªÉ m·ªü file video: {video_path}")
        return False
    
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    # T√≠nh to√°n frame interval ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c sample_rate mong mu·ªën
    frame_interval = max(1, int(fps / sample_rate))
    
    # Thi·∫øt l·∫≠p video writer
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, sample_rate, (width, height))
    
    prev_frame = None
    hsv = np.zeros((height, width, 3), dtype=np.uint8)
    hsv[..., 1] = 255  # Saturation
    
    for frame_idx in tqdm(range(0, total_frames, frame_interval), desc="T·∫°o video optical flow"):
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        
        if not ret:
            break
        
        # Convert to grayscale and reduce noise
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        gray = cv2.GaussianBlur(gray, (5, 5), 0)
        
        if prev_frame is None:
            prev_frame = gray
            continue
        
        # Calculate optical flow
        flow = cv2.calcOpticalFlowFarneback(prev_frame, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
        
        # Split flow into x and y components
        flow_x, flow_y = flow[:, :, 0], flow[:, :, 1]
        
        # Calculate global (camera) motion
        mean_flow_x = np.mean(flow_x)
        mean_flow_y = np.mean(flow_y)
        
        # Remove camera motion
        char_flow_x = flow_x - mean_flow_x
        char_flow_y = flow_y - mean_flow_y
        
        # Convert to polar coordinates for visualization
        mag, ang = cv2.cartToPolar(char_flow_x, char_flow_y)
        
        # Map angle to hue (0-180 degrees)
        hsv[..., 0] = ang * 180 / np.pi / 2
        # Map magnitude to value
        hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)
        
        # Convert HSV to BGR for visualization
        flow_vis = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
        
        # Draw vector field (subsampled)
        step = 16
        for y in range(step//2, height, step):
            for x in range(step//2, width, step):
                # Draw camera motion vectors in red
                dx_camera = mean_flow_x * 5  # Scale for visualization
                dy_camera = mean_flow_y * 5
                cv2.arrowedLine(
                    frame, (x, y), 
                    (int(x + dx_camera), int(y + dy_camera)), 
                    (0, 0, 255), 1, tipLength=0.3
                )
                
                # Draw character motion vectors in green
                dx_char = char_flow_x[y, x] * 5
                dy_char = char_flow_y[y, x] * 5
                if abs(dx_char) > 1 or abs(dy_char) > 1:  # Only draw significant motion
                    cv2.arrowedLine(
                        frame, (x, y), 
                        (int(x + dx_char), int(y + dy_char)), 
                        (0, 255, 0), 1, tipLength=0.3
                    )
        
        # Add optical flow visualization as overlay
        alpha = 0.5
        overlay = cv2.addWeighted(frame, 0.7, flow_vis, 0.3, 0)
        
        # Add text showing camera motion
        cv2.putText(
            overlay, 
            f"Camera Motion: dx={mean_flow_x:.2f}, dy={mean_flow_y:.2f}", 
            (10, height - 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1
        )
        
        # Add text showing frame info
        cv2.putText(
            overlay, 
            f"Frame: {frame_idx}/{total_frames} ({frame_idx/fps:.1f}s)", 
            (10, height - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1
        )
        
        out.write(overlay)
        prev_frame = gray
        
    cap.release()
    out.release()
    print(f"‚úÖ Video optical flow ƒë√£ ƒë∆∞·ª£c t·∫°o: {output_path}")
    return True

def detect_high_action_scenes(video_path, model=None, device="cuda", top_k=5, min_duration=2.0, merge_threshold=3.0):
    """
    Ph√°t hi·ªán c√°c c·∫£nh c√≥ h√†nh ƒë·ªông m·∫°nh nh·∫•t b·∫±ng c√°ch k·∫øt h·ª£p ph√¢n t√≠ch t·ª´ nhi·ªÅu ngu·ªìn:
    - Optical Flow: ƒëo l∆∞·ªùng m·ª©c ƒë·ªô chuy·ªÉn ƒë·ªông
    - Scene Detection: ph√°t hi·ªán c√°c ƒëi·ªÉm c·∫Øt c·∫£nh
    - YOLOv5: ph√°t hi·ªán s·ª± hi·ªán di·ªán v√† ho·∫°t ƒë·ªông c·ªßa ng∆∞·ªùi
    
    Args:
        video_path: ƒê∆∞·ªùng d·∫´n video c·∫ßn ph√¢n t√≠ch
        model: M√¥ h√¨nh YOLOv5 (n·∫øu None, s·∫Ω t·∫£i m√¥ h√¨nh m·∫∑c ƒë·ªãnh)
        device: Thi·∫øt b·ªã x·ª≠ l√Ω ("cuda" ho·∫∑c "cpu")
        top_k: S·ªë l∆∞·ª£ng c·∫£nh h√†nh ƒë·ªông cao nh·∫•t mu·ªën tr·∫£ v·ªÅ
        min_duration: Th·ªùi l∆∞·ª£ng t·ªëi thi·ªÉu c·ªßa m·ªói ƒëo·∫°n h√†nh ƒë·ªông (gi√¢y)
        merge_threshold: Ng∆∞·ª°ng th·ªùi gian ƒë·ªÉ gh√©p c√°c c·∫£nh g·∫ßn nhau (gi√¢y)
        
    Returns:
        Danh s√°ch c√°c c·∫∑p (start_time, end_time) l√† th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu v√† k·∫øt th√∫c c·ªßa c·∫£nh h√†nh ƒë·ªông cao
    """
    print("üîç Ph√°t hi·ªán c·∫£nh h√†nh ƒë·ªông m·∫°nh b·∫±ng ph∆∞∆°ng ph√°p k·∫øt h·ª£p...")
    
    # 1. Kh·ªüi t·∫°o model YOLOv5 n·∫øu kh√¥ng c√≥
    if model is None:
        try:
            model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)
            model.to(device)
            model.classes = [0]  # Ch·ªâ quan t√¢m ƒë·∫øn l·ªõp ng∆∞·ªùi
            print("‚úÖ ƒê√£ kh·ªüi t·∫°o model YOLOv5 ƒë·ªÉ ph√°t hi·ªán ng∆∞·ªùi")
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫£i model YOLOv5: {e}")
            model = None
    
    # 2. Ph√¢n t√≠ch chuy·ªÉn ƒë·ªông v·ªõi optical flow
    print("B∆∞·ªõc 1/3: Ph√¢n t√≠ch chuy·ªÉn ƒë·ªông b·∫±ng Optical Flow...")
    motion_segments = detect_motion_with_optical_flow(
        video_path, 
        sample_rate=2, 
        threshold=0.6,
        min_segment_duration=min_duration
    )
    print(f"‚Üí ƒê√£ ph√°t hi·ªán {len(motion_segments)} ƒëo·∫°n chuy·ªÉn ƒë·ªông m·∫°nh")
    
    # 3. Ph√°t hi·ªán c·∫Øt c·∫£nh 
    print("B∆∞·ªõc 2/3: Ph√°t hi·ªán c√°c ƒëi·ªÉm c·∫Øt c·∫£nh...")
    scene_changes = detect_scene_changes(video_path, threshold=25)
    print(f"‚Üí ƒê√£ ph√°t hi·ªán {len(scene_changes)} ƒëi·ªÉm c·∫Øt c·∫£nh")
    
    # 4. Ph√°t hi·ªán ng∆∞·ªùi v√† ho·∫°t ƒë·ªông c·ªßa h·ªç v·ªõi YOLOv5
    print("B∆∞·ªõc 3/3: Ph√¢n t√≠ch ho·∫°t ƒë·ªông c·ªßa ng∆∞·ªùi v·ªõi YOLOv5...")
    if model is not None:
        person_segments = detect_action_with_yolo_batched(
            video_path, 
            model=model, 
            device=device, 
            batch_size=8, 
            frame_rate=1
        )
        print(f"‚Üí ƒê√£ ph√°t hi·ªán {len(person_segments)} ƒëo·∫°n c√≥ ho·∫°t ƒë·ªông c·ªßa ng∆∞·ªùi")
    else:
        person_segments = []
        print("‚ö†Ô∏è Kh√¥ng c√≥ model YOLOv5, b·ªè qua ph√¢n t√≠ch ho·∫°t ƒë·ªông c·ªßa ng∆∞·ªùi")
    
    # 5. K·∫øt h·ª£p k·∫øt qu·∫£ ph√¢n t√≠ch ƒë·ªÉ t·∫°o ƒëi·ªÉm s·ªë h√†nh ƒë·ªông cho m·ªói gi√¢y trong video
    print("ƒêang k·∫øt h·ª£p ph√¢n t√≠ch t·ª´ c√°c ngu·ªìn kh√°c nhau...")
    
    # L·∫•y th√¥ng tin th·ªùi l∆∞·ª£ng video
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = int(total_frames / fps) + 1
    cap.release()
    
    # Kh·ªüi t·∫°o m·∫£ng ƒëi·ªÉm h√†nh ƒë·ªông theo th·ªùi gian
    action_scores = np.zeros(duration)
    
    # Th√™m ƒëi·ªÉm t·ª´ ph√¢n t√≠ch optical flow
    for start, end in motion_segments:
        start_idx = int(start)
        end_idx = min(int(end), duration - 1)
        action_scores[start_idx:end_idx+1] += 2.0  # Tr·ªçng s·ªë cao cho chuy·ªÉn ƒë·ªông
    
    # Th√™m ƒëi·ªÉm t·ª´ ph√¢n t√≠ch c·∫Øt c·∫£nh (t·∫°o window quanh ƒëi·ªÉm c·∫Øt c·∫£nh)
    scene_window = 2  # 2 gi√¢y tr∆∞·ªõc v√† sau ƒëi·ªÉm c·∫Øt c·∫£nh
    for change_time in scene_changes:
        time_idx = int(change_time)
        if time_idx < duration:
            window_start = max(0, time_idx - scene_window)
            window_end = min(duration - 1, time_idx + scene_window)
            # Th√™m ƒëi·ªÉm cho c√°c c·∫£nh c√≥ c·∫Øt c·∫£nh nhanh
            action_scores[window_start:window_end+1] += 1.0
    
    # Th√™m ƒëi·ªÉm t·ª´ ph√¢n t√≠ch ng∆∞·ªùi
    for start, end in person_segments:
        start_idx = int(start)
        end_idx = min(int(end), duration - 1)
        action_scores[start_idx:end_idx+1] += 1.5  # Tr·ªçng s·ªë cho s·ª± hi·ªán di·ªán c·ªßa ng∆∞·ªùi
    
    # 6. T√≠nh to√°n density c·ªßa scene changes ƒë·ªÉ ph√°t hi·ªán c·∫£nh h√†nh ƒë·ªông (th∆∞·ªùng c√≥ c·∫Øt c·∫£nh nhanh)
    scene_density = analyze_scene_change_density(scene_changes, window_size=5)
    for i, density in enumerate(scene_density):
        if i < duration:
            action_scores[i] += density * 1.5
    
    # 7. L√†m m·ªãn ƒëi·ªÉm s·ªë b·∫±ng moving average
    window_size = 5
    smoothed_scores = np.convolve(action_scores, np.ones(window_size)/window_size, mode='same')
    
    # 8. Ph√°t hi·ªán c√°c ƒëo·∫°n c√≥ ƒëi·ªÉm h√†nh ƒë·ªông cao
    threshold = np.mean(smoothed_scores) + np.std(smoothed_scores)
    high_action_segments = []
    in_high_action = False
    start_time = 0
    
    for i, score in enumerate(smoothed_scores):
        if score > threshold and not in_high_action:
            in_high_action = True
            start_time = i
        elif (score <= threshold * 0.7 or i == len(smoothed_scores) - 1) and in_high_action:
            in_high_action = False
            end_time = i
            # Ch·ªâ l·∫•y c√°c ƒëo·∫°n ƒë·ªß d√†i
            if end_time - start_time >= min_duration:
                high_action_segments.append([start_time, end_time])
    
    # 9. Gh√©p c√°c ƒëo·∫°n g·∫ßn nhau
    merged_segments = []
    if high_action_segments:
        merged_segments = [high_action_segments[0]]
        
        for current in high_action_segments[1:]:
            previous = merged_segments[-1]
            if current[0] - previous[1] < merge_threshold:
                previous[1] = current[1]  # M·ªü r·ªông ƒëo·∫°n tr∆∞·ªõc
            else:
                merged_segments.append(current)
    
    # 10. T√≠nh ƒëi·ªÉm cho m·ªói ƒëo·∫°n ƒë·ªÉ s·∫Øp x·∫øp theo m·ª©c ƒë·ªô h√†nh ƒë·ªông
    segment_scores = []
    for start, end in merged_segments:
        segment_duration = end - start
        # T√≠nh ƒëi·ªÉm trung b√¨nh trong ƒëo·∫°n
        segment_score = np.mean(smoothed_scores[start:end+1])
        # ƒêo·∫°n d√†i h∆°n ƒë∆∞·ª£c ∆∞u ti√™n n·∫øu ƒëi·ªÉm g·∫ßn b·∫±ng nhau
        final_score = segment_score * (1 + min(segment_duration / 60.0, 0.5))
        segment_scores.append((final_score, [start, end]))
    
    # S·∫Øp x·∫øp c√°c ƒëo·∫°n theo ƒëi·ªÉm t·ª´ cao xu·ªëng th·∫•p
    segment_scores.sort(reverse=True)
    
    # Ch·ªçn top K ƒëo·∫°n c√≥ ƒëi·ªÉm cao nh·∫•t
    top_segments = []
    for _, segment in segment_scores[:top_k]:
        top_segments.append(segment)
    
    # Hi·ªÉn th·ªã th√¥ng tin ƒëo·∫°n ƒë∆∞·ª£c ch·ªçn
    print(f"üéØ ƒê√£ ph√°t hi·ªán {len(top_segments)} c·∫£nh h√†nh ƒë·ªông m·∫°nh nh·∫•t:")
    for i, (start, end) in enumerate(top_segments):
        duration = end - start
        print(f"  C·∫£nh {i+1}: {start}s - {end}s (th·ªùi l∆∞·ª£ng: {duration:.1f}s)")
    
    return top_segments

def extract_high_action_scenes(video_path, output_path, model=None, device="cuda", top_k=5, buffer_time=1.0):
    """
    Tr√≠ch xu·∫•t c√°c c·∫£nh h√†nh ƒë·ªông m·∫°nh nh·∫•t t·ª´ video th√†nh m·ªôt video m·ªõi
    
    Args:
        video_path: ƒê∆∞·ªùng d·∫´n video g·ªëc
        output_path: ƒê∆∞·ªùng d·∫´n l∆∞u video k·∫øt qu·∫£
        model: M√¥ h√¨nh YOLOv5 (n·∫øu None, s·∫Ω t·∫£i m√¥ h√¨nh m·∫∑c ƒë·ªãnh)
        device: Thi·∫øt b·ªã x·ª≠ l√Ω ("cuda" ho·∫∑c "cpu")
        top_k: S·ªë l∆∞·ª£ng c·∫£nh h√†nh ƒë·ªông cao nh·∫•t mu·ªën tr√≠ch xu·∫•t
        buffer_time: Th·ªùi gian th√™m v√†o tr∆∞·ªõc v√† sau m·ªói ƒëo·∫°n (gi√¢y)
    
    Returns:
        True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
    """
    print("üé¨ ƒêang tr√≠ch xu·∫•t c√°c c·∫£nh h√†nh ƒë·ªông m·∫°nh nh·∫•t...")
    
    # Ph√°t hi·ªán c√°c c·∫£nh h√†nh ƒë·ªông m·∫°nh nh·∫•t
    action_segments = detect_high_action_scenes(
        video_path, 
        model=model, 
        device=device, 
        top_k=top_k
    )
    
    if not action_segments:
        print("‚ùå Kh√¥ng t√¨m th·∫•y c·∫£nh h√†nh ƒë·ªông n√†o")
        return False
    
    # Th√™m buffer time
    clip = VideoFileClip(video_path)
    duration = clip.duration
    
    buffered_segments = []
    for start, end in action_segments:
        buffered_start = max(0, start - buffer_time)
        buffered_end = min(duration, end + buffer_time)
        buffered_segments.append([buffered_start, buffered_end])
    
    # Tr√≠ch xu·∫•t c√°c ƒëo·∫°n video
    try:
        # S·ª≠ d·ª•ng GPU n·∫øu c√≥
        if device == "cuda" and torch.cuda.is_available():
            success = fast_extract_segments_cuda(video_path, output_path, buffered_segments)
        else:
            # S·ª≠ d·ª•ng CPU
            success = extract_segments_to_video(video_path, output_path, buffered_segments)
        
        if success:
            print(f"‚úÖ ƒê√£ tr√≠ch xu·∫•t th√†nh c√¥ng {len(buffered_segments)} c·∫£nh h√†nh ƒë·ªông v√†o: {output_path}")
        else:
            print("‚ùå Kh√¥ng th·ªÉ tr√≠ch xu·∫•t c√°c c·∫£nh h√†nh ƒë·ªông")
        return success
    except Exception as e:
        print(f"‚ùå L·ªói khi tr√≠ch xu·∫•t c·∫£nh h√†nh ƒë·ªông: {e}")
        return False

def extract_highlight_clips(video_path, timestamps, output_path, buffer_time=1.0, use_cuda=True):
    """
    Tr√≠ch xu·∫•t c√°c ƒëo·∫°n highlight t·ª´ video d·ª±a tr√™n timestamps v√† gh√©p th√†nh video m·ªõi
    
    Args:
        video_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn video g·ªëc
        timestamps: Danh s√°ch c√°c c·∫∑p [start_time, end_time]
        output_path: ƒê∆∞·ªùng d·∫´n l∆∞u video k·∫øt qu·∫£
        buffer_time: Th·ªùi gian b·ªï sung tr∆∞·ªõc v√† sau m·ªói ƒëo·∫°n (gi√¢y)
        use_cuda: S·ª≠ d·ª•ng GPU ƒë·ªÉ tƒÉng t·ªëc n·∫øu c√≥
    
    Returns:
        True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
    """
    if not timestamps:
        print("‚ùå Kh√¥ng c√≥ timestamps ƒë·ªÉ tr√≠ch xu·∫•t")
        return False
    
    print(f"üìã Chu·∫©n b·ªã tr√≠ch xu·∫•t {len(timestamps)} ƒëo·∫°n highlights...")
    
    # Ki·ªÉm tra th√¥ng tin video ƒë·ªÉ ƒë·∫£m b·∫£o timestamps h·ª£p l·ªá
    clip = VideoFileClip(video_path)
    duration = clip.duration
    
    # Th√™m buffer time v√† ƒë·∫£m b·∫£o timestamps n·∫±m trong gi·ªõi h·∫°n c·ªßa video
    buffered_segments = []
    total_extract_duration = 0
    
    for start, end in timestamps:
        buffered_start = max(0, start - buffer_time)
        buffered_end = min(duration, end + buffer_time)
        # Ch·ªâ th√™m c√°c ƒëo·∫°n c√≥ ƒë·ªô d√†i h·ª£p l√Ω
        if buffered_end - buffered_start >= 1.0:
            buffered_segments.append([buffered_start, buffered_end])
            total_extract_duration += (buffered_end - buffered_start)
    
    if not buffered_segments:
        print("‚ùå Kh√¥ng c√≥ ƒëo·∫°n n√†o h·ª£p l·ªá sau khi th√™m buffer time")
        clip.close()
        return False
    
    # S·∫Øp x·∫øp c√°c ƒëo·∫°n theo th·ª© t·ª± th·ªùi gian
    buffered_segments.sort(key=lambda x: x[0])
    
    # Hi·ªÉn th·ªã th√¥ng tin tr√≠ch xu·∫•t
    print(f"üé¨ Tr√≠ch xu·∫•t {len(buffered_segments)} ƒëo·∫°n highlight (t·ªïng th·ªùi l∆∞·ª£ng: {total_extract_duration:.1f}s)")
    
    # Th·ª≠ s·ª≠ d·ª•ng GPU ƒë·ªÉ tr√≠ch xu·∫•t n·∫øu c√≥ th·ªÉ
    if use_cuda and torch.cuda.is_available():
        print("üöÄ S·ª≠ d·ª•ng GPU ƒë·ªÉ tƒÉng t·ªëc tr√≠ch xu·∫•t video")
        success = fast_extract_segments_cuda(video_path, output_path, buffered_segments)
        if success:
            print(f"‚úÖ ƒê√£ t·∫°o video highlight th√†nh c√¥ng: {output_path}")
            clip.close()
            return True
        else:
            print("‚ö†Ô∏è Tr√≠ch xu·∫•t v·ªõi GPU th·∫•t b·∫°i, chuy·ªÉn sang ph∆∞∆°ng ph√°p th√¥ng th∆∞·ªùng")
        
    # S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p th√¥ng th∆∞·ªùng (CPU)
    try:
        clips = []
        for i, (start, end) in enumerate(buffered_segments):
            print(f"  ƒêo·∫°n {i+1}/{len(buffered_segments)}: {start:.1f}s - {end:.1f}s (ƒë·ªô d√†i: {end-start:.1f}s)")
            clips.append(clip.subclip(start, end))
        
        if clips:
            print(f"üîÑ ƒêang gh√©p {len(clips)} ƒëo·∫°n video...")
            final_clip = concatenate_videoclips(clips)
            # Xu·∫•t video v·ªõi codec H.264
            print(f"üíæ ƒêang xu·∫•t video highlight ra {output_path}...")
            final_clip.write_videofile(
                output_path, 
                codec="libx264", 
                audio_codec="aac",
                threads=min(4, os.cpu_count() or 2),
                preset="medium"  # C√¢n b·∫±ng gi·ªØa t·ªëc ƒë·ªô xu·∫•t v√† ch·∫•t l∆∞·ª£ng
            )
            final_clip.close()
            clip.close()
            print(f"‚úÖ ƒê√£ t·∫°o video highlight th√†nh c√¥ng: {output_path}")
            return True
        else:
            clip.close()
            print("‚ùå Kh√¥ng c√≥ ƒëo·∫°n video n√†o h·ª£p l·ªá ƒë·ªÉ gh√©p")
            return False
    except Exception as e:
        print(f"‚ùå L·ªói khi t·∫°o video highlight: {e}")
        try:
            clip.close()
        except:
            pass
        return False

def initialize_yolo8_model(device="cuda", model_name="yolov8s.pt", task="detect"):
    """
    Initialize YOLOv8 model for object detection or action recognition
    
    Args:
        device: Device to run the model on ("cuda" or "cpu")
        model_name: Name or path of the model to load ("yolov8s.pt", "yolov8n-pose.pt", etc.)
        task: Type of task ("detect", "pose", "seg", "classify")
        
    Returns:
        Initialized YOLOv8 model
    """
    print(f"üîç Initializing YOLOv8 model for {task}...")
    try:
        from ultralytics import YOLO
        
        # Check if model exists or download it
        model_path = os.path.join('models', model_name)
        if not os.path.exists(model_path):
            model_path = model_name  # Use direct name for downloading
        
        # Load YOLOv8 model
        model = YOLO(model_path)
        
        # Set device
        if device == "cuda" and torch.cuda.is_available():
            model.to(device)
            print(f"‚úÖ YOLOv8 model loaded on {device}")
        else:
            print("‚ö†Ô∏è CUDA not available, using CPU")
        return model
    except Exception as e:
        print(f"‚ùå Failed to load YOLOv8: {e}")
        return None

def detect_high_action_scenes_yolo8(video_path, model=None, device="cuda", top_k=5, min_duration=2.0, 
                                  merge_threshold=3.0, confidence=0.25, task="detect"):
    """
    Enhanced detection of high action scenes using YOLOv8 and optical flow
    
    Args:
        video_path: Path to the video file
        model: YOLOv8 model (if None, will initialize one)
        device: Device to run the model on
        top_k: Number of top action scenes to return
        min_duration: Minimum duration for action scenes
        merge_threshold: Threshold for merging nearby scenes
        confidence: Confidence threshold for YOLOv8 detection
        task: Type of YOLOv8 task ("detect", "pose")
        
    Returns:
        List of top action segments as [start_time, end_time]
    """
    print("üé¨ Detecting high action scenes with YOLOv8...")
    
    # Initialize YOLOv8 if not provided
    if model is None:
        if task == "pose":
            model = initialize_yolo8_model(device, "yolov8s-pose.pt", task)
        else:
            model = initialize_yolo8_model(device, "yolov8s.pt", task)
            
        if model is None:
            print("‚ö†Ô∏è Failed to initialize YOLOv8, falling back to optical flow only")
            # Fallback to older method if YOLOv8 initialization fails
            return detect_high_action_scenes(video_path, None, device, top_k, min_duration, merge_threshold)
    
    # Get video info
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = frame_count / fps
    cap.release()
    
    # Extract frames with Decord for faster processing
    sample_rate = 1  # 1 frame per second
    frames, timestamps, _ = None, None, None  # Initialize with None values
    if DECORD_AVAILABLE:
        try:
            # FIX: Handle 3 return values properly
            frames, timestamps, fps = extract_frames_with_decord(
                video_path, sample_rate=sample_rate, device_id=0 if device == "cuda" else -1)
        except Exception as e:
            print(f"Error with Decord frame extraction: {e}")
            frames, timestamps = None, None
        
    # Fallback to OpenCV if Decord fails
    if frames is None:
        frames, timestamps = extract_frames_with_opencv(video_path, sample_rate=sample_rate)
        
    if not frames:
        print("‚ùå Failed to extract frames")
        return []
        
    print(f"‚úÖ Extracted {len(frames)} frames for analysis")
    
    # Process frames in batches with YOLOv8
    batch_size = 8
    yolo_results = []
    
    print("üîç Analyzing frames with YOLOv8...")
    for i in tqdm(range(0, len(frames), batch_size)):
        batch_frames = frames[i:i+batch_size]
        
        # Process batch with YOLOv8
        batch_results = model.predict(
            batch_frames, 
            conf=confidence, 
            verbose=False
        )
        
        # Store results
        for j, result in enumerate(batch_results):
            idx = i + j
            if idx < len(timestamps):
                # For pose detection, we get keypoints
                if task == "pose":
                    keypoints = result.keypoints
                    num_people = len(result.boxes) if hasattr(result, 'boxes') else 0
                    yolo_results.append({
                        'timestamp': timestamps[idx],
                        'num_people': num_people,
                        'has_keypoints': keypoints is not None and len(keypoints) > 0
                    })
                # For object detection
                else:
                    # Count people (class 0)
                    boxes = result.boxes
                    people = [box for box in boxes if int(box.cls) == 0]
                    yolo_results.append({
                        'timestamp': timestamps[idx],
                        'num_people': len(people),
                        'boxes': boxes
                    })
    
    # Start parallel optical flow analysis
    print("üîÑ Starting optical flow analysis...")
    motion_segments = detect_motion_with_optical_flow(
        video_path, 
        sample_rate=2,  # Higher sample rate for optical flow
        threshold=0.6,
        min_segment_duration=min_duration
    )
    
    # Calculate action scores from YOLOv8 results
    action_scores = []
    for result in yolo_results:
        # More people = higher score
        people_score = min(result['num_people'] * 0.5, 3.0)  # Cap at 3.0
        
        # Add results to list with timestamp
        action_scores.append({
            'timestamp': result['timestamp'],
            'score': people_score,
        })
    
    # Combine action scores with optical flow results to create final scores
    # Create a time-indexed score array
    time_range = np.arange(0, int(duration) + 1)
    combined_scores = np.zeros(len(time_range))
    
    # Add YOLOv8 scores
    for result in action_scores:
        time_idx = int(result['timestamp'])
        if time_idx < len(combined_scores):
            # Add people score
            combined_scores[time_idx] += result['score']
        
    # Add optical flow scores
    for start, end in motion_segments:
        start_idx = int(start)
        end_idx = min(int(end), len(combined_scores) - 1)
        combined_scores[start_idx:end_idx+1] += 2.0  # Higher weight for motion
    
    # Smooth scores
    window_size = 5
    smoothed_scores = np.convolve(combined_scores, np.ones(window_size)/window_size, mode='same')
    
    # Find high action segments
    threshold = np.mean(smoothed_scores) + np.std(smoothed_scores)
    high_action_segments = []
    in_high_action = False
    start_time = 0
    
    for i, score in enumerate(smoothed_scores):
        if score > threshold and not in_high_action:
            in_high_action = True
            start_time = i
        elif (score <= threshold * 0.7 or i == len(smoothed_scores) - 1) and in_high_action:
            in_high_action = False
            end_time = i
            if end_time - start_time >= min_duration:
                high_action_segments.append([start_time, end_time])
    
    # Merge segments that are close to each other
    merged_segments = []
    if high_action_segments:
        merged_segments = [high_action_segments[0]]
        
        for current in high_action_segments[1:]:
            previous = merged_segments[-1]
            if current[0] - previous[1] < merge_threshold:
                previous[1] = current[1]
            else:
                merged_segments.append(current)
    
    # Score segments for ranking
    segment_scores = []
    for start, end in merged_segments:
        segment_duration = end - start
        # Average score in the segment
        segment_score = np.mean(smoothed_scores[start:end+1])
        # Slightly favor longer segments with similar scores
        final_score = segment_score * (1 + min(segment_duration / 60.0, 0.5))
        segment_scores.append((final_score, [start, end]))
    
    # Sort by score and get top segments
    segment_scores.sort(reverse=True)
    top_segments = []
    for _, segment in segment_scores[:top_k]:
        top_segments.append(segment)
    
    # Show selected segments
    print(f"üéØ Found {len(top_segments)} top action scenes:")
    for i, (start, end) in enumerate(top_segments):
        duration = end - start
        print(f"  Scene {i+1}: {start}s - {end}s (duration: {duration:.1f}s)")
    
    return top_segments

def create_single_action_short(video_path, output_path, model=None, device="cuda", 
                             buffer_time=1.0, min_duration=5.0, max_duration=15.0):
    """Create a short video with a single high-action moment"""
    print("üé¨ Creating single action highlight video...")
    
    # Check GPU availability again to make sure
    if device == "cuda" and not torch.cuda.is_available():
        force_cuda = os.environ.get('FORCE_CUDA', '0') == '1'
        if not force_cuda:
            print("‚ö†Ô∏è GPU kh√¥ng kh·∫£ d·ª•ng, chuy·ªÉn sang CPU")
            device = "cpu"
    
    # Initialize YOLOv8 model if not provided
    if model is None:
        from yolo8_detection import initialize_yolo8_model
        model = initialize_yolo8_model(device)
        if model is None:
            print("‚ùå Kh√¥ng th·ªÉ kh·ªüi t·∫°o YOLOv8 model")
            return False
    
    # Rest of the function...

def create_multi_action_short(video_path, output_path, model=None, device="cuda", 
                            target_duration=30.0, max_scenes=5, buffer_time=1.0,
                            min_segment_duration=3.0):
    """
    Create a short video with multiple high-action moments merged together
    
    Args:
        video_path: Path to the input video
        output_path: Path to save the output video
        model: YOLOv8 model (if None, will initialize one)
        device: Device to run the model on
        target_duration: Target duration of the final video
        max_scenes: Maximum number of scenes to include
        buffer_time: Time to add before and after each segment
        min_segment_duration: Minimum duration for each action segment
        
    Returns:
        True if successful, False otherwise
    """
    print(f"üé¨ Creating multi-action short video (target duration: {target_duration:.1f}s)...")
    
    # Initialize YOLOv8 if not provided
    if model is None:
        model = initialize_yolo8_model(device)
    
    # Get top action segments
    segments = detect_high_action_scenes_yolo8(
        video_path, model, device, top_k=max_scenes, 
        min_duration=min_segment_duration
    )
    
    if not segments:
        print("‚ùå No action scenes detected")
        return False
    
    # Get video duration
    cap = cv2.VideoCapture(video_path)
    video_duration = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) / cap.get(cv2.CAP_PROP_FPS))
    cap.release()
    
    # Add buffer time to segments
    buffered_segments = []
    for start, end in segments:
        buffered_start = max(0, start - buffer_time)
        buffered_end = min(video_duration, end + buffer_time)
        buffered_segments.append([buffered_start, buffered_end])
    
    # Calculate total duration and trim if necessary
    total_duration = sum(end - start for start, end in buffered_segments)
    if total_duration > target_duration:
        print(f"‚ö†Ô∏è Total duration ({total_duration:.1f}s) exceeds target ({target_duration:.1f}s), trimming...")
        # Prioritize keeping the highest-scored segments
        selected_segments = []
        current_duration = 0
        
        for start, end in buffered_segments:
            segment_duration = end - start
            if current_duration + segment_duration <= target_duration:
                selected_segments.append([start, end])
                current_duration += segment_duration
            else:
                # If we can fit part of this segment, trim it
                remaining_time = target_duration - current_duration
                if remaining_time >= min_segment_duration:
                    # Trim from the end to preserve the action intro
                    selected_segments.append([start, start + remaining_time])
                    current_duration += remaining_time
                break
        
        buffered_segments = selected_segments
    
    # Extract and combine segments
    print(f"üìπ Extracting {len(buffered_segments)} segments (total: {total_duration:.1f}s)...")
    
    # Use GPU acceleration if available
    if device == "cuda" and torch.cuda.is_available():
        success = fast_extract_segments_cuda(video_path, output_path, buffered_segments)
    else:
        # Use CPU with moviepy
        try:
            clip = VideoFileClip(video_path)
            clips = []
            
            for i, (start, end) in enumerate(buffered_segments):
                print(f"  Segment {i+1}: {start:.1f}s - {end:.1f}s (duration: {end-start:.1f}s)")
                clips.append(clip.subclip(start, end))
            
            if clips:
                final_clip = concatenate_videoclips(clips)
                final_clip.write_videofile(output_path, codec="libx264", audio_codec="aac")
                final_clip.close()
                clip.close()
                success = True
            else:
                success = False
        except Exception as e:
            print(f"‚ùå Error creating video: {e}")
            success = False
    
    if success:
        print(f"‚úÖ Successfully created multi-action short: {output_path}")
        return True
    else:
        print("‚ùå Failed to create short video")
        return False

def extract_highlight_clips(video_path, timestamps, output_path, buffer_time=1.0, use_cuda=True):
    """
    Tr√≠ch xu·∫•t c√°c ƒëo·∫°n highlight t·ª´ video d·ª±a tr√™n timestamps v√† gh√©p th√†nh video m·ªõi
    
    Args:
        video_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn video g·ªëc
        timestamps: Danh s√°ch c√°c c·∫∑p [start_time, end_time]
        output_path: ƒê∆∞·ªùng d·∫´n l∆∞u video k·∫øt qu·∫£
        buffer_time: Th·ªùi gian b·ªï sung tr∆∞·ªõc v√† sau m·ªói ƒëo·∫°n (gi√¢y)
        use_cuda: S·ª≠ d·ª•ng GPU ƒë·ªÉ tƒÉng t·ªëc n·∫øu c√≥
    
    Returns:
        True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
    """
    # ...existing code...

def detect_motion_with_optical_flow_cpp(video_path, sample_rate=2, threshold=0.5):
    """
    Enhanced version of detect_motion_with_optical_flow using C++ acceleration
    """
    print("üîç Analyzing video motion using C++ accelerated optical flow...")
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"‚ùå Cannot open video: {video_path}")
        return []
    
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    # Calculate frame interval based on sample rate
    frame_interval = max(1, int(fps / sample_rate))
    
    prev_frame = None
    motion_scores = []
    timestamps = []
    
    for frame_idx in tqdm(range(0, total_frames, frame_interval), desc="Analyzing motion"):
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        
        if not ret:
            break
        
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        if prev_frame is None:
            prev_frame = gray
            timestamps.append(frame_idx / fps)
            motion_scores.append(0)
            continue
        
        # Calculate optical flow using optimized function
        flow_x, flow_y = calculate_optical_flow_optimized(prev_frame, gray)
        
        # Calculate magnitude and global motion
        magnitude = np.sqrt(flow_x**2 + flow_y**2)
        mean_flow_x = np.mean(flow_x)
        mean_flow_y = np.mean(flow_y)
        
        # Compute character motion by removing camera motion
        character_flow_x = flow_x - mean_flow_x
        character_flow_y = flow_y - mean_flow_y
        character_magnitude = np.sqrt(character_flow_x**2 + character_flow_y**2)
        character_motion = np.mean(character_magnitude)
        
        motion_scores.append(character_motion)
        timestamps.append(frame_idx / fps)
        prev_frame = gray
        
    cap.release()
    
    # Rest of the function (finding segments, etc.) remains the same

def calculate_optical_flow_optimized(prev_frame, curr_frame, device="cuda"):
    """
    Calculate optical flow using C++ extension if available, otherwise fall back to OpenCV
    """
    try:
        from utils.gpu_utils import try_load_cpp_extensions
        cpp_extensions = try_load_cpp_extensions()
        
        if "optical_flow" in cpp_extensions:
            optical_flow_cpp = cpp_extensions["optical_flow"]
            use_cuda = device == "cuda" and torch.cuda.is_available()
            device_id = 0 if use_cuda else -1
            print("Using C++ optical flow implementation")
            flow_x, flow_y = optical_flow_cpp.calculate_flow(
                prev_frame, curr_frame, use_cuda, device_id)
            return flow_x, flow_y
    except Exception as e:
        print(f"Error using C++ optical flow: {e}")
        print("Falling back to OpenCV optical flow implementation")
    
    flow = cv2.calcOpticalFlowFarneback(prev_frame, curr_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)
    return flow[:, :, 0], flow[:, :, 1]
